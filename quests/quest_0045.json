{
  "problem_id": 45,
  "title": "Linear Kernel Function",
  "category": "Machine Learning",
  "difficulty": "easy",
  "description": "Write a Python function `kernel_function` that computes the linear kernel between two input vectors `x1` and `x2`. The linear kernel is defined as the dot product (inner product) of two vectors.",
  "example": {
    "input": "import numpy as np\n\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])\n\nresult = kernel_function(x1, x2)\nprint(result)",
    "output": "32",
    "reasoning": "The linear kernel between x1 and x2 is computed as:1\\*4 + 2\\*5 + 3\\*6 = 32"
  },
  "starter_code": "import numpy as np\n\ndef kernel_function(x1, x2):\n\t# Your code here\n\tpass\n",
  "sub_quests": [
    {
      "step": 1,
      "title": "Vector Representation and Components",
      "relation_to_problem": "Understanding vector representation is fundamental to implementing the linear kernel, as it operates on vectors by accessing and manipulating their individual components element-wise.",
      "prerequisites": [
        "Basic Python",
        "Lists and Arrays",
        "Indexing"
      ],
      "learning_objectives": [
        "Define vectors formally in finite-dimensional spaces",
        "Represent vectors as NumPy arrays in Python",
        "Access and manipulate vector components by index",
        "Understand vector dimensionality and its implications"
      ],
      "math_content": {
        "definition": "A vector $\\mathbf{x}$ in $\\mathbb{R}^n$ is an ordered n-tuple of real numbers. Formally, $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ where each $x_i \\in \\mathbb{R}$ is called the $i$-th component of $\\mathbf{x}$. The set $\\mathbb{R}^n = \\{(x_1, \\ldots, x_n) : x_i \\in \\mathbb{R}\\}$ forms a vector space under component-wise addition and scalar multiplication.",
        "notation": "$\\mathbf{x} \\in \\mathbb{R}^n$ means vector $\\mathbf{x}$ has $n$ components; $x_i$ denotes the $i$-th component where $i \\in \\{1, 2, \\ldots, n\\}$; $\\dim(\\mathbf{x}) = n$ denotes the dimension",
        "theorem": "Vector Equality Theorem: Two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ are equal if and only if $x_i = y_i$ for all $i \\in \\{1, \\ldots, n\\}$.",
        "proof_sketch": "($\\Rightarrow$) If $\\mathbf{x} = \\mathbf{y}$, then by definition of vector equality, all corresponding components must match. ($\\Leftarrow$) If $x_i = y_i$ for all $i$, then the ordered n-tuples are identical, hence $\\mathbf{x} = \\mathbf{y}$.",
        "examples": [
          "Vector $\\mathbf{v} = (2, -3, 5) \\in \\mathbb{R}^3$ has components $v_1 = 2$, $v_2 = -3$, $v_3 = 5$",
          "In Python: v = np.array([2, -3, 5]) represents the same vector; v[0] = 2, v[1] = -3, v[2] = 5",
          "Zero vector $\\mathbf{0} = (0, 0, \\ldots, 0) \\in \\mathbb{R}^n$ has all components equal to 0"
        ]
      },
      "key_formulas": [
        {
          "name": "Component Access",
          "latex": "$x_i = \\mathbf{x}[i-1]$ (in 0-indexed systems)",
          "description": "Access the $i$-th mathematical component using 0-based indexing in programming"
        },
        {
          "name": "Vector Dimension",
          "latex": "$\\dim(\\mathbf{x}) = n$ where $\\mathbf{x} \\in \\mathbb{R}^n$",
          "description": "The number of components determines the vector's dimension"
        }
      ],
      "exercise": {
        "description": "Implement a function that extracts and returns all components of a vector as a Python list. This builds the foundation for component-wise operations needed in the linear kernel.",
        "function_signature": "def get_vector_components(x: np.ndarray) -> list:",
        "starter_code": "import numpy as np\n\ndef get_vector_components(x: np.ndarray) -> list:\n    # Your code here: return a list of all components\n    pass",
        "test_cases": [
          {
            "input": "get_vector_components(np.array([1, 2, 3]))",
            "expected": "[1, 2, 3]",
            "explanation": "The vector has three components: 1, 2, and 3"
          },
          {
            "input": "get_vector_components(np.array([5.5, -2.3]))",
            "expected": "[5.5, -2.3]",
            "explanation": "Works with floating-point components"
          },
          {
            "input": "get_vector_components(np.array([7]))",
            "expected": "[7]",
            "explanation": "A 1-dimensional vector has a single component"
          }
        ]
      },
      "common_mistakes": [
        "Confusing 0-based indexing (programming) with 1-based indexing (mathematics)",
        "Attempting to access components beyond the vector's dimension",
        "Not checking if vectors have the same dimension before operations"
      ],
      "hint": "Use NumPy array methods or Python list conversion to iterate through components",
      "references": [
        "Vector spaces",
        "NumPy array basics",
        "Linear algebra fundamentals"
      ]
    },
    {
      "step": 2,
      "title": "Element-wise Multiplication of Vectors",
      "relation_to_problem": "The linear kernel requires computing products of corresponding components from two vectors, which is the first step in the dot product calculation.",
      "prerequisites": [
        "Vector components",
        "Array indexing",
        "Iteration"
      ],
      "learning_objectives": [
        "Define the Hadamard product (element-wise multiplication)",
        "Implement element-wise multiplication for equal-dimensional vectors",
        "Understand the distinction between element-wise and matrix multiplication",
        "Validate dimension compatibility before operations"
      ],
      "math_content": {
        "definition": "The Hadamard product (element-wise product) of two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ is a vector $\\mathbf{z} \\in \\mathbb{R}^n$ defined by $\\mathbf{z} = \\mathbf{x} \\odot \\mathbf{y}$ where $z_i = x_i \\cdot y_i$ for all $i \\in \\{1, \\ldots, n\\}$. This operation is also denoted as $\\mathbf{x} \\circ \\mathbf{y}$ in some texts.",
        "notation": "$\\mathbf{z} = \\mathbf{x} \\odot \\mathbf{y}$ represents element-wise multiplication; $z_i = x_i y_i$ for each component",
        "theorem": "Hadamard Product Properties: For vectors $\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in \\mathbb{R}^n$ and scalar $\\alpha \\in \\mathbb{R}$: (1) Commutativity: $\\mathbf{x} \\odot \\mathbf{y} = \\mathbf{y} \\odot \\mathbf{x}$; (2) Associativity: $(\\mathbf{x} \\odot \\mathbf{y}) \\odot \\mathbf{z} = \\mathbf{x} \\odot (\\mathbf{y} \\odot \\mathbf{z})$; (3) Scalar multiplication: $(\\alpha \\mathbf{x}) \\odot \\mathbf{y} = \\alpha(\\mathbf{x} \\odot \\mathbf{y})$",
        "proof_sketch": "Commutativity: $(\\mathbf{x} \\odot \\mathbf{y})_i = x_i y_i = y_i x_i = (\\mathbf{y} \\odot \\mathbf{x})_i$ by commutativity of real number multiplication. Since this holds for all $i$, the vectors are equal.",
        "examples": [
          "Let $\\mathbf{x} = (2, 3, 4)$ and $\\mathbf{y} = (1, 0, 5)$. Then $\\mathbf{x} \\odot \\mathbf{y} = (2 \\cdot 1, 3 \\cdot 0, 4 \\cdot 5) = (2, 0, 20)$",
          "For $\\mathbf{a} = (1, 2)$ and $\\mathbf{b} = (3, 4)$: $\\mathbf{a} \\odot \\mathbf{b} = (1 \\cdot 3, 2 \\cdot 4) = (3, 8)$",
          "The operation is undefined when dimensions don't match: $(1, 2) \\odot (3, 4, 5)$ is not defined"
        ]
      },
      "key_formulas": [
        {
          "name": "Hadamard Product",
          "latex": "$(\\mathbf{x} \\odot \\mathbf{y})_i = x_i \\cdot y_i$",
          "description": "Multiply corresponding components at each position i"
        },
        {
          "name": "Dimension Requirement",
          "latex": "$\\dim(\\mathbf{x}) = \\dim(\\mathbf{y}) = n$",
          "description": "Both vectors must have the same dimension for element-wise multiplication"
        }
      ],
      "exercise": {
        "description": "Implement a function that computes the element-wise product of two vectors. This intermediate result is crucial for computing the dot product in the linear kernel.",
        "function_signature": "def elementwise_multiply(x1: np.ndarray, x2: np.ndarray) -> np.ndarray:",
        "starter_code": "import numpy as np\n\ndef elementwise_multiply(x1: np.ndarray, x2: np.ndarray) -> np.ndarray:\n    # Your code here: return element-wise product\n    # Should return array where result[i] = x1[i] * x2[i]\n    pass",
        "test_cases": [
          {
            "input": "elementwise_multiply(np.array([1, 2, 3]), np.array([4, 5, 6]))",
            "expected": "np.array([4, 10, 18])",
            "explanation": "Component-wise: 1*4=4, 2*5=10, 3*6=18"
          },
          {
            "input": "elementwise_multiply(np.array([2, -3]), np.array([4, 5]))",
            "expected": "np.array([8, -15])",
            "explanation": "Handles negative numbers: 2*4=8, (-3)*5=-15"
          },
          {
            "input": "elementwise_multiply(np.array([0, 1, 0]), np.array([5, 6, 7]))",
            "expected": "np.array([0, 6, 0])",
            "explanation": "Zeros propagate: 0*5=0, 1*6=6, 0*7=0"
          }
        ]
      },
      "common_mistakes": [
        "Using matrix multiplication (*@*) instead of element-wise multiplication",
        "Not validating that both vectors have the same length",
        "Confusing element-wise product with dot product (which returns a scalar)"
      ],
      "hint": "NumPy arrays support element-wise multiplication with the * operator, or you can iterate manually",
      "references": [
        "Hadamard product",
        "NumPy broadcasting",
        "Element-wise operations"
      ]
    },
    {
      "step": 3,
      "title": "Summation and Series Notation",
      "relation_to_problem": "The linear kernel aggregates element-wise products into a single scalar value through summation, which is the final step in computing the dot product.",
      "prerequisites": [
        "Element-wise operations",
        "Array iteration",
        "Basic arithmetic"
      ],
      "learning_objectives": [
        "Understand sigma notation and its role in vector operations",
        "Implement summation algorithms for array elements",
        "Recognize summation properties and their computational implications",
        "Connect mathematical summation to programming loops and reduce operations"
      ],
      "math_content": {
        "definition": "The summation (or sum) of a sequence of numbers $a_1, a_2, \\ldots, a_n$ is denoted using sigma notation: $\\sum_{i=1}^{n} a_i = a_1 + a_2 + \\cdots + a_n$. The variable $i$ is the index of summation, 1 is the lower bound, $n$ is the upper bound, and $a_i$ is the general term.",
        "notation": "$\\sum_{i=1}^{n} a_i$ represents the sum from $i=1$ to $i=n$; equivalent to $\\sum_i a_i$ when bounds are clear from context",
        "theorem": "Linearity of Summation: For sequences $(a_i)$ and $(b_i)$ and scalars $\\alpha, \\beta \\in \\mathbb{R}$: (1) $\\sum_{i=1}^{n} (a_i + b_i) = \\sum_{i=1}^{n} a_i + \\sum_{i=1}^{n} b_i$; (2) $\\sum_{i=1}^{n} \\alpha a_i = \\alpha \\sum_{i=1}^{n} a_i$; (3) $\\sum_{i=1}^{n} (\\alpha a_i + \\beta b_i) = \\alpha \\sum_{i=1}^{n} a_i + \\beta \\sum_{i=1}^{n} b_i$",
        "proof_sketch": "Property (1): By commutativity and associativity of addition, $(a_1 + b_1) + (a_2 + b_2) + \\cdots + (a_n + b_n) = (a_1 + a_2 + \\cdots + a_n) + (b_1 + b_2 + \\cdots + b_n)$. Property (2): Factor out the constant: $\\alpha a_1 + \\alpha a_2 + \\cdots + \\alpha a_n = \\alpha(a_1 + a_2 + \\cdots + a_n)$.",
        "examples": [
          "$\\sum_{i=1}^{3} i^2 = 1^2 + 2^2 + 3^2 = 1 + 4 + 9 = 14$",
          "$\\sum_{i=1}^{4} (2i + 1) = 3 + 5 + 7 + 9 = 24$, or using linearity: $2\\sum_{i=1}^{4} i + \\sum_{i=1}^{4} 1 = 2(10) + 4 = 24$",
          "For array $[4, 10, 18]$: $\\sum_{i=1}^{3} a_i = 4 + 10 + 18 = 32$"
        ]
      },
      "key_formulas": [
        {
          "name": "General Summation",
          "latex": "$S = \\sum_{i=1}^{n} a_i$",
          "description": "Sum all elements from index 1 to n"
        },
        {
          "name": "Constant Factor",
          "latex": "$\\sum_{i=1}^{n} c \\cdot a_i = c \\cdot \\sum_{i=1}^{n} a_i$",
          "description": "Constants can be factored out of summations"
        },
        {
          "name": "Empty Sum",
          "latex": "$\\sum_{i=1}^{0} a_i = 0$",
          "description": "Sum over empty range is defined as zero"
        }
      ],
      "exercise": {
        "description": "Implement a function that computes the sum of all elements in a numerical array. This operation converts the element-wise products into the final scalar kernel value.",
        "function_signature": "def sum_array(arr: np.ndarray) -> float:",
        "starter_code": "import numpy as np\n\ndef sum_array(arr: np.ndarray) -> float:\n    # Your code here: return the sum of all elements\n    pass",
        "test_cases": [
          {
            "input": "sum_array(np.array([4, 10, 18]))",
            "expected": "32",
            "explanation": "4 + 10 + 18 = 32, matching the element-wise products from the kernel example"
          },
          {
            "input": "sum_array(np.array([1, -2, 3, -4]))",
            "expected": "-2",
            "explanation": "1 + (-2) + 3 + (-4) = -2, demonstrating signed arithmetic"
          },
          {
            "input": "sum_array(np.array([5.5, 2.5, 3.0]))",
            "expected": "11.0",
            "explanation": "5.5 + 2.5 + 3.0 = 11.0, works with floating-point numbers"
          },
          {
            "input": "sum_array(np.array([]))",
            "expected": "0",
            "explanation": "Empty array sum is 0 by convention"
          }
        ]
      },
      "common_mistakes": [
        "Forgetting to initialize accumulator to 0 before summing",
        "Not handling empty arrays (should return 0)",
        "Integer overflow for large arrays (less common in Python)",
        "Confusing sum with mean or other aggregate functions"
      ],
      "hint": "You can use a loop with an accumulator, or leverage NumPy's built-in sum function",
      "references": [
        "Sigma notation",
        "Reduction operations",
        "NumPy aggregation functions"
      ]
    },
    {
      "step": 4,
      "title": "Inner Product (Dot Product) Definition",
      "relation_to_problem": "The inner product is the mathematical foundation of the linear kernel, combining element-wise multiplication and summation into a single similarity measure.",
      "prerequisites": [
        "Vector components",
        "Element-wise multiplication",
        "Summation"
      ],
      "learning_objectives": [
        "Define the inner product formally as a bilinear form",
        "Understand geometric interpretation (projection and angle)",
        "Prove key properties: symmetry, linearity, positive definiteness",
        "Recognize the inner product as the linear kernel K(x, y) = ⟨x, y⟩"
      ],
      "math_content": {
        "definition": "The inner product (or dot product) on $\\mathbb{R}^n$ is a function $\\langle \\cdot, \\cdot \\rangle : \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}$ defined by $\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\sum_{i=1}^{n} x_i y_i = x_1 y_1 + x_2 y_2 + \\cdots + x_n y_n$. Alternative notations include $\\mathbf{x} \\cdot \\mathbf{y}$ and $\\mathbf{x}^T \\mathbf{y}$ (matrix notation).",
        "notation": "$\\langle \\mathbf{x}, \\mathbf{y} \\rangle$ or $\\mathbf{x} \\cdot \\mathbf{y}$ denotes the inner product; $\\|\\mathbf{x}\\| = \\sqrt{\\langle \\mathbf{x}, \\mathbf{x} \\rangle}$ is the induced norm",
        "theorem": "Inner Product Properties: For all $\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in \\mathbb{R}^n$ and $\\alpha \\in \\mathbb{R}$: (1) Symmetry: $\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\langle \\mathbf{y}, \\mathbf{x} \\rangle$; (2) Linearity in first argument: $\\langle \\alpha \\mathbf{x} + \\mathbf{y}, \\mathbf{z} \\rangle = \\alpha \\langle \\mathbf{x}, \\mathbf{z} \\rangle + \\langle \\mathbf{y}, \\mathbf{z} \\rangle$; (3) Positive definiteness: $\\langle \\mathbf{x}, \\mathbf{x} \\rangle \\geq 0$ with equality iff $\\mathbf{x} = \\mathbf{0}$",
        "proof_sketch": "Symmetry: $\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\sum_{i=1}^{n} x_i y_i = \\sum_{i=1}^{n} y_i x_i = \\langle \\mathbf{y}, \\mathbf{x} \\rangle$ by commutativity of multiplication. Positive definiteness: $\\langle \\mathbf{x}, \\mathbf{x} \\rangle = \\sum_{i=1}^{n} x_i^2 \\geq 0$ as sum of squares; equals 0 iff all $x_i = 0$.",
        "examples": [
          "$\\langle (1, 2, 3), (4, 5, 6) \\rangle = 1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6 = 4 + 10 + 18 = 32$",
          "$\\langle (1, 0, -1), (2, 3, 2) \\rangle = 1 \\cdot 2 + 0 \\cdot 3 + (-1) \\cdot 2 = 2 + 0 - 2 = 0$ (orthogonal vectors)",
          "$\\langle (3, 4), (3, 4) \\rangle = 3^2 + 4^2 = 25 = \\|\\mathbf{x}\\|^2$ (norm squared)"
        ]
      },
      "key_formulas": [
        {
          "name": "Inner Product Definition",
          "latex": "$\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\sum_{i=1}^{n} x_i y_i$",
          "description": "Sum of element-wise products; the core formula for linear kernel"
        },
        {
          "name": "Geometric Interpretation",
          "latex": "$\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\|\\mathbf{x}\\| \\|\\mathbf{y}\\| \\cos \\theta$",
          "description": "Relates inner product to vector magnitudes and angle θ between them"
        },
        {
          "name": "Cauchy-Schwarz Inequality",
          "latex": "$|\\langle \\mathbf{x}, \\mathbf{y} \\rangle| \\leq \\|\\mathbf{x}\\| \\|\\mathbf{y}\\|$",
          "description": "Fundamental bound on inner product magnitude"
        }
      ],
      "exercise": {
        "description": "Implement a function that computes the inner product (dot product) of two vectors by combining element-wise multiplication and summation. This is the core computation of the linear kernel.",
        "function_signature": "def dot_product(x1: np.ndarray, x2: np.ndarray) -> float:",
        "starter_code": "import numpy as np\n\ndef dot_product(x1: np.ndarray, x2: np.ndarray) -> float:\n    # Your code here: compute the inner product\n    # Use the element-wise multiplication and summation\n    pass",
        "test_cases": [
          {
            "input": "dot_product(np.array([1, 2, 3]), np.array([4, 5, 6]))",
            "expected": "32",
            "explanation": "1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32"
          },
          {
            "input": "dot_product(np.array([1, 0, -1]), np.array([2, 3, 2]))",
            "expected": "0",
            "explanation": "1*2 + 0*3 + (-1)*2 = 2 + 0 - 2 = 0 (orthogonal vectors)"
          },
          {
            "input": "dot_product(np.array([3, 4]), np.array([3, 4]))",
            "expected": "25",
            "explanation": "3*3 + 4*4 = 9 + 16 = 25 (dot product with itself gives squared norm)"
          },
          {
            "input": "dot_product(np.array([-2, 5]), np.array([3, 1]))",
            "expected": "-1",
            "explanation": "(-2)*3 + 5*1 = -6 + 5 = -1 (can be negative)"
          }
        ]
      },
      "common_mistakes": [
        "Returning a vector instead of a scalar (confusing with element-wise product)",
        "Not handling negative values correctly in the product",
        "Attempting dot product on vectors of different dimensions",
        "Mixing up row/column vector conventions in matrix notation"
      ],
      "hint": "Combine your element-wise multiplication with summation, or use NumPy's dot function",
      "references": [
        "Inner product spaces",
        "Euclidean space",
        "Bilinear forms",
        "Vector norms"
      ]
    },
    {
      "step": 5,
      "title": "Kernel Functions and the Linear Kernel",
      "relation_to_problem": "Understanding kernel functions as similarity measures connects the mathematical inner product to its application in machine learning algorithms like SVMs.",
      "prerequisites": [
        "Inner product",
        "Vector spaces",
        "Function notation"
      ],
      "learning_objectives": [
        "Define kernel functions formally as symmetric positive semi-definite functions",
        "Understand the role of kernels in measuring similarity",
        "Recognize the linear kernel as K(x, y) = ⟨x, y⟩",
        "Compare linear kernel with other kernel types (polynomial, RBF)",
        "Understand when linear kernels are appropriate for ML tasks"
      ],
      "math_content": {
        "definition": "A kernel function is a function $K: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ that can be expressed as an inner product in some feature space $\\mathcal{H}$: $K(\\mathbf{x}, \\mathbf{y}) = \\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{y}) \\rangle_{\\mathcal{H}}$ for some feature map $\\phi: \\mathcal{X} \\to \\mathcal{H}$. The linear kernel is defined as $K_{\\text{linear}}(\\mathbf{x}, \\mathbf{y}) = \\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\sum_{i=1}^{n} x_i y_i$, where the feature map is the identity: $\\phi(\\mathbf{x}) = \\mathbf{x}$.",
        "notation": "$K(\\mathbf{x}, \\mathbf{y})$ denotes kernel evaluation; $\\phi: \\mathcal{X} \\to \\mathcal{H}$ is the feature map; $K_{\\text{linear}}, K_{\\text{poly}}, K_{\\text{RBF}}$ denote different kernel types",
        "theorem": "Mercer's Theorem (simplified): A symmetric function $K: \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}$ is a valid kernel if and only if its kernel matrix $\\mathbf{K}$ with entries $K_{ij} = K(\\mathbf{x}_i, \\mathbf{x}_j)$ is positive semi-definite for any finite set of points $\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_m\\}$.",
        "proof_sketch": "For the linear kernel, $K_{\\text{linear}}(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x}^T \\mathbf{y}$. The kernel matrix is $\\mathbf{K} = \\mathbf{X}\\mathbf{X}^T$ where $\\mathbf{X}$ has rows $\\mathbf{x}_i^T$. For any vector $\\mathbf{c}$: $\\mathbf{c}^T \\mathbf{K} \\mathbf{c} = \\mathbf{c}^T \\mathbf{X}\\mathbf{X}^T \\mathbf{c} = \\|\\mathbf{X}^T \\mathbf{c}\\|^2 \\geq 0$, proving positive semi-definiteness.",
        "examples": [
          "Linear kernel: $K_{\\text{linear}}((1,2,3), (4,5,6)) = 1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6 = 32$",
          "Polynomial kernel: $K_{\\text{poly}}(\\mathbf{x}, \\mathbf{y}) = (\\langle \\mathbf{x}, \\mathbf{y} \\rangle + c)^d$ for degree $d$ and constant $c$",
          "RBF (Gaussian) kernel: $K_{\\text{RBF}}(\\mathbf{x}, \\mathbf{y}) = \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{y}\\|^2}{2\\sigma^2}\\right)$ for bandwidth $\\sigma$",
          "Linear kernel on normalized vectors gives cosine similarity: if $\\|\\mathbf{x}\\| = \\|\\mathbf{y}\\| = 1$, then $K(\\mathbf{x}, \\mathbf{y}) = \\cos \\theta$"
        ]
      },
      "key_formulas": [
        {
          "name": "Linear Kernel",
          "latex": "$K_{\\text{linear}}(\\mathbf{x}, \\mathbf{y}) = \\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\sum_{i=1}^{n} x_i y_i$",
          "description": "The simplest kernel; equivalent to inner product in original space"
        },
        {
          "name": "Kernel Symmetry",
          "latex": "$K(\\mathbf{x}, \\mathbf{y}) = K(\\mathbf{y}, \\mathbf{x})$",
          "description": "All valid kernels must be symmetric"
        },
        {
          "name": "Kernel Composition",
          "latex": "$K_3(\\mathbf{x}, \\mathbf{y}) = K_1(\\mathbf{x}, \\mathbf{y}) + K_2(\\mathbf{x}, \\mathbf{y})$",
          "description": "Sum of valid kernels is also a valid kernel"
        }
      ],
      "exercise": {
        "description": "Implement the linear kernel function that takes two vectors and returns their similarity as measured by the inner product. Also verify the kernel's symmetry property.",
        "function_signature": "def linear_kernel(x1: np.ndarray, x2: np.ndarray) -> float:",
        "starter_code": "import numpy as np\n\ndef linear_kernel(x1: np.ndarray, x2: np.ndarray) -> float:\n    # Your code here: implement the linear kernel\n    # K(x1, x2) = <x1, x2> = sum(x1[i] * x2[i])\n    pass",
        "test_cases": [
          {
            "input": "linear_kernel(np.array([1, 2, 3]), np.array([4, 5, 6]))",
            "expected": "32",
            "explanation": "This is the main problem example: 1*4 + 2*5 + 3*6 = 32"
          },
          {
            "input": "linear_kernel(np.array([2, -1]), np.array([3, 4]))",
            "expected": "2",
            "explanation": "2*3 + (-1)*4 = 6 - 4 = 2"
          },
          {
            "input": "linear_kernel(np.array([1, 0]), np.array([0, 1]))",
            "expected": "0",
            "explanation": "Orthogonal vectors have kernel value 0"
          },
          {
            "input": "linear_kernel(np.array([5]), np.array([3]))",
            "expected": "15",
            "explanation": "Works with 1-dimensional vectors: 5*3 = 15"
          }
        ]
      },
      "common_mistakes": [
        "Not validating that both vectors have the same dimension",
        "Implementing a different kernel type (polynomial, RBF) instead of linear",
        "Forgetting that kernel output is always a scalar, not a vector",
        "Confusing linear kernel with linear regression or linear classification"
      ],
      "hint": "The linear kernel IS the dot product - apply what you learned about inner products directly",
      "references": [
        "Kernel methods",
        "Support Vector Machines",
        "Reproducing Kernel Hilbert Spaces",
        "Mercer's theorem"
      ]
    },
    {
      "step": 6,
      "title": "Applications and Computational Considerations",
      "relation_to_problem": "Understanding practical implementation details and edge cases ensures robust kernel function code suitable for real machine learning applications.",
      "prerequisites": [
        "Linear kernel implementation",
        "NumPy arrays",
        "Error handling"
      ],
      "learning_objectives": [
        "Implement dimension validation for kernel inputs",
        "Handle edge cases (zero vectors, single-element vectors)",
        "Understand computational complexity: O(n) for n-dimensional vectors",
        "Recognize applications in SVMs, kernel ridge regression, and similarity-based learning",
        "Compare naive implementation vs optimized NumPy operations"
      ],
      "math_content": {
        "definition": "Computational complexity of the linear kernel: Given vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$, computing $K(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{n} x_i y_i$ requires $n$ multiplications and $n-1$ additions, yielding $O(n)$ time complexity and $O(1)$ space complexity (excluding input storage).",
        "notation": "$\\mathcal{O}(n)$ denotes linear time complexity; $\\mathcal{O}(1)$ denotes constant space complexity",
        "theorem": "Dimension Compatibility: The linear kernel $K(\\mathbf{x}, \\mathbf{y})$ is defined only when $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ for the same $n$. If $\\mathbf{x} \\in \\mathbb{R}^m$ and $\\mathbf{y} \\in \\mathbb{R}^n$ with $m \\neq n$, the kernel is undefined.",
        "proof_sketch": "The summation $\\sum_{i=1}^{n} x_i y_i$ requires both $x_i$ and $y_i$ to exist for each $i$. If dimensions differ, at least one component would be undefined, making the sum invalid.",
        "examples": [
          "Zero vector kernel: $K(\\mathbf{0}, \\mathbf{y}) = \\sum_{i=1}^{n} 0 \\cdot y_i = 0$ for any $\\mathbf{y}$",
          "Identity test: $K(\\mathbf{e}_i, \\mathbf{x}) = x_i$ where $\\mathbf{e}_i$ is the $i$-th standard basis vector",
          "In SVM with linear kernel: decision function is $f(\\mathbf{x}) = \\text{sign}\\left(\\sum_{i=1}^{m} \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b\\right)$",
          "NumPy optimization: np.dot(x, y) uses BLAS libraries for optimized computation vs manual loop"
        ]
      },
      "key_formulas": [
        {
          "name": "Time Complexity",
          "latex": "$T(n) = \\mathcal{O}(n)$",
          "description": "Linear in the dimension of input vectors"
        },
        {
          "name": "Kernel Matrix Element",
          "latex": "$K_{ij} = K(\\mathbf{x}_i, \\mathbf{x}_j)$",
          "description": "Entry in Gram matrix for dataset $\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_m\\}$"
        },
        {
          "name": "Zero Vector Property",
          "latex": "$K(\\mathbf{x}, \\mathbf{0}) = 0$ for all $\\mathbf{x}$",
          "description": "Zero vector is orthogonal to all vectors"
        }
      ],
      "exercise": {
        "description": "Implement a robust linear kernel function with input validation, error handling for dimension mismatches, and consideration for edge cases. This is production-ready code for the main problem.",
        "function_signature": "def robust_linear_kernel(x1: np.ndarray, x2: np.ndarray) -> float:",
        "starter_code": "import numpy as np\n\ndef robust_linear_kernel(x1: np.ndarray, x2: np.ndarray) -> float:\n    # Your code here: implement linear kernel with validation\n    # 1. Check that x1 and x2 have same dimension\n    # 2. Handle edge cases (empty, zero vectors)\n    # 3. Compute and return the inner product\n    pass",
        "test_cases": [
          {
            "input": "robust_linear_kernel(np.array([1, 2, 3]), np.array([4, 5, 6]))",
            "expected": "32",
            "explanation": "Standard case: 1*4 + 2*5 + 3*6 = 32"
          },
          {
            "input": "robust_linear_kernel(np.array([0, 0, 0]), np.array([1, 2, 3]))",
            "expected": "0",
            "explanation": "Zero vector produces zero kernel value"
          },
          {
            "input": "robust_linear_kernel(np.array([2.5, 3.7]), np.array([1.2, 4.1]))",
            "expected": "18.17",
            "explanation": "Floating-point: 2.5*1.2 + 3.7*4.1 = 3.0 + 15.17 = 18.17"
          },
          {
            "input": "robust_linear_kernel(np.array([100]), np.array([-50]))",
            "expected": "-5000",
            "explanation": "Single-element vectors: 100*(-50) = -5000"
          }
        ]
      },
      "common_mistakes": [
        "Not checking dimension compatibility before computation",
        "Not handling floating-point precision in comparisons",
        "Inefficient implementation (manual loops instead of vectorized operations)",
        "Not considering that kernel values can be negative",
        "Attempting to modify input vectors during computation"
      ],
      "hint": "Use NumPy's shape attribute to check dimensions, and np.dot() for efficient computation",
      "references": [
        "NumPy documentation",
        "SVM implementation",
        "Kernel methods in practice",
        "BLAS optimization"
      ]
    }
  ]
}