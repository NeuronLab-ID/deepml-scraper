{
  "mdx_file": "d987eb1f-4827-4701-905d-8d61ba6009b3.mdx",
  "tinygrad_starter_code": "ZnJvbSB0aW55Z3JhZC50ZW5zb3IgaW1wb3J0IFRlbnNvcgoKZGVmIGxpbmVhcl9yZWdyZXNzaW9uX2dyYWRpZW50X2Rlc2NlbnRfdGcoWCwgeSwgYWxwaGEsIGl0ZXJhdGlvbnMpIC0+IFRlbnNvcjoKICAgICIiIgogICAgU29sdmUgbGluZWFyIHJlZ3Jlc3Npb24gdmlhIGdyYWRpZW50IGRlc2NlbnQgdXNpbmcgdGlueWdyYWQgYXV0b2dyYWQuCiAgICBYOiBUZW5zb3Igb3IgY29udmVydGlibGUgc2hhcGUgKG0sbik7IHk6IHNoYXBlIChtLCkgb3IgKG0sMSkuCiAgICBhbHBoYTogbGVhcm5pbmcgcmF0ZTsgaXRlcmF0aW9uczogbnVtYmVyIG9mIHN0ZXBzLgogICAgUmV0dXJucyBhIDEtRCBUZW5zb3Igb2YgbGVuZ3RoIG4sIHJvdW5kZWQgdG8gNCBkZWNpbWFscy4KICAgICIiIgogICAgWF90ID0gVGVuc29yKFgpLmZsb2F0KCkKICAgIHlfdCA9IFRlbnNvcih5KS5mbG9hdCgpLnJlc2hhcGUoLTEsMSkKICAgIG0sIG4gPSBYX3Quc2hhcGUKICAgIHRoZXRhID0gVGVuc29yKFtbMC4wXSBmb3IgXyBpbiByYW5nZShuKV0pCiAgICAjIFlvdXIgaW1wbGVtZW50YXRpb24gaGVyZQogICAgcGFzcwo=",
  "test_cases": [
    {
      "test": "print(linear_regression_gradient_descent(np.array([[1, 1], [1, 2], [1, 3]]), np.array([1, 2, 3]), 0.01, 1000))",
      "expected_output": "[0.1107, 0.9513]"
    },
    {
      "test": "print(linear_regression_gradient_descent(np.array([[1, 5], [1, 7], [1, 9]]), np.array([10, 14, 18]), 0.01, 5000))",
      "expected_output": "[0.0211, 1.9971]"
    }
  ],
  "pytorch_difficulty": "medium",
  "likes": "0",
  "video": "https://youtu.be/rHJ-DfFvpkQ",
  "cuda_test_cases": [
    {
      "test": "#include <iostream>\n#include <vector>\nstd::vector<float> linear_regression_gd(const std::vector<std::vector<float>>& X, const std::vector<float>& y, float alpha, int iterations);\nint main() { auto r=linear_regression_gd({{1,1},{1,2},{1,3}},{1,2,3},0.01f,1000); std::cout<<\"[\"<<r[0]<<\", \"<<r[1]<<\"]\"<<std::endl; return 0; }",
      "expected_output": "[0.1107, 0.9513]"
    },
    {
      "test": "#include <iostream>\n#include <vector>\nstd::vector<float> linear_regression_gd(const std::vector<std::vector<float>>& X, const std::vector<float>& y, float alpha, int iterations);\nint main() { auto r=linear_regression_gd({{1,5},{1,7},{1,9}},{10,14,18},0.01f,5000); std::cout<<\"[\"<<r[0]<<\", \"<<r[1]<<\"]\"<<std::endl; return 0; }",
      "expected_output": "[0.0211, 1.9971]"
    }
  ],
  "example": {
    "input": "X = np.array([[1, 1], [1, 2], [1, 3]]), y = np.array([1, 2, 3]), alpha = 0.01, iterations = 1000",
    "output": "np.array([0.1107, 0.9513])",
    "reasoning": "The linear model is y = 0.0 + 1.0*x, which fits the input data after gradient descent optimization."
  },
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpkZWYgbGluZWFyX3JlZ3Jlc3Npb25fZ3JhZGllbnRfZGVzY2VudChYLCB5LCBhbHBoYSwgaXRlcmF0aW9ucykgLT4gdG9yY2guVGVuc29yOgogICAgIiIiCiAgICBTb2x2ZSBsaW5lYXIgcmVncmVzc2lvbiB2aWEgZ3JhZGllbnQgZGVzY2VudCB1c2luZyBQeVRvcmNoIGF1dG9ncmFkLgogICAgWDogVGVuc29yIG9yIGNvbnZlcnRpYmxlIHNoYXBlIChtLG4pOyB5OiBzaGFwZSAobSwpIG9yIChtLDEpLgogICAgYWxwaGE6IGxlYXJuaW5nIHJhdGU7IGl0ZXJhdGlvbnM6IG51bWJlciBvZiBzdGVwcy4KICAgIFJldHVybnMgYSAxLUQgdGVuc29yIG9mIGxlbmd0aCBuLCByb3VuZGVkIHRvIDQgZGVjaW1hbHMuCiAgICAiIiIKICAgIFhfdCA9IHRvcmNoLmFzX3RlbnNvcihYLCBkdHlwZT10b3JjaC5mbG9hdCkKICAgIHlfdCA9IHRvcmNoLmFzX3RlbnNvcih5LCBkdHlwZT10b3JjaC5mbG9hdCkucmVzaGFwZSgtMSwxKQogICAgbSwgbiA9IFhfdC5zaGFwZQogICAgdGhldGEgPSB0b3JjaC56ZXJvcygobiwxKSwgcmVxdWlyZXNfZ3JhZD1UcnVlKQogICAgIyBZb3VyIGltcGxlbWVudGF0aW9uIGhlcmUKICAgIHBhc3M=",
  "learn_section": "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",
  "cuda_starter_code": "I2luY2x1ZGUgPGN1ZGFfcnVudGltZS5oPgojaW5jbHVkZSA8aW9zdHJlYW0+CiNpbmNsdWRlIDx2ZWN0b3I+CiNpbmNsdWRlIDxjbWF0aD4KCnN0ZDo6dmVjdG9yPGZsb2F0PiBsaW5lYXJfcmVncmVzc2lvbl9nZChjb25zdCBzdGQ6OnZlY3RvcjxzdGQ6OnZlY3RvcjxmbG9hdD4+JiBYLCBjb25zdCBzdGQ6OnZlY3RvcjxmbG9hdD4mIHksIGZsb2F0IGFscGhhLCBpbnQgaXRlcmF0aW9ucyk7",
  "tinygrad_test_cases": [
    {
      "test": "from tinygrad.tensor import Tensor\nres = linear_regression_gradient_descent_tg(\n    [[1.0,0.0],[0.0,1.0]],\n    [5.0,3.0],\n    0.1,\n    10\n)\nprint(res.numpy().tolist())",
      "expected_output": "[5.0, 3.0]"
    },
    {
      "test": "from tinygrad.tensor import Tensor\nX = [[1.0,1.0],[1.0,2.0],[1.0,3.0]]\ny = [1.0,2.0,3.0]\nres = linear_regression_gradient_descent_tg(X, y, 0.01, 1000)\nprint(res.numpy().tolist())",
      "expected_output": "[0.0, 1.0]"
    }
  ],
  "contributor": [
    {
      "profile_link": "https://github.com/Selbl",
      "name": "Selbl"
    },
    {
      "profile_link": "https://www.youtube.com/@StoatScript/videos",
      "name": "StoatScript"
    },
    {
      "profile_link": "https://github.com/ana-baltaretu",
      "name": "anisca22"
    }
  ],
  "description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBwZXJmb3JtcyBsaW5lYXIgcmVncmVzc2lvbiB1c2luZyBncmFkaWVudCBkZXNjZW50LiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgTnVtUHkgYXJyYXlzIFggKGZlYXR1cmVzIHdpdGggYSBjb2x1bW4gb2Ygb25lcyBmb3IgdGhlIGludGVyY2VwdCkgYW5kIHkgKHRhcmdldCkgYXMgaW5wdXQsIGFsb25nIHdpdGggbGVhcm5pbmcgcmF0ZSBhbHBoYSBhbmQgdGhlIG51bWJlciBvZiBpdGVyYXRpb25zLCBhbmQgcmV0dXJuIHRoZSBjb2VmZmljaWVudHMgb2YgdGhlIGxpbmVhciByZWdyZXNzaW9uIG1vZGVsIGFzIGEgTnVtUHkgYXJyYXkuIFJvdW5kIHlvdXIgYW5zd2VyIHRvIGZvdXIgZGVjaW1hbCBwbGFjZXMuIC0wLjAgaXMgYSB2YWxpZCByZXN1bHQgZm9yIHJvdW5kaW5nIGEgdmVyeSBzbWFsbCBudW1iZXIu",
  "id": "15",
  "tinygrad_difficulty": "medium",
  "difficulty": "easy",
  "cuda_difficulty": "hard",
  "dislikes": "0",
  "category": "Machine Learning",
  "starter_code": "import numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n\t\"\"\"\n\tPerform linear regression using gradient descent.\n\n\tm = number of training examples\n\tn = number of parameters (features), technically n-1 features, 1st column is for intercept\n\n\tX: shape (m, n), `m` training examples with `n` input values for each feature\n\ty: shape (m, 1) array with the target values (ground truth)\n\talpha: learning rate\n\titerations: number of gradient descent steps\n\t\"\"\"\n\n\tm, n = X.shape\n\ty = y.reshape(-1, 1) \t# Make sure y is a column vector\n\ttheta = np.zeros((n, 1))\n\n\t# TODO: Your code here\n\n\treturn np.round(theta.flatten(), 4) \t# Rounded to 4 decimals",
  "title": "Linear Regression Using Gradient Descent",
  "createdAt": "November 10, 2025 at 10:07:27â€¯AM UTC-0500",
  "pytorch_test_cases": [
    {
      "test": "import torch\nres = linear_regression_gradient_descent(\n    torch.eye(2),\n    torch.tensor([5.0, 3.0]),\n    0.1,\n    10\n)\nprint(res.detach().numpy().tolist())",
      "expected_output": "[3.2565999031066895, 1.9539999961853027]"
    },
    {
      "test": "import torch\nX = torch.tensor([[1.0,1.0],[1.0,2.0],[1.0,3.0]])\ny = torch.tensor([1.0,2.0,3.0])\nres = linear_regression_gradient_descent(X, y, 0.01, 1000)\nprint(res.detach().numpy().tolist())",
      "expected_output": "[0.03319999948143959, 0.9854000210762024]"
    }
  ],
  "description_decoded": "Write a Python function that performs linear regression using gradient descent. The function should take NumPy arrays X (features with a column of ones for the intercept) and y (target) as input, along with learning rate alpha and the number of iterations, and return the coefficients of the linear regression model as a NumPy array. Round your answer to four decimal places. -0.0 is a valid result for rounding a very small number.",
  "learn_section_decoded": "\n## Linear Regression Using Gradient Descent\n\nLinear regression can also be performed using a technique called gradient descent, where the coefficients (or weights) of the model are iteratively adjusted to minimize a cost function (usually mean squared error). This method is particularly useful when the number of features is too large for analytical solutions like the normal equation or when the feature matrix is not invertible.\n\nThe gradient descent algorithm updates the weights by moving in the direction of the negative gradient of the cost function with respect to the weights. The updates occur iteratively until the algorithm converges to a minimum of the cost function.\n\nThe update rule for each weight is given by:\n$$\n\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right)x_j^{(i)}\n$$\n\n### Explanation of Terms\n1. $\\alpha$ is the learning rate.\n2. $m$ is the number of training examples.\n3. $h_{\\theta}(x^{(i)})$ is the hypothesis function at iteration $i$.\n4. $x^{(i)}$ is the feature vector of the $i^{\\text{th}}$ training example.\n5. $y^{(i)}$ is the actual target value for the $i^{\\text{th}}$ training example.\n6. $x_j^{(i)}$ is the value of feature $j$ for the $i^{\\text{th}}$ training example.\n\n### Key Points\n- **Learning Rate**: The choice of learning rate is crucial for the convergence and performance of gradient descent. \n  - A small learning rate may lead to slow convergence.\n  - A large learning rate may cause overshooting and divergence.\n- **Number of Iterations**: The number of iterations determines how long the algorithm runs before it converges or stops.\n\n### Practical Implementation\nImplementing gradient descent involves:\n1. Initializing the weights theta ($\\theta$), with 0s in our case, \n2. Compute the predicted values for all training examples using the current weights\n3. Computing the gradient for each iteration step by aggregating the errors over all training examples and scaling it by the learning rate,\n4. and iteratively updating the weights according to the update rule.",
  "tinygrad_starter_code_decoded": "from tinygrad.tensor import Tensor\n\ndef linear_regression_gradient_descent_tg(X, y, alpha, iterations) -> Tensor:\n    \"\"\"\n    Solve linear regression via gradient descent using tinygrad autograd.\n    X: Tensor or convertible shape (m,n); y: shape (m,) or (m,1).\n    alpha: learning rate; iterations: number of steps.\n    Returns a 1-D Tensor of length n, rounded to 4 decimals.\n    \"\"\"\n    X_t = Tensor(X).float()\n    y_t = Tensor(y).float().reshape(-1,1)\n    m, n = X_t.shape\n    theta = Tensor([[0.0] for _ in range(n)])\n    # Your implementation here\n    pass\n"
}