{
  "description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gY29tcHV0ZSB0aGUgZGVyaXZhdGl2ZSBvZiBjb21wb3NpdGUgZnVuY3Rpb25zIHVzaW5nIHRoZSBjaGFpbiBydWxlLiBHaXZlbiBhIGxpc3Qgb2YgZnVuY3Rpb25zIChhcHBsaWVkIHJpZ2h0IHRvIGxlZnQpIGFuZCBhIHBvaW50IHgsIGNhbGN1bGF0ZSB0aGUgZGVyaXZhdGl2ZSBhdCB0aGF0IHBvaW50LiBBdmFpbGFibGUgZnVuY3Rpb25zOiAnc3F1YXJlJyAoeMKyKSwgJ3NpbicsICdleHAnLCAnbG9nJy4gVGhlIGNoYWluIHJ1bGUgc3RhdGVzIHRoYXQgZm9yIGgoeCkgPSBmKGcoeCkpLCB0aGUgZGVyaXZhdGl2ZSBpcyBoJyh4KSA9IGYnKGcoeCkpIMK3IGcnKHgpLg==",
  "id": "214",
  "test_cases": [
    {
      "test": "result = compute_chain_rule_gradient(['sin', 'square'], 1.0); print(f\"{result:.6f}\")",
      "expected_output": "1.080605"
    },
    {
      "test": "result = compute_chain_rule_gradient(['exp', 'sin', 'square'], 0.5); print(f\"{result:.6f}\")",
      "expected_output": "1.240883"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "functions=['sin', 'square'], x=1.0",
    "output": "1.080605",
    "reasoning": "For h(x) = sin(x²): Inner function g(x) = x², outer f(u) = sin(u). At x=1: g(1)=1, g'(x)=2x so g'(1)=2. f'(u)=cos(u) so f'(1)=cos(1)≈0.540. Chain rule: h'(1) = f'(g(1))·g'(1) = cos(1)·2 ≈ 1.0806."
  },
  "category": "Calculus",
  "starter_code": "import numpy as np\n\ndef compute_chain_rule_gradient(functions: list[str], x: float) -> float:\n\t\"\"\"\n\tCompute derivative of composite functions using chain rule.\n\t\n\tArgs:\n\t\tfunctions: List of function names (applied right to left)\n\t\t          Available: 'square', 'sin', 'exp', 'log'\n\t\tx: Point at which to evaluate derivative\n\t\n\tReturns:\n\t\tDerivative value at x\n\t\n\tExample:\n\t\t['sin', 'square'] represents sin(x²)\n\t\t['exp', 'sin', 'square'] represents exp(sin(x²))\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Chain Rule for Composite Functions",
  "createdAt": "November 24, 2025 at 5:59:37 PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement a function to compute the derivative of composite functions using the chain rule. Given a list of functions (applied right to left) and a point x, calculate the derivative at that point. Available functions: 'square' (x²), 'sin', 'exp', 'log'. The chain rule states that for h(x) = f(g(x)), the derivative is h'(x) = f'(g(x)) · g'(x).",
  "learn_section_decoded": "### The Chain Rule\n\nThe chain rule computes derivatives of composite functions by multiplying derivatives of each layer.\n\n#### Basic Chain Rule\n\nFor $h(x) = f(g(x))$:\n$$\nh'(x) = f'(g(x)) \\cdot g'(x)\n$$\n\n**In words**: Derivative of outer function (evaluated at inner function) times derivative of inner function.\n\n#### Intuition\n\nThink of **rate of change propagation**:\n- How fast does $h$ change with respect to $g$? That's $f'(g)$\n- How fast does $g$ change with respect to $x$? That's $g'(x)$\n- Combined rate: multiply them\n\n**Example**: If temperature changes 2°C per hour, and a thermostat reading changes 0.5 units per °C, then the reading changes $2 \\times 0.5 = 1$ unit per hour.\n\n#### Standard Example\n\n**Function**: $h(x) = \\sin(x^2)$\n\n**Decompose**:\n- Inner: $g(x) = x^2$\n- Outer: $f(u) = \\sin(u)$\n- Composition: $h(x) = f(g(x))$\n\n**Derivatives**:\n- $g'(x) = 2x$\n- $f'(u) = \\cos(u)$\n\n**Chain rule**:\n$$\nh'(x) = f'(g(x)) \\cdot g'(x) = \\cos(x^2) \\cdot 2x\n$$\n\n**At $x=1$**:\n$$\nh'(1) = \\cos(1) \\cdot 2 = 0.540 \\cdot 2 = 1.081\n$$\n\n#### Multiple Compositions\n\nFor $h(x) = f_3(f_2(f_1(x)))$:\n$$\nh'(x) = f_3'(f_2(f_1(x))) \\cdot f_2'(f_1(x)) \\cdot f_1'(x)\n$$\n\n**Pattern**: Work from outside to inside, multiply all derivatives.\n\n#### Algorithm\n\n**Input**: Composition $f_n(f_{n-1}(...f_1(x)))$, point $x$\n\n**Step 1**: Forward pass - compute intermediate values\n$$\n\\begin{align}\nu_0 &= x \\\\\nu_1 &= f_1(u_0) \\\\\nu_2 &= f_2(u_1) \\\\\n&\\vdots \\\\\nu_n &= f_n(u_{n-1})\n\\end{align}\n$$\n\n**Step 2**: Backward pass - multiply derivatives\n$$\n\\frac{dh}{dx} = \\prod_{i=1}^{n} f_i'(u_{i-1})\n$$\n\n**Output**: Product of all derivatives\n\n#### Detailed Example\n\n**Function**: $h(x) = e^{\\sin(x^2)}$\n\n**Decomposition**:\n- $u_1 = x^2$\n- $u_2 = \\sin(u_1)$\n- $h = e^{u_2}$\n\n**At $x = 0.5$**:\n\n**Forward pass**:\n$$\n\\begin{align}\nu_1 &= (0.5)^2 = 0.25 \\\\\nu_2 &= \\sin(0.25) = 0.2474 \\\\\nh &= e^{0.2474} = 1.2807\n\\end{align}\n$$\n\n**Derivatives**:\n$$\n\\begin{align}\n\\frac{du_1}{dx} &= 2x = 1.0 \\\\\n\\frac{du_2}{du_1} &= \\cos(u_1) = \\cos(0.25) = 0.9689 \\\\\n\\frac{dh}{du_2} &= e^{u_2} = e^{0.2474} = 1.2807\n\\end{align}\n$$\n\n**Chain rule**:\n$$\n\\frac{dh}{dx} = \\frac{dh}{du_2} \\cdot \\frac{du_2}{du_1} \\cdot \\frac{du_1}{dx} = 1.2807 \\times 0.9689 \\times 1.0 = 1.2409\n$$\n\n#### Common Functions and Derivatives\n\n**Power**: $f(x) = x^2 \\Rightarrow f'(x) = 2x$\n\n**Exponential**: $f(x) = e^x \\Rightarrow f'(x) = e^x$\n\n**Logarithm**: $f(x) = \\ln(x) \\Rightarrow f'(x) = \\frac{1}{x}$\n\n**Sine**: $f(x) = \\sin(x) \\Rightarrow f'(x) = \\cos(x)$\n\n**Cosine**: $f(x) = \\cos(x) \\Rightarrow f'(x) = -\\sin(x)$\n\n#### Special Cases\n\n**Identity composition**: $\\ln(e^x)$\n- $g(x) = e^x$, $g'(x) = e^x$\n- $f(u) = \\ln(u)$, $f'(u) = 1/u$\n- $h'(x) = \\frac{1}{e^x} \\cdot e^x = 1$ ✓\n\n**Nested squares**: $(x^2)^2 = x^4$\n- Direct: $(x^4)' = 4x^3$\n- Chain rule: $2x^2 \\cdot 2x = 4x^3$ ✓\n\n#### Leibniz Notation\n\nChain rule in Leibniz notation:\n$$\n\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n$$\n\nFor multiple variables:\n$$\n\\frac{dy}{dx} = \\frac{dy}{dv} \\cdot \\frac{dv}{du} \\cdot \\frac{du}{dx}\n$$\n\nThe $du$ terms \"cancel\" (intuition, not rigorous).\n\n#### Connection to Backpropagation\n\nIn neural networks, chain rule computes gradients through layers:\n\n**Forward**: $z = \\sigma(Wx + b)$, $L = \\ell(z, y)$\n\n**Backward**: \n$$\n\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n$$\n\nThis is the chain rule! Each layer multiplies gradients backward.\n\n**Deep network**: $L = f_n(...f_2(f_1(x)))$\n$$\n\\frac{\\partial L}{\\partial x} = \\frac{\\partial f_n}{\\partial f_{n-1}} \\cdots \\frac{\\partial f_2}{\\partial f_1} \\cdot \\frac{\\partial f_1}{\\partial x}\n$$\n\n#### Key Properties\n\n**1. Associativity**: Can compute left-to-right or right-to-left\n\n**2. Local computation**: Each layer only needs its own derivative\n\n**3. Efficiency**: One forward pass, one backward pass\n\n**4. Automatic differentiation**: Computers implement this algorithmically\n\n#### Applications\n\n- **Neural network training**: Backpropagation\n- **Optimization**: Computing gradients for gradient descent\n- **Physics**: Related rates problems\n- **Economics**: Marginal analysis with composite functions"
}