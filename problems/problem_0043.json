{
  "description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gYHJpZGdlX2xvc3NgIHRoYXQgaW1wbGVtZW50cyB0aGUgUmlkZ2UgUmVncmVzc2lvbiBsb3NzIGZ1bmN0aW9uLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSAyRCBudW1weSBhcnJheSBgWGAgcmVwcmVzZW50aW5nIHRoZSBmZWF0dXJlIG1hdHJpeCwgYSAxRCBudW1weSBhcnJheSBgd2AgcmVwcmVzZW50aW5nIHRoZSBjb2VmZmljaWVudHMsIGEgMUQgbnVtcHkgYXJyYXkgYHlfdHJ1ZWAgcmVwcmVzZW50aW5nIHRoZSB0cnVlIGxhYmVscywgYW5kIGEgZmxvYXQgYGFscGhhYCByZXByZXNlbnRpbmcgdGhlIHJlZ3VsYXJpemF0aW9uIHBhcmFtZXRlci4gVGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gdGhlIFJpZGdlIGxvc3MsIHdoaWNoIGNvbWJpbmVzIHRoZSBNZWFuIFNxdWFyZWQgRXJyb3IgKE1TRSkgYW5kIGEgcmVndWxhcml6YXRpb24gdGVybS4=",
  "mdx_file": "42621736-0bbf-47c8-b46e-2ba5edbc4f75.mdx",
  "test_cases": [
    {
      "test": "X = np.array([[1,1],[2,1],[3,1],[4,1]])\nW = np.array([.2,2])\ny = np.array([2,3,4,5])\nalpha = 0.1\noutput = ridge_loss(X, W, y, alpha)\nprint(output)",
      "expected_output": "2.204"
    },
    {
      "test": "X = np.array([[1,1,4],[2,1,2],[3,1,.1],[4,1,1.2],[1,2,3]])\nW = np.array([.2,2,5])\ny = np.array([2,3,4,5,2])\nalpha = 0.1\noutput = ridge_loss(X, W, y, alpha)\nprint(output)",
      "expected_output": "164.402"
    }
  ],
  "difficulty": "easy",
  "pytorch_difficulty": "easy",
  "video": "",
  "likes": "0",
  "marimo_link": "https://open-deep-ml.github.io/DML-OpenProblem/problem-43/index.html",
  "example": {
    "input": "import numpy as np\n\nX = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\nw = np.array([0.2, 2])\ny_true = np.array([2, 3, 4, 5])\nalpha = 0.1\n\nloss = ridge_loss(X, w, y_true, alpha)\nprint(loss)",
    "output": "2.204",
    "reasoning": "The Ridge loss is calculated using the Mean Squared Error (MSE) and a regularization term. The output represents the combined loss value."
  },
  "dislikes": "0",
  "category": "Machine Learning",
  "starter_code": "import numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n\t# Your code here\n\tpass\n",
  "title": "Implement Ridge Regression Loss Function",
  "learn_section": "CiMjIFJpZGdlIFJlZ3Jlc3Npb24gTG9zcwoKUmlkZ2UgUmVncmVzc2lvbiBpcyBhIGxpbmVhciByZWdyZXNzaW9uIG1ldGhvZCB3aXRoIGEgcmVndWxhcml6YXRpb24gdGVybSB0byBwcmV2ZW50IG92ZXJmaXR0aW5nIGJ5IGNvbnRyb2xsaW5nIHRoZSBzaXplIG9mIHRoZSBjb2VmZmljaWVudHMuCgojIyMgS2V5IENvbmNlcHRzOgoxLiAqKlJlZ3VsYXJpemF0aW9uKio6ICAKICAgQWRkcyBhIHBlbmFsdHkgdG8gdGhlIGxvc3MgZnVuY3Rpb24gdG8gZGlzY291cmFnZSBsYXJnZSBjb2VmZmljaWVudHMsIGhlbHBpbmcgdG8gZ2VuZXJhbGl6ZSB0aGUgbW9kZWwuCgoyLiAqKk1lYW4gU3F1YXJlZCBFcnJvciAoTVNFKSoqOiAgCiAgIE1lYXN1cmVzIHRoZSBhdmVyYWdlIHNxdWFyZWQgZGlmZmVyZW5jZSBiZXR3ZWVuIGFjdHVhbCBhbmQgcHJlZGljdGVkIHZhbHVlcy4KCjMuICoqUGVuYWx0eSBUZXJtKio6ICAKICAgVGhlIHN1bSBvZiB0aGUgc3F1YXJlZCBjb2VmZmljaWVudHMsIHNjYWxlZCBieSB0aGUgcmVndWxhcml6YXRpb24gcGFyYW1ldGVyICQgXGxhbWJkYSAkLCB3aGljaCBjb250cm9scyB0aGUgc3RyZW5ndGggb2YgdGhlIHJlZ3VsYXJpemF0aW9uLgoKIyMjIFJpZGdlIExvc3MgRnVuY3Rpb24KVGhlIFJpZGdlIExvc3MgZnVuY3Rpb24gY29tYmluZXMgTVNFIGFuZCB0aGUgcGVuYWx0eSB0ZXJtOgokJApMKFxiZXRhKSA9IFxmcmFjezF9e259IFxzdW1fe2k9MX1ee259ICh5X2kgLSBcaGF0e3l9X2kpXjIgKyBcbGFtYmRhIFxzdW1fe2o9MX1ee3B9IFxiZXRhX2peMgokJAoKIyMjIEltcGxlbWVudGF0aW9uIFN0ZXBzOgoxLiAqKkNhbGN1bGF0ZSBNU0UqKjogIAogICBDb21wdXRlIHRoZSBhdmVyYWdlIHNxdWFyZWQgZGlmZmVyZW5jZSBiZXR3ZWVuIGFjdHVhbCBhbmQgcHJlZGljdGVkIHZhbHVlcy4KCjIuICoqQWRkIFJlZ3VsYXJpemF0aW9uIFRlcm0qKjogIAogICBDb21wdXRlIHRoZSBzdW0gb2Ygc3F1YXJlZCBjb2VmZmljaWVudHMgbXVsdGlwbGllZCBieSAkIFxsYW1iZGEgJC4KCjMuICoqQ29tYmluZSBhbmQgTWluaW1pemUqKjogIAogICBTdW0gTVNFIGFuZCB0aGUgcmVndWxhcml6YXRpb24gdGVybSB0byBmb3JtIHRoZSBSaWRnZSBsb3NzLCB0aGVuIG1pbmltaXplIHRoaXMgbG9zcyB0byBmaW5kIHRoZSBvcHRpbWFsIGNvZWZmaWNpZW50cy4KCg==",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "import torch\nX = torch.tensor([[1.0, 1.0], [2.0, 1.0], [3.0, 1.0], [4.0, 1.0]])\nw = torch.tensor([0.2, 2.0])\ny_true = torch.tensor([2.0, 3.0, 4.0, 5.0])\nalpha = 0.1\noutput = ridge_loss(X, w, y_true, alpha)\nprint(round(float(output), 3))",
      "expected_output": "2.204"
    },
    {
      "test": "import torch\nX = torch.tensor([[1.0, 1.0, 4.0], [2.0, 1.0, 2.0], [3.0, 1.0, 0.1], [4.0, 1.0, 1.2], [1.0, 2.0, 3.0]])\nw = torch.tensor([0.2, 2.0, 5.0])\ny_true = torch.tensor([2.0, 3.0, 4.0, 5.0, 2.0])\nalpha = 0.1\noutput = ridge_loss(X, w, y_true, alpha)\nprint(round(float(output), 3))",
      "expected_output": "164.402"
    },
    {
      "test": "import torch\nX = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nw = torch.tensor([1.0, 1.0])\ny_true = torch.tensor([3.0, 7.0, 11.0])\nalpha = 0.5\noutput = ridge_loss(X, w, y_true, alpha)\nprint(round(float(output), 1))",
      "expected_output": "1.0"
    }
  ],
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpkZWYgcmlkZ2VfbG9zcyhYOiB0b3JjaC5UZW5zb3IsIHc6IHRvcmNoLlRlbnNvciwgeV90cnVlOiB0b3JjaC5UZW5zb3IsIGFscGhhOiBmbG9hdCkgLT4gdG9yY2guVGVuc29yOgogICAgIiIiCiAgICBJbXBsZW1lbnRzIHRoZSBSaWRnZSBSZWdyZXNzaW9uIExvc3MgRnVuY3Rpb24gdXNpbmcgUHlUb3JjaC4KICAgIAogICAgQXJnczoKICAgICAgICBYOiBGZWF0dXJlIG1hdHJpeCBvZiBzaGFwZSAobl9zYW1wbGVzLCBuX2ZlYXR1cmVzKQogICAgICAgIHc6IFdlaWdodCB2ZWN0b3Igb2Ygc2hhcGUgKG5fZmVhdHVyZXMsKQogICAgICAgIHlfdHJ1ZTogVHJ1ZSB0YXJnZXQgdmFsdWVzIG9mIHNoYXBlIChuX3NhbXBsZXMsKQogICAgICAgIGFscGhhOiBSZWd1bGFyaXphdGlvbiBwYXJhbWV0ZXIgKGxhbWJkYSkKICAgIAogICAgUmV0dXJuczoKICAgICAgICBUaGUgUmlkZ2UgbG9zcyB2YWx1ZSBhcyBhIHNjYWxhciB0ZW5zb3IKICAgICIiIgogICAgIyBZb3VyIGltcGxlbWVudGF0aW9uIGhlcmUKICAgIHBhc3MK",
  "description_decoded": "Write a Python function `ridge_loss` that implements the Ridge Regression loss function. The function should take a 2D numpy array `X` representing the feature matrix, a 1D numpy array `w` representing the coefficients, a 1D numpy array `y_true` representing the true labels, and a float `alpha` representing the regularization parameter. The function should return the Ridge loss, which combines the Mean Squared Error (MSE) and a regularization term.",
  "learn_section_decoded": "\n## Ridge Regression Loss\n\nRidge Regression is a linear regression method with a regularization term to prevent overfitting by controlling the size of the coefficients.\n\n### Key Concepts:\n1. **Regularization**:  \n   Adds a penalty to the loss function to discourage large coefficients, helping to generalize the model.\n\n2. **Mean Squared Error (MSE)**:  \n   Measures the average squared difference between actual and predicted values.\n\n3. **Penalty Term**:  \n   The sum of the squared coefficients, scaled by the regularization parameter $ \\lambda $, which controls the strength of the regularization.\n\n### Ridge Loss Function\nThe Ridge Loss function combines MSE and the penalty term:\n$$\nL(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n$$\n\n### Implementation Steps:\n1. **Calculate MSE**:  \n   Compute the average squared difference between actual and predicted values.\n\n2. **Add Regularization Term**:  \n   Compute the sum of squared coefficients multiplied by $ \\lambda $.\n\n3. **Combine and Minimize**:  \n   Sum MSE and the regularization term to form the Ridge loss, then minimize this loss to find the optimal coefficients.\n\n"
}