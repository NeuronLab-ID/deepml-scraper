{
  "description": "SW1wbGVtZW50IHRoZSBidWRnZXQtY29uc3RyYWluZWQgcmVpbmZvcmNlbWVudCBsZWFybmluZyBsb3NzIGZ1bmN0aW9uIGZyb20gdGhlIEtpbWkgSzIgcGFwZXIuIEluIEsyJ3MgUkwgdHJhaW5pbmcsIGEgJ0J1ZGdldCBDb250cm9sJyBtZWNoYW5pc20gcGVuYWxpemVzIHJlc3BvbnNlcyB0aGF0IGV4Y2VlZCBhIHRva2VuIGJ1ZGdldCB0byBpbXByb3ZlIGluZmVyZW5jZSBlZmZpY2llbmN5LiBUaGUgYmFzZSBSTCBsb3NzIHVzZXMgYSBzcXVhcmVkIGFkdmFudGFnZSBmb3JtIHdpdGggS0wgcmVndWxhcml6YXRpb24uIFlvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgdGhlIGNvbXBsZXRlIGxvc3MgY29tcHV0YXRpb24gaW5jbHVkaW5nIHRoZSBidWRnZXQgcGVuYWx0eS4=",
  "id": "228",
  "test_cases": [
    {
      "test": "import numpy as np\nrewards = np.array([[1.0, 0.5]])\nlog_probs = np.array([[-1.0, -1.0]])\nold_log_probs = np.array([[-1.0, -1.0]])\nresponse_lengths = np.array([[50, 60]])\nresult = rl_budget_loss(rewards, log_probs, old_log_probs, response_lengths, 100, 0.1, 0.01)\nprint(round(result, 6))",
      "expected_output": "0.0625"
    },
    {
      "test": "import numpy as np\nrewards = np.array([[1.0, 0.5]])\nlog_probs = np.array([[-1.0, -1.5]])\nold_log_probs = np.array([[-1.2, -1.3]])\nresponse_lengths = np.array([[150, 80]])\nresult = rl_budget_loss(rewards, log_probs, old_log_probs, response_lengths, 100, 0.1, 0.01)\nprint(round(result, 6))",
      "expected_output": "0.0004"
    }
  ],
  "difficulty": "medium",
  "pytorch_difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "rewards = [[1.0, 0.5]], log_probs = [[-1.0, -1.5]], old_log_probs = [[-1.2, -1.3]], response_lengths = [[150, 80]], token_budget = 100, kl_coef = 0.1, budget_penalty_coef = 0.01",
    "output": "0.0004",
    "reasoning": "First response (150 tokens) exceeds budget (100) by 50, so penalty = -0.01 * 50 = -0.5. Adjusted rewards become [0.5, 0.5]. Baseline = 0.5, so advantages = [0.0, 0.0]. KL terms = 0.1 * [0.2, -0.2] = [0.02, -0.02]. Loss = mean((0-0.02)², (0-(-0.02))²) = 0.0004"
  },
  "category": "Reinforcement Learning",
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpkZWYgcmxfYnVkZ2V0X2xvc3MoCiAgICByZXdhcmRzOiB0b3JjaC5UZW5zb3IsCiAgICBsb2dfcHJvYnM6IHRvcmNoLlRlbnNvciwKICAgIG9sZF9sb2dfcHJvYnM6IHRvcmNoLlRlbnNvciwKICAgIHJlc3BvbnNlX2xlbmd0aHM6IHRvcmNoLlRlbnNvciwKICAgIHRva2VuX2J1ZGdldDogaW50LAogICAga2xfY29lZjogZmxvYXQsCiAgICBidWRnZXRfcGVuYWx0eV9jb2VmOiBmbG9hdAopIC0+IHRvcmNoLlRlbnNvcjoKICAgICIiIgogICAgQ29tcHV0ZSB0aGUgYnVkZ2V0LWNvbnN0cmFpbmVkIFJMIGxvc3MgdXNpbmcgUHlUb3JjaC4KICAgIAogICAgQXJnczoKICAgICAgICByZXdhcmRzOiBTaGFwZSAoYmF0Y2hfc2l6ZSwgSykgLSByZXdhcmRzIGZvciBLIHNhbXBsZXMgcGVyIHByb21wdAogICAgICAgIGxvZ19wcm9iczogU2hhcGUgKGJhdGNoX3NpemUsIEspIC0gbG9nIM+AX864KHl8eCkgY3VycmVudCBwb2xpY3kKICAgICAgICBvbGRfbG9nX3Byb2JzOiBTaGFwZSAoYmF0Y2hfc2l6ZSwgSykgLSBsb2cgz4Bfb2xkKHl8eCkgb2xkIHBvbGljeQogICAgICAgIHJlc3BvbnNlX2xlbmd0aHM6IFNoYXBlIChiYXRjaF9zaXplLCBLKSAtIHRva2VuIGxlbmd0aHMgb2YgcmVzcG9uc2VzCiAgICAgICAgdG9rZW5fYnVkZ2V0OiBNYXhpbXVtIGFsbG93ZWQgdG9rZW5zIGJlZm9yZSBwZW5hbHR5CiAgICAgICAga2xfY29lZjogz4QgY29lZmZpY2llbnQgZm9yIEtMIHJlZ3VsYXJpemF0aW9uCiAgICAgICAgYnVkZ2V0X3BlbmFsdHlfY29lZjogzrsgY29lZmZpY2llbnQgZm9yIGJ1ZGdldCBwZW5hbHR5CiAgICAgICAgCiAgICBSZXR1cm5zOgogICAgICAgIFNjYWxhciBsb3NzIHRlbnNvcgogICAgIiIiCiAgICAjIFlvdXIgY29kZSBoZXJlCiAgICBwYXNz",
  "title": "Budget-Constrained RL Loss",
  "createdAt": "December 10, 2025 at 9:50:39 AM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "import torch\nrewards = torch.tensor([[1.0, 0.5]])\nlog_probs = torch.tensor([[-1.0, -1.0]])\nold_log_probs = torch.tensor([[-1.0, -1.0]])\nresponse_lengths = torch.tensor([[50, 60]])\nresult = rl_budget_loss(rewards, log_probs, old_log_probs, response_lengths, 100, 0.1, 0.01)\nprint(round(result.item(), 6))",
      "expected_output": "0.0625"
    },
    {
      "test": "import torch\nrewards = torch.tensor([[1.0, 0.5]])\nlog_probs = torch.tensor([[-1.0, -1.5]])\nold_log_probs = torch.tensor([[-1.2, -1.3]])\nresponse_lengths = torch.tensor([[150, 80]])\nresult = rl_budget_loss(rewards, log_probs, old_log_probs, response_lengths, 100, 0.1, 0.01)\nprint(round(result.item(), 6))",
      "expected_output": "0.0004"
    },
    {
      "test": "import torch\nrewards = torch.tensor([[1.0, 0.8, 0.6], [0.5, 0.7, 0.9]])\nlog_probs = torch.tensor([[-1.0, -1.2, -1.4], [-0.8, -1.0, -1.2]])\nold_log_probs = torch.tensor([[-1.1, -1.1, -1.1], [-0.9, -0.9, -0.9]])\nresponse_lengths = torch.tensor([[80, 120, 150], [90, 100, 200]])\nresult = rl_budget_loss(rewards, log_probs, old_log_probs, response_lengths, 100, 0.1, 0.01)\nprint(round(result.item(), 4))",
      "expected_output": "0.1159"
    },
    {
      "test": "import torch\nrewards = torch.tensor([[0.5, 0.5, 0.5, 0.5]])\nlog_probs = torch.tensor([[-2.0, -2.0, -2.0, -2.0]])\nold_log_probs = torch.tensor([[-2.0, -2.0, -2.0, -2.0]])\nresponse_lengths = torch.tensor([[100, 100, 100, 100]])\nresult = rl_budget_loss(rewards, log_probs, old_log_probs, response_lengths, 100, 0.1, 0.01)\nprint(round(result.item(), 6))",
      "expected_output": "0.0"
    }
  ],
  "learn_section": "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",
  "starter_code": "import numpy as np\n\ndef rl_budget_loss(\n    rewards: np.ndarray,\n    log_probs: np.ndarray,\n    old_log_probs: np.ndarray,\n    response_lengths: np.ndarray,\n    token_budget: int,\n    kl_coef: float,\n    budget_penalty_coef: float\n) -> float:\n    \"\"\"\n    Compute the budget-constrained RL loss.\n    \n    The loss combines:\n    1. Budget penalty for responses exceeding token_budget\n    2. Advantage estimation (adjusted reward - baseline)\n    3. KL regularization between current and old policy\n    \n    Loss formula: E[(advantage - kl_term)^2]\n    \n    Args:\n        rewards: Shape (batch_size, K) - rewards for K samples per prompt\n        log_probs: Shape (batch_size, K) - log π_θ(y|x) current policy\n        old_log_probs: Shape (batch_size, K) - log π_old(y|x) old policy\n        response_lengths: Shape (batch_size, K) - token lengths of responses\n        token_budget: Maximum allowed tokens before penalty\n        kl_coef: τ coefficient for KL regularization\n        budget_penalty_coef: λ coefficient for budget penalty\n        \n    Returns:\n        Scalar loss value (float)\n    \"\"\"\n    # Your code here\n    pass",
  "description_decoded": "Implement the budget-constrained reinforcement learning loss function from the Kimi K2 paper. In K2's RL training, a 'Budget Control' mechanism penalizes responses that exceed a token budget to improve inference efficiency. The base RL loss uses a squared advantage form with KL regularization. Your task is to implement the complete loss computation including the budget penalty.",
  "learn_section_decoded": "## Budget-Constrained RL Loss\n\n### Background\n\nThe Kimi K2 model uses reinforcement learning to improve performance across diverse tasks. A key challenge is that RL often causes models to generate excessively long responses, which increases inference costs without proportional benefits on many tasks.\n\n### The Base RL Loss\n\nK2 uses a squared advantage form with KL regularization:\n\n$$L_{RL}(\\theta) = \\mathbb{E}_x\\left[\\frac{1}{K}\\sum_{i=1}^{K}\\left(r(x,y_i) - \\bar{r}(x) - \\tau \\log\\frac{\\pi_\\theta(y_i|x)}{\\pi_{old}(y_i|x)}\\right)^2\\right]$$\n\nWhere:\n- $r(x, y_i)$ is the reward for response $y_i$ to prompt $x$\n- $\\bar{r}(x)$ is the mean reward across K sampled responses (the baseline)\n- $\\tau$ is the KL regularization coefficient\n- $\\pi_\\theta$ is the current policy and $\\pi_{old}$ is the previous policy\n\n### Budget Control Mechanism\n\nTo encourage token-efficient responses, K2 introduces a budget penalty:\n\n$$r_{effective}(x, y_i) = r(x, y_i) - \\lambda \\cdot \\max(0, L_i - B)$$\n\nWhere:\n- $L_i$ is the length (in tokens) of response $y_i$\n- $B$ is the token budget\n- $\\lambda$ is the budget penalty coefficient\n\nResponses exceeding the budget receive a penalty proportional to the excess length. This incentivizes the model to generate concise solutions.\n\n### Algorithm Steps\n\n1. **Compute Budget Penalties**: For each response, calculate the penalty based on how much it exceeds the budget\n2. **Adjust Rewards**: Subtract penalties from raw rewards\n3. **Compute Baseline**: Mean of adjusted rewards per prompt\n4. **Compute Advantages**: Adjusted reward minus baseline\n5. **Compute KL Terms**: $\\tau \\cdot (\\log\\pi_\\theta - \\log\\pi_{old})$\n6. **Compute Squared Loss**: $(A_i - KL_i)^2$ for each sample\n7. **Average**: Mean over all prompts and samples\n\n### Why Squared Loss?\n\nUnlike PPO's clipped objective, this squared form:\n- Provides smoother gradients\n- Naturally penalizes both over- and under-estimation\n- Works well with the group-relative advantage normalization\n\n### Impact\n\nAccording to the K2 paper: \"This approach significantly enhances the model's token efficiency, encouraging concise yet effective solutions across all domains.\""
}