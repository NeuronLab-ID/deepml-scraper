{
  "description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gYHJlbHVgIHRoYXQgaW1wbGVtZW50cyB0aGUgUmVjdGlmaWVkIExpbmVhciBVbml0IChSZUxVKSBhY3RpdmF0aW9uIGZ1bmN0aW9uLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSBzaW5nbGUgZmxvYXQgYXMgaW5wdXQgYW5kIHJldHVybiB0aGUgdmFsdWUgYWZ0ZXIgYXBwbHlpbmcgdGhlIFJlTFUgZnVuY3Rpb24uIFRoZSBSZUxVIGZ1bmN0aW9uIHJldHVybnMgdGhlIGlucHV0IGlmIGl0J3MgZ3JlYXRlciB0aGFuIDAsIG90aGVyd2lzZSwgaXQgcmV0dXJucyAwLg==",
  "mdx_file": "655e16ea-1159-42c0-b911-85573844c388.mdx",
  "test_cases": [
    {
      "test": "print(relu(0))",
      "expected_output": "0"
    },
    {
      "test": "print(relu(1))",
      "expected_output": "1"
    }
  ],
  "difficulty": "easy",
  "pytorch_difficulty": "easy",
  "likes": "0",
  "video": "https://youtu.be/1hq_bTksuOQ",
  "example": {
    "input": "print(relu(0)) \nprint(relu(1)) \nprint(relu(-1))",
    "output": "0\n1\n0",
    "reasoning": "The ReLU function is applied to the input values 0, 1, and -1. The output is 0 for negative values and the input value for non-negative values."
  },
  "dislikes": "0",
  "category": "Deep Learning",
  "starter_code": "def relu(z: float) -> float:\n\t# Your code here\n\tpass\n",
  "title": "Implement ReLU Activation Function",
  "learn_section": "CiMjIFVuZGVyc3RhbmRpbmcgdGhlIFJlTFUgQWN0aXZhdGlvbiBGdW5jdGlvbgoKVGhlIFJlTFUgKFJlY3RpZmllZCBMaW5lYXIgVW5pdCkgYWN0aXZhdGlvbiBmdW5jdGlvbiBpcyB3aWRlbHkgdXNlZCBpbiBuZXVyYWwgbmV0d29ya3MsIHBhcnRpY3VsYXJseSBpbiBoaWRkZW4gbGF5ZXJzIG9mIGRlZXAgbGVhcm5pbmcgbW9kZWxzLiBJdCBtYXBzIGFueSByZWFsLXZhbHVlZCBudW1iZXIgdG8gdGhlIG5vbi1uZWdhdGl2ZSByYW5nZSAkWzAsIFxpbmZ0eSkkLCB3aGljaCBoZWxwcyBpbnRyb2R1Y2Ugbm9uLWxpbmVhcml0eSBpbnRvIHRoZSBtb2RlbCB3aGlsZSBtYWludGFpbmluZyBjb21wdXRhdGlvbmFsIGVmZmljaWVuY3kuCgojIyMgTWF0aGVtYXRpY2FsIERlZmluaXRpb24KVGhlIFJlTFUgZnVuY3Rpb24gaXMgbWF0aGVtYXRpY2FsbHkgZGVmaW5lZCBhczoKJCQKZih6KSA9IFxtYXgoMCwgeikKJCQKd2hlcmUgJHokIGlzIHRoZSBpbnB1dCB0byB0aGUgZnVuY3Rpb24uCgojIyMgQ2hhcmFjdGVyaXN0aWNzCi0gKipPdXRwdXQgUmFuZ2UqKjogVGhlIG91dHB1dCBpcyBhbHdheXMgaW4gdGhlIHJhbmdlICRbMCwgXGluZnR5KSQuIFZhbHVlcyBiZWxvdyAwIGFyZSBtYXBwZWQgdG8gMCwgd2hpbGUgcG9zaXRpdmUgdmFsdWVzIGFyZSByZXRhaW5lZC4KLSAqKlNoYXBlKio6IFRoZSBmdW5jdGlvbiBoYXMgYW4gIkwiIHNoYXBlZCBjdXJ2ZSB3aXRoIGEgaG9yaXpvbnRhbCBheGlzIGF0ICR5ID0gMCQgYW5kIGEgbGluZWFyIGluY3JlYXNlIGZvciBwb3NpdGl2ZSAkeiQuCi0gKipHcmFkaWVudCoqOiBUaGUgZ3JhZGllbnQgaXMgMSBmb3IgcG9zaXRpdmUgdmFsdWVzIG9mICR6JCBhbmQgMCBmb3Igbm9uLXBvc2l0aXZlIHZhbHVlcy4gVGhpcyBtZWFucyB0aGUgZnVuY3Rpb24gaXMgbGluZWFyIGZvciBwb3NpdGl2ZSBpbnB1dHMgYW5kIGZsYXQgKHplcm8gZ3JhZGllbnQpIGZvciBuZWdhdGl2ZSBpbnB1dHMuCgpUaGlzIGZ1bmN0aW9uIGlzIHBhcnRpY3VsYXJseSB1c2VmdWwgaW4gZGVlcCBsZWFybmluZyBtb2RlbHMgYXMgaXQgaW50cm9kdWNlcyBub24tbGluZWFyaXR5IHdoaWxlIGJlaW5nIGNvbXB1dGF0aW9uYWxseSBlZmZpY2llbnQsIGhlbHBpbmcgdG8gY2FwdHVyZSBjb21wbGV4IHBhdHRlcm5zIGluIHRoZSBkYXRhLgo=",
  "contributor": [
    {
      "profile_link": "https://github.com/doshi-kevin",
      "name": "Kevin Doshi"
    },
    {
      "profile_link": "https://www.youtube.com/@StoatScript/videos",
      "name": "StoatScript"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "import torch\nres = relu(torch.tensor([0.0, 1.0, -1.0]))\nprint(res.numpy().tolist())",
      "expected_output": "[0.0, 1.0, 0.0]"
    },
    {
      "test": "import torch\nres = relu(torch.tensor([-5.0, -2.0, 0.0, 2.0, 5.0]))\nprint(res.numpy().tolist())",
      "expected_output": "[0.0, 0.0, 0.0, 2.0, 5.0]"
    },
    {
      "test": "import torch\nres = relu(torch.tensor([[1.0, -1.0], [-2.0, 2.0]]))\nprint(res.numpy().tolist())",
      "expected_output": "[[1.0, 0.0], [0.0, 2.0]]"
    }
  ],
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpkZWYgcmVsdSh6OiB0b3JjaC5UZW5zb3IpIC0+IHRvcmNoLlRlbnNvcjoKICAgICIiIgogICAgSW1wbGVtZW50cyB0aGUgUmVMVSAoUmVjdGlmaWVkIExpbmVhciBVbml0KSBhY3RpdmF0aW9uIGZ1bmN0aW9uIHVzaW5nIFB5VG9yY2guCiAgICAKICAgIEFyZ3M6CiAgICAgICAgejogSW5wdXQgdGVuc29yIG9mIGFueSBzaGFwZQogICAgCiAgICBSZXR1cm5zOgogICAgICAgIFRlbnNvciB3aXRoIFJlTFUgYXBwbGllZCBlbGVtZW50LXdpc2U6IG1heCgwLCB6KQogICAgIiIiCiAgICAjIFlvdXIgaW1wbGVtZW50YXRpb24gaGVyZQogICAgcGFzcwo=",
  "marmo_link": "https://open-deep-ml.github.io/DML-OpenProblem/problem-42/",
  "description_decoded": "Write a Python function `relu` that implements the Rectified Linear Unit (ReLU) activation function. The function should take a single float as input and return the value after applying the ReLU function. The ReLU function returns the input if it's greater than 0, otherwise, it returns 0.",
  "learn_section_decoded": "\n## Understanding the ReLU Activation Function\n\nThe ReLU (Rectified Linear Unit) activation function is widely used in neural networks, particularly in hidden layers of deep learning models. It maps any real-valued number to the non-negative range $[0, \\infty)$, which helps introduce non-linearity into the model while maintaining computational efficiency.\n\n### Mathematical Definition\nThe ReLU function is mathematically defined as:\n$$\nf(z) = \\max(0, z)\n$$\nwhere $z$ is the input to the function.\n\n### Characteristics\n- **Output Range**: The output is always in the range $[0, \\infty)$. Values below 0 are mapped to 0, while positive values are retained.\n- **Shape**: The function has an \"L\" shaped curve with a horizontal axis at $y = 0$ and a linear increase for positive $z$.\n- **Gradient**: The gradient is 1 for positive values of $z$ and 0 for non-positive values. This means the function is linear for positive inputs and flat (zero gradient) for negative inputs.\n\nThis function is particularly useful in deep learning models as it introduces non-linearity while being computationally efficient, helping to capture complex patterns in the data.\n"
}