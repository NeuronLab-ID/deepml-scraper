{
  "description": "U3BlY2lhbCB0aGFua3MgdG8gQW5kcmVqIEthcnBhdGh5IGZvciBtYWtpbmcgYSB2aWRlbyBhYm91dCB0aGlzLCBpZiB5b3UgaGF2ZW4ndCBhbHJlYWR5IGNoZWNrIG91dCBoaXMgdmlkZW9zIG9uIFlvdVR1YmUgaHR0cHM6Ly95b3V0dS5iZS9WTWotM1MxdGt1MD9zaT1namxuRlA0bzNKUk45ZFRnLiBXcml0ZSBhIFB5dGhvbiBjbGFzcyBzaW1pbGFyIHRvIHRoZSBwcm92aWRlZCAnVmFsdWUnIGNsYXNzIHRoYXQgaW1wbGVtZW50cyB0aGUgYmFzaWMgYXV0b2dyYWQgb3BlcmF0aW9uczogYWRkaXRpb24sIG11bHRpcGxpY2F0aW9uLCBhbmQgUmVMVSBhY3RpdmF0aW9uLiBUaGUgY2xhc3Mgc2hvdWxkIGhhbmRsZSBzY2FsYXIgdmFsdWVzIGFuZCBzaG91bGQgY29ycmVjdGx5IGNvbXB1dGUgZ3JhZGllbnRzIGZvciB0aGVzZSBvcGVyYXRpb25zIHRocm91Z2ggYXV0b21hdGljIGRpZmZlcmVudGlhdGlvbi4=",
  "mdx_file": "c9a4ef2e-c9b9-4653-8a48-c44af59a762e.mdx",
  "tinygrad_difficulty": "medium",
  "tinygrad_starter_code": "ZnJvbSB0aW55Z3JhZC50ZW5zb3IgaW1wb3J0IFRlbnNvcgoKY2xhc3MgVmFsdWU6CiAgICAiIiJTYW1lIGlkZWEsIGJ1dCB1c2luZyB0aW55Z3JhZOKAmXMgYXV0b21hdGljIGRpZmZlcmVudGlhdGlvbi4iIiIKCiAgICBkZWYgX19pbml0X18oc2VsZiwgZGF0YSwgX3RlbnNvcj1Ob25lKToKICAgICAgICBzZWxmLl90ID0gX3RlbnNvciBpZiBfdGVuc29yIGlzIG5vdCBOb25lIGVsc2UgVGVuc29yKGZsb2F0KGRhdGEpLCByZXF1aXJlc19ncmFkPVRydWUpCgogICAgQHByb3BlcnR5CiAgICBkZWYgZGF0YShzZWxmKToKICAgICAgICByZXR1cm4gZmxvYXQoc2VsZi5fdC5udW1weSgpKQoKICAgIEBwcm9wZXJ0eQogICAgZGVmIGdyYWQoc2VsZik6CiAgICAgICAgZyA9IHNlbGYuX3QuZ3JhZAogICAgICAgIHJldHVybiAwIGlmIGcgaXMgTm9uZSBlbHNlIGZsb2F0KGcubnVtcHkoKSkKCiAgICBkZWYgX19yZXByX18oc2VsZik6CiAgICAgICAgZGVmIGZtdCh4KToKICAgICAgICAgICAgcmV0dXJuIGludCh4KSBpZiBmbG9hdCh4KS5pc19pbnRlZ2VyKCkgZWxzZSByb3VuZCh4LCA0KQogICAgICAgIHJldHVybiBmIlZhbHVlKGRhdGE9e2ZtdChzZWxmLmRhdGEpfSwgZ3JhZD17Zm10KHNlbGYuZ3JhZCl9KSIKCiAgICBkZWYgX3dyYXAoc2VsZiwgb3RoZXIpOgogICAgICAgIHJldHVybiBvdGhlciBpZiBpc2luc3RhbmNlKG90aGVyLCBWYWx1ZSkgZWxzZSBWYWx1ZShvdGhlcikKCiAgICBkZWYgX19hZGRfXyhzZWxmLCBvdGhlcik6CiAgICAgICAgb3RoZXIgPSBzZWxmLl93cmFwKG90aGVyKQogICAgICAgIHJldHVybiBWYWx1ZSgwLCBfdGVuc29yPXNlbGYuX3QgKyBvdGhlci5fdCkKCiAgICBfX3JhZGRfXyA9IF9fYWRkX18KCiAgICBkZWYgX19tdWxfXyhzZWxmLCBvdGhlcik6CiAgICAgICAgb3RoZXIgPSBzZWxmLl93cmFwKG90aGVyKQogICAgICAgIHJldHVybiBWYWx1ZSgwLCBfdGVuc29yPXNlbGYuX3QgKiBvdGhlci5fdCkKCiAgICBfX3JtdWxfXyA9IF9fbXVsX18KCiAgICBkZWYgcmVsdShzZWxmKToKICAgICAgICByZXR1cm4gVmFsdWUoMCwgX3RlbnNvcj1zZWxmLl90LnJlbHUoKSkKCiAgICBkZWYgYmFja3dhcmQoc2VsZik6CiAgICAgICAgc2VsZi5fdC5iYWNrd2FyZCgpCg==",
  "test_cases": [
    {
      "test": "a = Value(2);b = Value(3);c = Value(10);d = a + b * c  ;e = Value(7) * Value(2);f = e + d;g = f.relu()  \ng.backward()\nprint(a,b,c,d,e,f,g)\n",
      "expected_output": " Value(data=2, grad=1) Value(data=3, grad=10) Value(data=10, grad=3) Value(data=32, grad=1) Value(data=14, grad=1) Value(data=46, grad=1) Value(data=46, grad=1)"
    }
  ],
  "pytorch_difficulty": "medium",
  "likes": "0",
  "video": [
    "https://youtu.be/VMj-3S1tku0?si=gjlnFP4o3JRN9dTg",
    "https://youtu.be/heSdPbAfFH4"
  ],
  "difficulty": "medium",
  "dislikes": "0",
  "example": {
    "input": "a = Value(2)\n        b = Value(-3)\n        c = Value(10)\n        d = a + b * c\n        e = d.relu()\n        e.backward()\n        print(a, b, c, d, e)",
    "reasoning": "The output reflects the forward computation and gradients after backpropagation. The ReLU on 'd' zeros out its output and gradient due to the negative data value.",
    "output": "Value(data=2, grad=0) Value(data=-3, grad=0) Value(data=10, grad=0)"
  },
  "category": "Deep Learning",
  "starter_code": "class Value:\n\tdef __init__(self, data, _children=(), _op=''):\n\t\tself.data = data\n\t\tself.grad = 0\n\t\tself._backward = lambda: None\n\t\tself._prev = set(_children)\n\t\tself._op = _op\n\tdef __repr__(self):\n\t\treturn f\"Value(data={self.data}, grad={self.grad})\"\n\n\tdef __add__(self, other):\n\t\t # Implement addition here\n\t\tpass\n\n\tdef __mul__(self, other):\n\t\t# Implement multiplication here\n\t\tpass\n\n\tdef relu(self):\n\t\t# Implement ReLU here\n\t\tpass\n\n\tdef backward(self):\n\t\t# Implement backward pass here\n\t\tpass",
  "learn_section": "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",
  "title": "Implementing Basic Autograd Operations",
  "contributor": null,
  "pytorch_test_cases": [
    {
      "test": "a = Value(2); b = Value(3); c = Value(10); d = a + b * c; e = Value(7) * Value(2); f = e + d; g = f.relu(); g.backward(); print(a, b, c, d, e, f, g)",
      "expected_output": "Value(data=2, grad=1) Value(data=3, grad=10) Value(data=10, grad=3) Value(data=32, grad=1) Value(data=14, grad=1) Value(data=46, grad=1) Value(data=46, grad=1)"
    }
  ],
  "tinygrad_test_cases": [
    {
      "test": "a = Value(2); b = Value(3); c = Value(10); d = a + b * c; e = Value(7) * Value(2); f = e + d; g = f.relu(); g.backward(); print(a, b, c, d, e, f, g)",
      "expected_output": "Value(data=2, grad=1) Value(data=3, grad=10) Value(data=10, grad=3) Value(data=32, grad=1) Value(data=14, grad=1) Value(data=46, grad=1) Value(data=46, grad=1)"
    }
  ],
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpjbGFzcyBWYWx1ZToKICAgICIiIkEgdGlueSBzY2FsYXIgd3JhcHBlciB0aGF0IGRlbGVnYXRlcyBhbGwgZ3JhZGllbnQgd29yayB0byBQeVRvcmNoIGF1dG9ncmFkLiIiIgoKICAgIGRlZiBfX2luaXRfXyhzZWxmLCBkYXRhLCBfdGVuc29yPU5vbmUpOgogICAgICAgICMgbGVhZiBub2RlOiBjcmVhdGUgZnJlc2ggdGVuc29yIHdpdGggZ3JhZDsgaW50ZXJuYWwgbm9kZTogcmV1c2UgdGVuc29yCiAgICAgICAgc2VsZi5fdCA9IF90ZW5zb3IgaWYgX3RlbnNvciBpcyBub3QgTm9uZSBlbHNlIHRvcmNoLnRlbnNvcihmbG9hdChkYXRhKSwgcmVxdWlyZXNfZ3JhZD1UcnVlKQogICAgICAgICMgbWFrZSBzdXJlIGV2ZXJ5IFRlbnNvciAobGVhZiBvciBub3QpIGtlZXBzIGl0cyBncmFkIGZvciBwcmludGluZwogICAgICAgIHNlbGYuX3QucmV0YWluX2dyYWQoKQoKICAgICMgLS0tLS0tLSBjb252ZW5pZW5jZXMgLS0tLS0tLQogICAgQHByb3BlcnR5CiAgICBkZWYgZGF0YShzZWxmKToKICAgICAgICByZXR1cm4gc2VsZi5fdC5pdGVtKCkKCiAgICBAcHJvcGVydHkKICAgIGRlZiBncmFkKHNlbGYpOgogICAgICAgIGcgPSBzZWxmLl90LmdyYWQKICAgICAgICByZXR1cm4gMCBpZiBnIGlzIE5vbmUgZWxzZSBnLml0ZW0oKQoKICAgIGRlZiBfX3JlcHJfXyhzZWxmKToKICAgICAgICBkZWYgZm10KHgpOgogICAgICAgICAgICByZXR1cm4gaW50KHgpIGlmIGZsb2F0KHgpLmlzX2ludGVnZXIoKSBlbHNlIHJvdW5kKHgsIDQpCiAgICAgICAgcmV0dXJuIGYiVmFsdWUoZGF0YT17Zm10KHNlbGYuZGF0YSl9LCBncmFkPXtmbXQoc2VsZi5ncmFkKX0pIgoKICAgICMgZW5zdXJlIHJocyBpcyBWYWx1ZQogICAgZGVmIF93cmFwKHNlbGYsIG90aGVyKToKICAgICAgICByZXR1cm4gb3RoZXIgaWYgaXNpbnN0YW5jZShvdGhlciwgVmFsdWUpIGVsc2UgVmFsdWUob3RoZXIpCgogICAgIyAtLS0tLS0tIGFyaXRobWV0aWMgb3BzIC0tLS0tLS0KICAgIGRlZiBfX2FkZF9fKHNlbGYsIG90aGVyKToKICAgICAgICBvdGhlciA9IHNlbGYuX3dyYXAob3RoZXIpCiAgICAgICAgcmV0dXJuIFZhbHVlKDAsIF90ZW5zb3I9c2VsZi5fdCArIG90aGVyLl90KQoKICAgIF9fcmFkZF9fID0gX19hZGRfXwoKICAgIGRlZiBfX211bF9fKHNlbGYsIG90aGVyKToKICAgICAgICBvdGhlciA9IHNlbGYuX3dyYXAob3RoZXIpCiAgICAgICAgcmV0dXJuIFZhbHVlKDAsIF90ZW5zb3I9c2VsZi5fdCAqIG90aGVyLl90KQoKICAgIF9fcm11bF9fID0gX19tdWxfXwoKICAgICMgLS0tLS0tLSBhY3RpdmF0aW9uIC0tLS0tLS0KICAgIGRlZiByZWx1KHNlbGYpOgogICAgICAgIHJldHVybiBWYWx1ZSgwLCBfdGVuc29yPXRvcmNoLnJlbHUoc2VsZi5fdCkpCgogICAgIyAtLS0tLS0tIGJhY2stcHJvcCBlbnRyeSAtLS0tLS0tCiAgICBkZWYgYmFja3dhcmQoc2VsZik6CiAgICAgICAgc2VsZi5fdC5iYWNrd2FyZCgpCg==",
  "description_decoded": "Special thanks to Andrej Karpathy for making a video about this, if you haven't already check out his videos on YouTube https://youtu.be/VMj-3S1tku0?si=gjlnFP4o3JRN9dTg. Write a Python class similar to the provided 'Value' class that implements the basic autograd operations: addition, multiplication, and ReLU activation. The class should handle scalar values and should correctly compute gradients for these operations through automatic differentiation.",
  "learn_section_decoded": "\n## Understanding Mathematical Concepts in Autograd Operations\n\n*First, watch the video in the Solution section.*\n\nThis task focuses on implementing basic automatic differentiation mechanisms for neural networks. The operations of addition, multiplication, and ReLU are fundamental to neural network computations and their training through backpropagation.\n\n### Mathematical Foundations\n\n**Addition (`__add__`)**  \n- **Forward Pass**: For two scalar values \\( a \\) and \\( b \\), their sum \\( s \\) is:\n  $$\n  s = a + b\n  $$\n- **Backward Pass**: The derivative of \\( s \\) with respect to both \\( a \\) and \\( b \\) is 1. During backpropagation, the gradient of the output is passed directly to both inputs.\n\n**Multiplication (`__mul__`)**  \n- **Forward Pass**: For two scalar values \\( a \\) and \\( b \\), their product \\( p \\) is:\n  $$\n  p = a \\times b\n  $$\n- **Backward Pass**: The gradient of \\( p \\) with respect to \\( a \\) is \\( b \\), and with respect to \\( b \\) is \\( a \\). During backpropagation, each input's gradient is the product of the other input and the output's gradient.\n\n**ReLU Activation (`relu`)**  \n- **Forward Pass**: The ReLU function is defined as:\n  $$\n  R(x) = \\max(0, x)\n  $$\n  This function outputs \\( x \\) if \\( x \\) is positive, and 0 otherwise.\n- **Backward Pass**: The derivative of the ReLU function is 1 for \\( x > 0 \\) and 0 for \\( x \\leq 0 \\). The gradient is propagated through the function only if the input is positive; otherwise, it stops.\n\n### Conceptual Application in Neural Networks\n- **Addition and Multiplication**: These operations are ubiquitous in neural networks, forming the basis for computing weighted sums of inputs in the neurons.\n- **ReLU Activation**: Commonly used as an activation function in neural networks due to its simplicity and effectiveness in introducing non-linearity, making learning complex patterns possible.\n\nUnderstanding these operations and their implications on gradient flow is crucial for designing and training effective neural network models. By implementing these from scratch, you gain deeper insights into the workings of more sophisticated deep learning libraries.\n\n",
  "tinygrad_starter_code_decoded": "from tinygrad.tensor import Tensor\n\nclass Value:\n    \"\"\"Same idea, but using tinygradâ€™s automatic differentiation.\"\"\"\n\n    def __init__(self, data, _tensor=None):\n        self._t = _tensor if _tensor is not None else Tensor(float(data), requires_grad=True)\n\n    @property\n    def data(self):\n        return float(self._t.numpy())\n\n    @property\n    def grad(self):\n        g = self._t.grad\n        return 0 if g is None else float(g.numpy())\n\n    def __repr__(self):\n        def fmt(x):\n            return int(x) if float(x).is_integer() else round(x, 4)\n        return f\"Value(data={fmt(self.data)}, grad={fmt(self.grad)})\"\n\n    def _wrap(self, other):\n        return other if isinstance(other, Value) else Value(other)\n\n    def __add__(self, other):\n        other = self._wrap(other)\n        return Value(0, _tensor=self._t + other._t)\n\n    __radd__ = __add__\n\n    def __mul__(self, other):\n        other = self._wrap(other)\n        return Value(0, _tensor=self._t * other._t)\n\n    __rmul__ = __mul__\n\n    def relu(self):\n        return Value(0, _tensor=self._t.relu())\n\n    def backward(self):\n        self._t.backward()\n"
}