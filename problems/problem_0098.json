{
  "description": "SW1wbGVtZW50IHRoZSBQUmVMVSAoUGFyYW1ldHJpYyBSZUxVKSBhY3RpdmF0aW9uIGZ1bmN0aW9uLCBhIHZhcmlhbnQgb2YgdGhlIFJlTFUgYWN0aXZhdGlvbiBmdW5jdGlvbiB0aGF0IGludHJvZHVjZXMgYSBsZWFybmFibGUgcGFyYW1ldGVyIGZvciBuZWdhdGl2ZSBpbnB1dHMuIFlvdXIgdGFzayBpcyB0byBjb21wdXRlIHRoZSBQUmVMVSBhY3RpdmF0aW9uIHZhbHVlIGZvciBhIGdpdmVuIGlucHV0Lg==",
  "id": "98",
  "test_cases": [
    {
      "test": "print(prelu(2.0))",
      "expected_output": "2.0"
    },
    {
      "test": "print(prelu(0.0))",
      "expected_output": "0.0"
    }
  ],
  "difficulty": "easy",
  "marimo_link": "https://open-deep-ml.github.io/DML-OpenProblem/problem-98",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "prelu(-2.0, alpha=0.25)",
    "output": "-0.5",
    "reasoning": "For x = -2.0 and alpha = 0.25, the PReLU activation is calculated as $PReLU(x) = \\alpha x = 0.25 \\times -2.0 = -0.5$."
  },
  "category": "Deep Learning",
  "starter_code": "def prelu(x: float, alpha: float = 0.25) -> float:\n\t\"\"\"\n\tImplements the PReLU (Parametric ReLU) activation function.\n\n\tArgs:\n\t\tx: Input value\n\t\talpha: Slope parameter for negative values (default: 0.25)\n\n\tReturns:\n\t\tfloat: PReLU activation value\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Implement the PReLU Activation Function",
  "learn_section": "IyMjIFVuZGVyc3RhbmRpbmcgdGhlIFBSZUxVIChQYXJhbWV0cmljIFJlTFUpIEFjdGl2YXRpb24gRnVuY3Rpb24KClRoZSBQUmVMVSAoUGFyYW1ldHJpYyBSZWN0aWZpZWQgTGluZWFyIFVuaXQpIGlzIGFuIGFkdmFuY2VkIHZhcmlhbnQgb2YgdGhlIFJlTFUgYWN0aXZhdGlvbiBmdW5jdGlvbiB0aGF0IGludHJvZHVjZXMgYSBsZWFybmFibGUgcGFyYW1ldGVyIGZvciBuZWdhdGl2ZSBpbnB1dHMuIFRoaXMgbWFrZXMgaXQgbW9yZSBmbGV4aWJsZSB0aGFuIHN0YW5kYXJkIFJlTFUgYW5kIGhlbHBzIHByZXZlbnQgdGhlICJkeWluZyBSZUxVIiBwcm9ibGVtLgoKIyMjIyBNYXRoZW1hdGljYWwgRGVmaW5pdGlvbgoKVGhlIFBSZUxVIGZ1bmN0aW9uIGlzIGRlZmluZWQgYXM6CgokJApQUmVMVSh4KSA9IFxiZWdpbntjYXNlc30KeCAmIFx0ZXh0e2lmIH0geCA+IDAgXFwKXGFscGhhIHggJiBcdGV4dHtvdGhlcndpc2V9ClxlbmR7Y2FzZXN9CiQkCgpXaGVyZToKLSAkeCQgaXMgdGhlIGlucHV0IHZhbHVlCi0gJFxhbHBoYSQgaXMgYSBsZWFybmFibGUgcGFyYW1ldGVyICh0eXBpY2FsbHkgaW5pdGlhbGl6ZWQgdG8gYSBzbWFsbCB2YWx1ZSBsaWtlIDAuMjUpCgojIyMjIEtleSBDaGFyYWN0ZXJpc3RpY3MKCjEuICoqQWRhcHRpdmUgU2xvcGUqKjogVW5saWtlIFJlTFUgd2hpY2ggaGFzIGEgemVybyBzbG9wZSBmb3IgbmVnYXRpdmUgaW5wdXRzLCBQUmVMVSBsZWFybnMgdGhlIG9wdGltYWwgbmVnYXRpdmUgc2xvcGUgcGFyYW1ldGVyICgkXGFscGhhJCkgZHVyaW5nIHRyYWluaW5nLgoKMi4gKipPdXRwdXQgUmFuZ2UqKjogCiAgIC0gRm9yICR4ID4gMCQ6IE91dHB1dCBlcXVhbHMgaW5wdXQgKCR5ID0geCQpCiAgIC0gRm9yICR4IFxsZXEgMCQ6IE91dHB1dCBpcyBzY2FsZWQgYnkgJFxhbHBoYSQgKCR5ID0gXGFscGhhIHgkKQoKMy4gKipBZHZhbnRhZ2VzKio6CiAgIC0gSGVscHMgcHJldmVudCB0aGUgImR5aW5nIFJlTFUiIHByb2JsZW0KICAgLSBNb3JlIGZsZXhpYmxlIHRoYW4gc3RhbmRhcmQgUmVMVQogICAtIENhbiBpbXByb3ZlIG1vZGVsIHBlcmZvcm1hbmNlIHRocm91Z2ggbGVhcm5lZCBwYXJhbWV0ZXIKICAgLSBNYWludGFpbnMgdGhlIGNvbXB1dGF0aW9uYWwgZWZmaWNpZW5jeSBvZiBSZUxVCgo0LiAqKlNwZWNpYWwgQ2FzZXMqKjoKICAgLSBXaGVuICRcYWxwaGEgPSAwJCwgUFJlTFUgYmVjb21lcyBSZUxVCiAgIC0gV2hlbiAkXGFscGhhID0gMSQsIFBSZUxVIGJlY29tZXMgYSBsaW5lYXIgZnVuY3Rpb24KICAgLSBXaGVuICRcYWxwaGEkIGlzIHNtYWxsIChlLmcuLCAwLjAxKSwgUFJlTFUgYmVoYXZlcyBzaW1pbGFybHkgdG8gTGVha3kgUmVMVQoKUFJlTFUgaXMgcGFydGljdWxhcmx5IHVzZWZ1bCBpbiBkZWVwIG5ldXJhbCBuZXR3b3JrcyB3aGVyZSB0aGUgb3B0aW1hbCBuZWdhdGl2ZSBzbG9wZSBtaWdodCB2YXJ5IGFjcm9zcyBkaWZmZXJlbnQgbGF5ZXJzIG9yIGNoYW5uZWxzLg==",
  "contributor": [
    {
      "profile_link": "https://github.com/Haleshot",
      "name": "Haleshot"
    }
  ],
  "description_decoded": "Implement the PReLU (Parametric ReLU) activation function, a variant of the ReLU activation function that introduces a learnable parameter for negative inputs. Your task is to compute the PReLU activation value for a given input.",
  "learn_section_decoded": "### Understanding the PReLU (Parametric ReLU) Activation Function\n\nThe PReLU (Parametric Rectified Linear Unit) is an advanced variant of the ReLU activation function that introduces a learnable parameter for negative inputs. This makes it more flexible than standard ReLU and helps prevent the \"dying ReLU\" problem.\n\n#### Mathematical Definition\n\nThe PReLU function is defined as:\n\n$$\nPReLU(x) = \\begin{cases}\nx & \\text{if } x > 0 \\\\\n\\alpha x & \\text{otherwise}\n\\end{cases}\n$$\n\nWhere:\n- $x$ is the input value\n- $\\alpha$ is a learnable parameter (typically initialized to a small value like 0.25)\n\n#### Key Characteristics\n\n1. **Adaptive Slope**: Unlike ReLU which has a zero slope for negative inputs, PReLU learns the optimal negative slope parameter ($\\alpha$) during training.\n\n2. **Output Range**: \n   - For $x > 0$: Output equals input ($y = x$)\n   - For $x \\leq 0$: Output is scaled by $\\alpha$ ($y = \\alpha x$)\n\n3. **Advantages**:\n   - Helps prevent the \"dying ReLU\" problem\n   - More flexible than standard ReLU\n   - Can improve model performance through learned parameter\n   - Maintains the computational efficiency of ReLU\n\n4. **Special Cases**:\n   - When $\\alpha = 0$, PReLU becomes ReLU\n   - When $\\alpha = 1$, PReLU becomes a linear function\n   - When $\\alpha$ is small (e.g., 0.01), PReLU behaves similarly to Leaky ReLU\n\nPReLU is particularly useful in deep neural networks where the optimal negative slope might vary across different layers or channels."
}