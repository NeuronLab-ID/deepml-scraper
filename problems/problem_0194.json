{
  "description": "SW1wbGVtZW50IGZ1bmN0aW9ucyB0byAoMSkgZ2VuZXJhdGUgc21vb3RoZWQgb25lLWhvdCBsYWJlbCBkaXN0cmlidXRpb25zIGFuZCAoMikgY29tcHV0ZSBjcm9zcy1lbnRyb3B5IGxvc3MgdXNpbmcgdGhvc2Ugc21vb3RoZWQgbGFiZWxzIGZvciBhIG11bHRpLWNsYXNzIGNsYXNzaWZpY2F0aW9uIHByb2JsZW0uIFRoZSBpbXBsZW1lbnRhdGlvbiBzaG91bGQgaGFuZGxlIGRpZmZlcmVudCB2YWx1ZXMgb2YgdGhlIHNtb290aGluZyBwYXJhbWV0ZXIgZXBzaWxvbiwgdXNlIG51bWVyaWNhbGx5IHN0YWJsZSBsb2ctc29mdG1heCwgYW5kIHN1cHBvcnQgb3B0aW9uYWwgcm91bmRpbmcgb2YgdGhlIGZpbmFsIGxvc3MgdmFsdWUu",
  "id": "194",
  "test_cases": [
    {
      "test": "import numpy as np\nlogits = np.array([[2.0, 0.0, -1.0], [0.0, 1.0, 0.0]])\ny_true = np.array([0, 2])\nloss = label_smoothing_cross_entropy(logits, y_true, num_classes=3, epsilon=0.1, round_decimals=6)\nprint(loss)",
      "expected_output": "0.927312"
    },
    {
      "test": "import numpy as np\n# ε = 0 should match standard cross-entropy\nlogits = np.array([[3.0, 1.0, 0.0], [0.0, 1.0, 2.0], [2.0, 2.0, 2.0]])\ny_true = np.array([0, 2, 1])\ndef ce_no_smoothing(L, y):\n    L = np.asarray(L)\n    y = np.asarray(y)\n    m = L.max(axis=1, keepdims=True)\n    logsumexp = m + np.log(np.sum(np.exp(L - m), axis=1, keepdims=True))\n    logp = L - logsumexp\n    return float(-np.mean(logp[np.arange(L.shape[0]), y]))\nloss_eps0 = label_smoothing_cross_entropy(logits, y_true, num_classes=3, epsilon=0.0, round_decimals=8)\nloss_base = round(ce_no_smoothing(logits, y_true), 8)\nprint(round(loss_eps0 - loss_base, 8))",
      "expected_output": "0.0"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "logits = [[2.0, 0.0, -1.0], [0.0, 1.0, 0.0]]\ny_true = [0, 2]\nloss = label_smoothing_cross_entropy(logits, y_true, num_classes=3, epsilon=0.1, round_decimals=6)\nprint(loss)",
    "output": "0.927312",
    "reasoning": "With ε=0.1 and K=3, the true class gets 0.9333 probability and others 0.0333. Using stable log-softmax and averaging cross-entropy gives ≈ 0.927312."
  },
  "category": "Machine Learning",
  "starter_code": "import numpy as np\n\ndef smooth_labels(y_true, num_classes, epsilon):\n    \"\"\"\n    Create smoothed one-hot target vectors.\n\n    Args:\n        y_true: Iterable[int] of shape (N,) with values in [0, K-1]\n        num_classes: int, total number of classes (K)\n        epsilon: float in [0, 1]\n\n    Returns:\n        np.ndarray of shape (N, K) with smoothed probabilities.\n    \"\"\"\n    # Your implementation here\n    pass\n\n\ndef label_smoothing_cross_entropy(logits, y_true, num_classes, epsilon=0.1, round_decimals=None):\n    \"\"\"\n    Compute mean cross-entropy between logits and smoothed targets using stable log-softmax.\n\n    Args:\n        logits: Array-like of shape (N, K), model output scores.\n        y_true: Array-like of shape (N,), integer class indices.\n        num_classes: int, number of classes (K).\n        epsilon: float in [0, 1].\n        round_decimals: int | None, round the loss to this many decimals if given.\n\n    Returns:\n        float: Mean cross-entropy loss.\n    \"\"\"\n    # Your implementation here\n    pass\n",
  "title": "Implement Label Smoothing for Multi-Class Cross-Entropy",
  "createdAt": "November 9, 2025 at 1:47:53 PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement functions to (1) generate smoothed one-hot label distributions and (2) compute cross-entropy loss using those smoothed labels for a multi-class classification problem. The implementation should handle different values of the smoothing parameter epsilon, use numerically stable log-softmax, and support optional rounding of the final loss value.",
  "learn_section_decoded": "## Understanding Label Smoothing and Cross-Entropy\n\n### 1. Motivation\nIn classification tasks, targets are often one-hot encoded, meaning that the true class receives a probability of 1 and all others 0. This can cause **overconfidence** in neural networks. **Label smoothing** helps prevent overfitting by making targets slightly less certain.\n\n### 2. Label Smoothing Definition\nFor a $K$-class classification problem and smoothing parameter $\\varepsilon \\in [0,1]$, the smoothed target distribution for a true class $y$ is defined as:\n$$\n t_k =\n  \\begin{cases}\n    1 - \\varepsilon + \\frac{\\varepsilon}{K}, & \\text{if } k = y, \\\\\n    \\frac{\\varepsilon}{K}, & \\text{if } k \\neq y.\n  \\end{cases}\n$$\nHere, $\\varepsilon$ controls how much smoothing to apply:\n- $\\varepsilon = 0$: Standard one-hot encoding (no smoothing)\n- $\\varepsilon = 1$: Fully uniform distribution\n\nThe resulting smoothed vector still sums to 1.\n\n### 3. Cross-Entropy with Label Smoothing\nGiven model logits $z \\in \\mathbb{R}^K$ and smoothed target $t$, the **cross-entropy loss** is:\n$$\n\\mathcal{L}(z, t) = -\\sum_{k=1}^{K} t_k \\log p_k,\n$$\nwhere $p_k$ is the model’s predicted probability for class $k$:\n$$\np_k = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}.\n$$\n\n### 4. Numerical Stability with Log-Softmax\nTo avoid overflow when computing $e^{z_k}$, use the **log-sum-exp trick**:\n$$\n\\log p_k = z_k - \\Big(\\max_j z_j + \\log \\sum_{j=1}^{K} e^{z_j - \\max_j z_j}\\Big).\n$$\n\n### 5. Why It Matters\nLabel smoothing reduces the model’s overconfidence by distributing a small portion of probability mass across all classes, leading to:\n- Better calibration of predicted probabilities\n- Improved generalization\n- More stable gradients during training"
}