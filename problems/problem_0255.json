{
  "description": "SW1wbGVtZW50IHRoZSBGb2NhbCBMb3NzIGZ1bmN0aW9uLCB3aGljaCBpcyBkZXNpZ25lZCB0byBhZGRyZXNzIGNsYXNzIGltYmFsYW5jZSBpbiBjbGFzc2lmaWNhdGlvbiB0YXNrcyBieSBkb3duLXdlaWdodGluZyBlYXN5IGV4YW1wbGVzIGFuZCBmb2N1c2luZyBvbiBoYXJkIG1pc2NsYXNzaWZpZWQgZXhhbXBsZXMuIEdpdmVuIGdyb3VuZCB0cnV0aCBsYWJlbHMgKGFzIGNsYXNzIGluZGljZXMpLCBwcmVkaWN0ZWQgcHJvYmFiaWxpdGllcywgYSBmb2N1c2luZyBwYXJhbWV0ZXIgZ2FtbWEsIGFuZCBvcHRpb25hbCBjbGFzcyB3ZWlnaHRzIGFscGhhLCBjb21wdXRlIHRoZSBhdmVyYWdlIGZvY2FsIGxvc3MgYWNyb3NzIGFsbCBzYW1wbGVzLgoKVGhlIGZvY2FsIGxvc3MgbW9kaWZpZXMgdGhlIHN0YW5kYXJkIGNyb3NzLWVudHJvcHkgbG9zcyBieSBhZGRpbmcgYSBtb2R1bGF0aW5nIGZhY3RvciB0aGF0IHJlZHVjZXMgdGhlIGxvc3MgY29udHJpYnV0aW9uIGZyb20gd2VsbC1jbGFzc2lmaWVkIGV4YW1wbGVzLCBhbGxvd2luZyB0aGUgbW9kZWwgdG8gZm9jdXMgbGVhcm5pbmcgb24gaGFyZCBleGFtcGxlcy4KCllvdXIgZnVuY3Rpb24gc2hvdWxkOgoxLiBIYW5kbGUgYm90aCBudW1weSBhcnJheXMgYW5kIFB5dGhvbiBsaXN0cyBhcyBpbnB1dHMKMi4gQXBwbHkgY2xpcHBpbmcgdG8gcHJlZGljdGlvbnMgdG8gYXZvaWQgbnVtZXJpY2FsIGlzc3VlcyB3aXRoIGxvZygwKQozLiBTdXBwb3J0IGFuIG9wdGlvbmFsIGFscGhhIHBhcmFtZXRlciBmb3IgY2xhc3Mgd2VpZ2h0aW5nCjQuIFJldHVybiB0aGUgYXZlcmFnZSBmb2NhbCBsb3NzIGFzIGEgZmxvYXQ=",
  "id": "255",
  "test_cases": [
    {
      "test": "import numpy as np\nresult = focal_loss([0, 1, 2], [[0.9, 0.05, 0.05], [0.1, 0.8, 0.1], [0.1, 0.2, 0.7]], gamma=2.0)\nprint(round(result, 4))",
      "expected_output": "0.014"
    },
    {
      "test": "import numpy as np\nresult = focal_loss([0, 1, 0], [[0.8, 0.2], [0.3, 0.7], [0.6, 0.4]], gamma=2.0, alpha=[0.25, 0.75])\nprint(round(result, 4))",
      "expected_output": "0.0156"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "y_true = [0, 1, 2], y_pred = [[0.9, 0.05, 0.05], [0.1, 0.8, 0.1], [0.1, 0.2, 0.7]], gamma = 2.0",
    "output": "0.014",
    "reasoning": "For each sample, we first get the predicted probability for the true class: pt = [0.9, 0.8, 0.7]. Then we compute the focal weight: (1-pt)^gamma = [0.01, 0.04, 0.09]. The cross-entropy for each sample is: -log(pt) = [0.1054, 0.2231, 0.3567]. The focal loss for each sample is: focal_weight * ce = [0.00105, 0.00893, 0.0321]. The average focal loss is (0.00105 + 0.00893 + 0.0321) / 3 = 0.014."
  },
  "category": "Machine Learning",
  "starter_code": "import numpy as np\n\ndef focal_loss(y_true, y_pred, gamma=2.0, alpha=None):\n\t\"\"\"\n\tCompute Focal Loss for multi-class classification.\n\t\n\tArgs:\n\t\ty_true: Ground truth labels as class indices (list or 1D array)\n\t\ty_pred: Predicted probabilities (2D array, shape: [n_samples, n_classes])\n\t\tgamma: Focusing parameter (default: 2.0)\n\t\talpha: Class weights (optional, list or 1D array of length n_classes)\n\t\n\tReturns:\n\t\tfloat: Average focal loss\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Implement Focal Loss for Imbalanced Classification",
  "createdAt": "December 14, 2025 at 12:12:54â€¯PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement the Focal Loss function, which is designed to address class imbalance in classification tasks by down-weighting easy examples and focusing on hard misclassified examples. Given ground truth labels (as class indices), predicted probabilities, a focusing parameter gamma, and optional class weights alpha, compute the average focal loss across all samples.\n\nThe focal loss modifies the standard cross-entropy loss by adding a modulating factor that reduces the loss contribution from well-classified examples, allowing the model to focus learning on hard examples.\n\nYour function should:\n1. Handle both numpy arrays and Python lists as inputs\n2. Apply clipping to predictions to avoid numerical issues with log(0)\n3. Support an optional alpha parameter for class weighting\n4. Return the average focal loss as a float",
  "learn_section_decoded": "## Understanding Focal Loss\n\nFocal Loss was introduced in the paper \"Focal Loss for Dense Object Detection\" (Lin et al., 2017) to address the class imbalance problem in object detection, where background examples vastly outnumber foreground objects.\n\n### The Problem with Standard Cross-Entropy\n\nStandard cross-entropy loss for a single sample is:\n\n$$\nCE(p_t) = -\\log(p_t)\n$$\n\nwhere $p_t$ is the predicted probability for the true class. The issue is that even easily classified examples (high $p_t$) contribute to the loss, which can overwhelm the gradient from hard examples.\n\n### Focal Loss Definition\n\nFocal Loss adds a modulating factor $(1 - p_t)^\\gamma$ to the cross-entropy:\n\n$$\nFL(p_t) = -(1 - p_t)^\\gamma \\log(p_t)\n$$\n\nwhere:\n- $p_t$ is the model's estimated probability for the true class\n- $\\gamma \\geq 0$ is the focusing parameter\n\n### How the Focusing Parameter Works\n\n- When $\\gamma = 0$, Focal Loss equals standard Cross-Entropy\n- As $\\gamma$ increases, the effect of the modulating factor increases\n- For well-classified examples where $p_t$ is large (e.g., 0.9), $(1-p_t)^\\gamma$ becomes very small, reducing their contribution\n- For misclassified examples where $p_t$ is small (e.g., 0.1), the modulating factor is near 1, maintaining their full contribution\n\n### Adding Class Weights\n\nTo handle class imbalance further, we can add a weighting factor $\\alpha_t$:\n\n$$\nFL(p_t) = -\\alpha_t (1 - p_t)^\\gamma \\log(p_t)\n$$\n\nwhere $\\alpha_t$ is the weight for the true class.\n\n### Example Calculation\n\nConsider a sample with true class 0 and predictions $[0.9, 0.1]$:\n\n- $p_t = 0.9$ (well-classified)\n- With $\\gamma = 2$: $(1-0.9)^2 = 0.01$\n- Cross-entropy: $-\\log(0.9) \\approx 0.105$\n- Focal Loss: $0.01 \\times 0.105 = 0.00105$\n\nCompare to a misclassified example with $p_t = 0.2$:\n\n- With $\\gamma = 2$: $(1-0.2)^2 = 0.64$\n- Cross-entropy: $-\\log(0.2) \\approx 1.609$\n- Focal Loss: $0.64 \\times 1.609 = 1.03$\n\nThe focal loss for the hard example is nearly 1000x larger than for the easy example, allowing the model to focus on learning from hard cases."
}