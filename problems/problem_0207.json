{
  "description": "SW1wbGVtZW50IE1vbnRlIENhcmxvIFRyZWUgU2VhcmNoIChNQ1RTKSBmb3IgYSBzaW1wbGUgc2VxdWVudGlhbCBnYW1lLiBUaGUgZ2FtZTogc3RhcnQgYXQgYSBudW1iZXIsIHBsYXllcnMgYWx0ZXJuYXRlIGFkZGluZyBlaXRoZXIgMSBvciAyLCBhbmQgd2hvZXZlciByZWFjaGVzIGV4YWN0bHkgdGhlIHRhcmdldCB2YWx1ZSB3aW5zIChnb2luZyBvdmVyIG1lYW5zIHlvdSBsb3NlKS4gTUNUUyBzaG91bGQgZXhwbG9yZSBwb3NzaWJsZSBtb3ZlcyB0aHJvdWdoIHJhbmRvbSBzaW11bGF0aW9ucyBhbmQgcmV0dXJuIHRoZSBiZXN0IGZpcnN0IGFjdGlvbi4gVGhlIGFsZ29yaXRobSBoYXMgZm91ciBwaGFzZXM6IFNlbGVjdGlvbiAocGljayBwcm9taXNpbmcgbm9kZXMpLCBFeHBhbnNpb24gKGFkZCBuZXcgbm9kZXMpLCBTaW11bGF0aW9uIChyYW5kb20gcGxheW91dCksIGFuZCBCYWNrcHJvcGFnYXRpb24gKHVwZGF0ZSBzdGF0aXN0aWNzKS4=",
  "id": "207",
  "test_cases": [
    {
      "test": "import numpy as np; action = mcts_search(0, 2, iterations=1000, seed=42); print(action)",
      "expected_output": "1"
    },
    {
      "test": "import numpy as np; action = mcts_search(1, 2, iterations=1000, seed=42); print(action)",
      "expected_output": "2"
    }
  ],
  "difficulty": "hard",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "initial_state=1, max_value=2, iterations=1000, seed=42",
    "output": "2",
    "reasoning": "Starting at 1 with goal 2, the player can choose +1 (reaching 2, winning immediately) or +2 (reaching 3, going over and losing). However, MCTS considers this as a two-player game where the opponent also plays optimally. Through simulation, it discovers that the action leading to the best win rate is 2, likely due to how the adversarial game tree unfolds with alternating players."
  },
  "category": "Reinforcement Learning",
  "starter_code": "import numpy as np\nfrom typing import Optional\n\nclass MCTSNode:\n\tdef __init__(self, state: int, parent: Optional['MCTSNode'] = None):\n\t\tself.state = state\n\t\tself.parent = parent\n\t\tself.children = {}\n\t\tself.visits = 0\n\t\tself.value = 0.0\n\t\n\tdef is_leaf(self):\n\t\treturn len(self.children) == 0\n\t\n\tdef ucb1(self, c: float = 1.414) -> float:\n\t\t\"\"\"Upper Confidence Bound for Trees.\"\"\"\n\t\t# Your code here\n\t\tpass\n\ndef mcts_search(initial_state: int, max_value: int, iterations: int, seed: int = 42) -> int:\n\t\"\"\"\n\tMonte Carlo Tree Search for sequential game.\n\t\n\tArgs:\n\t\tinitial_state: Starting value\n\t\tmax_value: Target value to reach\n\t\titerations: Number of MCTS iterations\n\t\tseed: Random seed\n\t\n\tReturns:\n\t\tBest action (1 or 2)\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Monte Carlo Tree Search",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "createdAt": "November 17, 2025 at 6:41:58 PM UTC-0500",
  "description_decoded": "Implement Monte Carlo Tree Search (MCTS) for a simple sequential game. The game: start at a number, players alternate adding either 1 or 2, and whoever reaches exactly the target value wins (going over means you lose). MCTS should explore possible moves through random simulations and return the best first action. The algorithm has four phases: Selection (pick promising nodes), Expansion (add new nodes), Simulation (random playout), and Backpropagation (update statistics).",
  "learn_section_decoded": "### Understanding Monte Carlo Tree Search\n\nMonte Carlo Tree Search (MCTS) is a powerful algorithm for decision-making in games and planning problems. Unlike traditional minimax search, MCTS doesn't require evaluating every possible move - instead, it intelligently samples the most promising paths through random simulations.\n\n#### The Core Idea\n\nMCTS builds a search tree incrementally by running many simulated games (rollouts). It focuses computational effort on moves that appear promising while maintaining exploration of alternatives. After many iterations, the statistics guide us to the best action.\n\n#### The Four Phases\n\nEach MCTS iteration consists of four steps:\n\n**1. Selection**: Starting from the root, traverse the tree by selecting child nodes that balance exploitation (good past results) and exploration (uncertainty). Use UCB1 formula:\n\n$$\n\\text{UCB1}(n) = \\frac{Q(n)}{N(n)} + c\\sqrt{\\frac{\\ln N(\\text{parent})}{N(n)}}\n$$\n\nWhere:\n- $Q(n)$ = total reward from node $n$\n- $N(n)$ = visit count of node $n$  \n- $c$ = exploration constant (typically $\\sqrt{2} \\approx 1.414$)\n- First term = exploitation (average value)\n- Second term = exploration (uncertainty bonus)\n\n**2. Expansion**: When reaching a leaf node that's not terminal, add one or more child nodes representing available actions.\n\n**3. Simulation**: From the new node, play out the game randomly until reaching a terminal state. This gives a Monte Carlo estimate of the node's value.\n\n**4. Backpropagation**: Propagate the simulation result back up the tree, updating visit counts and values for all nodes on the path.\n\n#### UCB1: Balancing Exploration and Exploitation\n\nThe UCB1 formula is key to MCTS's success:\n\n**Exploitation term** $\\frac{Q(n)}{N(n)}$: Prefers nodes with high average rewards.\n\n**Exploration term** $c\\sqrt{\\frac{\\ln N(\\text{parent})}{N(n)}}$: Prefers nodes visited less often. As parent visits increase while node visits stay low, this term grows, ensuring unexplored options get tried.\n\n**Unvisited nodes**: Get $\\text{UCB1} = \\infty$, ensuring all children are tried at least once before any are revisited.\n\n#### Example: Simple Game\n\nGame: Start at 0, reach exactly 5. Can add 1 or 2 each turn.\n\n**Initial state**: Root node (state=0), no children.\n\n**Iteration 1**:\n- Selection: At root (leaf)\n- Expansion: Add children for actions +1 and +2\n- Simulation: Pick child (state=1), randomly play to end\n- Backpropagation: Update root and child statistics\n\n**Iteration 10**: Tree has grown. At root:\n- Action +1: 6 visits, 2 wins → avg = 0.33\n- Action +2: 3 visits, 0 wins → avg = 0.00\n\nFor action +1: $\\text{UCB1} = 0.33 + 1.414\\sqrt{\\frac{\\ln(10)}{6}} = 0.33 + 0.88 = 1.21$\n\nFor action +2: $\\text{UCB1} = 0.00 + 1.414\\sqrt{\\frac{\\ln(10)}{3}} = 0.00 + 1.24 = 1.24$\n\nAction +2 selected (higher UCB1 due to more uncertainty).\n\n**After 1000 iterations**: Visit counts reflect true action quality. Select action with most visits.\n\n#### Action Selection\n\nAfter all iterations, choose the action by:\n\n**Visit count** (most common): Pick child with highest $N(n)$. Robust to outliers.\n\n**Average value** (alternative): Pick child with highest $Q(n)/N(n)$. Can be sensitive to luck.\n\n**Max child**: Pick child with best single playout. Very risky.\n\nTypically use visit count for reliability.\n\n#### Why MCTS Works\n\n**Anytime algorithm**: Can be stopped after any number of iterations with a valid (if suboptimal) answer.\n\n**Asymptotic correctness**: With infinite iterations, MCTS converges to optimal play (under assumptions).\n\n**Handles large state spaces**: Focuses computation on promising regions rather than exhaustive search.\n\n**No domain knowledge required**: Works with just a simulator and win/loss detection. No need for hand-crafted evaluation functions.\n\n#### Key Properties\n\n**Tree policy**: UCB1 guides exploration in the tree (explored states).\n\n**Default policy**: Random playouts outside the tree (unexplored states).\n\n**Value estimates**: Based on Monte Carlo sampling, not exact calculation.\n\n**Progressive widening**: Tree grows toward promising actions automatically.\n\n#### Applications\n\n**Game AI**: AlphaGo used MCTS with neural networks to master Go. Also used in Chess, Poker, and many board games.\n\n**Planning**: Robot path planning, resource allocation, scheduling problems.\n\n**Reinforcement Learning**: Can be combined with learned value functions (AlphaZero).\n\n**Real-time Strategy**: Used in games requiring quick decisions with large action spaces.\n\n#### Improvements and Variations\n\n**Domain knowledge**: Can enhance simulation policy with heuristics instead of pure randomness.\n\n**Rapid Action Value Estimation (RAVE)**: Shares information between related nodes.\n\n**Progressive bias**: Incorporates prior knowledge into selection.\n\n**Parallel MCTS**: Run multiple simulations simultaneously on different threads.\n\n**Neural MCTS**: Use neural networks for evaluation (AlphaGo, AlphaZero)."
}