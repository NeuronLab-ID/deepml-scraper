{
  "description": "IyMgVGFzazogSW1wbGVtZW50IEtMIERpdmVyZ2VuY2UgQmV0d2VlbiBUd28gTm9ybWFsIERpc3RyaWJ1dGlvbnMKCllvdXIgdGFzayBpcyB0byBjb21wdXRlIHRoZSBLdWxsYmFjayBMZWlibGVyIChLTCkgZGl2ZXJnZW5jZSBiZXR3ZWVuIHR3byBub3JtYWwgZGlzdHJpYnV0aW9ucy4gS0wgZGl2ZXJnZW5jZSBtZWFzdXJlcyBob3cgb25lIHByb2JhYmlsaXR5IGRpc3RyaWJ1dGlvbiBkaWZmZXJzIGZyb20gYSBzZWNvbmQsIHJlZmVyZW5jZSBwcm9iYWJpbGl0eSBkaXN0cmlidXRpb24uCgpXcml0ZSBhIGZ1bmN0aW9uIGtsX2RpdmVyZ2VuY2Vfbm9ybWFsKG11X3AsIHNpZ21hX3AsIG11X3EsIHNpZ21hX3EpIHRoYXQgY2FsY3VsYXRlcyB0aGUgS0wgZGl2ZXJnZW5jZSBiZXR3ZWVuIHR3byBub3JtYWwgZGlzdHJpYnV0aW9ucy4KClRoZSBmdW5jdGlvbiBzaG91bGQgcmV0dXJuIHRoZSBLTCBkaXZlcmdlbmNlIGFzIGEgZmxvYXRpbmcgcG9pbnQgbnVtYmVyLgoKICAgIA==",
  "mdx_file": "e7a03d3b-e527-4457-92c8-4d1a5268cdec.mdx",
  "id": "56",
  "test_cases": [
    {
      "test": "import numpy as np\n\nmu_p = 0.0\nsigma_p = 1.0\nmu_q = 0.0\nsigma_q = 1.0\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))",
      "expected_output": "0.0"
    },
    {
      "test": "import numpy as np\n\nmu_p = 0.0\nsigma_p = 1.0\nmu_q = 1.0\nsigma_q = 1.0\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))",
      "expected_output": "0.5"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "example": {
    "input": "mu_p = 0.0\nsigma_p = 1.0\nmu_q = 1.0\nsigma_q = 1.0\n\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))",
    "output": "0.5",
    "reasoning": "The KL divergence between the normal distributions \\( P \\) and \\( Q \\) with parameters \\( \\mu_P = 0.0 \\), \\( \\sigma_P = 1.0 \\) and \\( \\mu_Q = 1.0 \\), \\( \\sigma_Q = 1.0 \\) is 0.5."
  },
  "dislikes": "0",
  "category": "Deep Learning",
  "starter_code": "import numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n\treturn 0.0\n",
  "learn_section": "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",
  "title": "KL Divergence Between Two Normal Distributions",
  "contributor": [
    {
      "profile_link": "https://github.com/Hui-cd",
      "name": "Hui"
    }
  ],
  "description_decoded": "## Task: Implement KL Divergence Between Two Normal Distributions\n\nYour task is to compute the Kullback Leibler (KL) divergence between two normal distributions. KL divergence measures how one probability distribution differs from a second, reference probability distribution.\n\nWrite a function kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q) that calculates the KL divergence between two normal distributions.\n\nThe function should return the KL divergence as a floating point number.\n\n    ",
  "learn_section_decoded": "## Understanding Kullback-Leibler Divergence (KL Divergence)\n\nThe **Kullback-Leibler (KL) divergence**, also known as relative entropy, measures the difference between two probability distributions. It quantifies how much information is lost when approximating one distribution with another.\n\n---\n\n### Definition of KL Divergence\n\nFor continuous variables, the KL divergence is defined as:\n\n$$\nKL(P \\parallel Q) = \\int p(x) \\log \\frac{p(x)}{q(x)} \\, dx\n$$\n\nwhere:\n- $p(x)$ is the probability density function of the **reference** distribution $P$.\n- $q(x)$ is the probability density function of the **comparison** distribution $Q$.\n\n---\n\n### KL Divergence Between Two Normal Distributions\n\nConsider two normal distributions $P$ and $Q$:\n\n- $P \\sim N(\\mu_P, \\sigma_P^2)$  \n- $Q \\sim N(\\mu_Q, \\sigma_Q^2)$\n\nFor these, the KL divergence simplifies to:\n\n$$\nKL(P \\parallel Q) = \\int p(x) \\left[\n\\log \\frac{\\sigma_Q}{\\sigma_P}\n+ \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2\\sigma_Q^2}\n- \\frac{1}{2}\n\\right] dx\n$$\n\nSince $p(x)$ is the PDF of $x$ under $P$, the integral over $p(x)$ just multiplies by 1 for each constant term. Thus, the final closed form is:\n\n$$\nKL(P \\parallel Q) =\n\\log \\frac{\\sigma_Q}{\\sigma_P}\n+ \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2\\sigma_Q^2}\n- \\frac{1}{2}\n$$\n\n---\n\n### Interpretation\n\nThis expression quantifies how one normal distribution $P$ **diverges** from another normal distribution $Q$. A KL divergence of zero indicates the two distributions are identical. As the divergence grows, it signals that $Q$ is a poorer approximation of $P$.\n\nThe KL divergence is **asymmetric**:\n\n$$\nKL(P \\parallel Q) \\neq KL(Q \\parallel P)\n$$\n\nmaking it sensitive to the **direction** of comparison.\n"
}