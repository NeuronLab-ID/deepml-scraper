{
  "description": "SW1wbGVtZW50IGEgc2ltcGxpZmllZCBuZXVyYWwgbWVtb3J5IHVwZGF0ZSBtZWNoYW5pc20gaW5zcGlyZWQgYnkgdGhlIFRpdGFucyBhcmNoaXRlY3R1cmUuIFRoZSBtZW1vcnkgdXBkYXRlIHJ1bGUgY29tYmluZXMgdGhyZWUga2V5IGNvbmNlcHRzOiAoMSkgYSBzdXJwcmlzZSBtZXRyaWMgYmFzZWQgb24gcHJlZGljdGlvbiBlcnJvciwgKDIpIG1vbWVudHVtIHRvIHRyYWNrIHN1cnByaXNlIGFjcm9zcyB0aW1lIHN0ZXBzLCBhbmQgKDMpIGEgZm9yZ2V0dGluZyBtZWNoYW5pc20gdmlhIHdlaWdodCBkZWNheS4gR2l2ZW4gYSBtZW1vcnkgc3RhdGUgTSwgaW5jb21pbmcga2V5LXZhbHVlIHBhaXIgKGssIHYpLCBhbmQgcGFyYW1ldGVycyBmb3IgbGVhcm5pbmcgcmF0ZSAodGhldGEpLCBtb21lbnR1bSBkZWNheSAoZXRhKSwgYW5kIGZvcmdldCBnYXRlIChhbHBoYSksIGNvbXB1dGUgdGhlIHVwZGF0ZWQgbWVtb3J5IHN0YXRlLiBUaGUgc3VycHJpc2UgaXMgbWVhc3VyZWQgYXMgdGhlIGdyYWRpZW50IG9mIHRoZSBhc3NvY2lhdGl2ZSBtZW1vcnkgbG9zcyB8fE0gQCBrIC0gdnx8XjIgd2l0aCByZXNwZWN0IHRvIE0sIHdoaWNoIGVxdWFscyAoTSBAIGsgLSB2KSBAIGsuVC4gVGhlIG1vbWVudHVtIGFjY3VtdWxhdGVzIHBhc3Qgc3VycHJpc2VzLCBhbmQgdGhlIGZvcmdldCBnYXRlIGNvbnRyb2xzIGhvdyBtdWNoIG9mIHRoZSBvbGQgbWVtb3J5IHRvIHJldGFpbi4=",
  "id": "267",
  "test_cases": [
    {
      "test": "import numpy as np\nnp.set_printoptions(precision=4, suppress=True)\nM = np.array([[1.0, 0.0], [0.0, 1.0]])\nS = np.array([[0.0, 0.0], [0.0, 0.0]])\nk = np.array([1.0, 0.0])\nv = np.array([2.0, 0.0])\nM_new, S_new = neural_memory_update(M, S, k, v)\nprint(M_new.tolist())",
      "expected_output": "[[1.09, 0.0], [0.0, 0.99]]"
    },
    {
      "test": "import numpy as np\nnp.set_printoptions(precision=4, suppress=True)\nM = np.array([[1.0, 0.0], [0.0, 1.0]])\nS = np.array([[0.0, 0.0], [0.0, 0.0]])\nk = np.array([1.0, 0.0])\nv = np.array([2.0, 0.0])\nM_new, S_new = neural_memory_update(M, S, k, v)\nprint(S_new.tolist())",
      "expected_output": "[[0.1, 0.0], [0.0, 0.0]]"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "M = [[1, 0], [0, 1]], S = [[0, 0], [0, 0]], k = [1, 0], v = [2, 0], theta = 0.1, eta = 0.9, alpha = 0.01",
    "output": "M_new = [[1.09, 0.0], [0.0, 0.99]], S_new = [[0.1, 0.0], [0.0, 0.0]]",
    "reasoning": "The memory tries to predict v from k: prediction = M @ k = [1, 0]. The error is [1, 0] - [2, 0] = [-1, 0]. The momentary surprise (gradient) is outer([-1, 0], [1, 0]) = [[-1, 0], [0, 0]]. The new momentum S_new = 0.9 * [[0,0],[0,0]] - 0.1 * [[-1,0],[0,0]] = [[0.1, 0], [0, 0]]. The new memory M_new = 0.99 * [[1,0],[0,1]] + [[0.1,0],[0,0]] = [[1.09, 0], [0, 0.99]]."
  },
  "category": "Deep Learning",
  "starter_code": "import numpy as np\n\ndef neural_memory_update(\n    M: np.ndarray,\n    S: np.ndarray,\n    k: np.ndarray,\n    v: np.ndarray,\n    theta: float = 0.1,\n    eta: float = 0.9,\n    alpha: float = 0.01\n) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Update neural memory using surprise-based learning with momentum and forgetting.\n    \n    Args:\n        M: Current memory state matrix of shape (d, d)\n        S: Current momentum/surprise accumulator of shape (d, d)\n        k: Key vector of shape (d,)\n        v: Value vector of shape (d,)\n        theta: Learning rate for momentary surprise (default: 0.1)\n        eta: Momentum decay factor for past surprise (default: 0.9)\n        alpha: Forget gate - fraction of old memory to forget (default: 0.01)\n    \n    Returns:\n        Tuple of (updated_M, updated_S) where:\n        - updated_M: New memory state after update\n        - updated_S: New momentum state after update\n    \n    \"\"\"\n    pass",
  "title": "Implement Neural Memory Update with Surprise and Momentum",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "createdAt": "December 15, 2025 at 1:45:15â€¯PM UTC-0500",
  "description_decoded": "Implement a simplified neural memory update mechanism inspired by the Titans architecture. The memory update rule combines three key concepts: (1) a surprise metric based on prediction error, (2) momentum to track surprise across time steps, and (3) a forgetting mechanism via weight decay. Given a memory state M, incoming key-value pair (k, v), and parameters for learning rate (theta), momentum decay (eta), and forget gate (alpha), compute the updated memory state. The surprise is measured as the gradient of the associative memory loss ||M @ k - v||^2 with respect to M, which equals (M @ k - v) @ k.T. The momentum accumulates past surprises, and the forget gate controls how much of the old memory to retain.",
  "learn_section_decoded": "## Neural Memory Update in Titans Architecture\n\nThe Titans architecture introduces a novel neural long-term memory module that learns to memorize at test time. This is fundamentally different from traditional approaches where model weights are fixed after training.\n\n### Key Concepts\n\n#### 1. Associative Memory Loss\n\nThe memory module learns to store key-value associations. Given a key $\\mathbf{k}$ and value $\\mathbf{v}$, the loss measures how well the memory $\\mathbf{M}$ can retrieve $\\mathbf{v}$ from $\\mathbf{k}$:\n\n$$\\ell(\\mathbf{M}; \\mathbf{k}, \\mathbf{v}) = \\|\\mathbf{M}\\mathbf{k} - \\mathbf{v}\\|_2^2$$\n\n#### 2. Surprise Metric\n\nThe **surprise** of an input is measured by the gradient of this loss. More surprising inputs (those the memory cannot predict well) get larger updates:\n\n$$\\text{Momentary Surprise} = \\nabla_\\mathbf{M} \\ell = (\\mathbf{M}\\mathbf{k} - \\mathbf{v})\\mathbf{k}^\\top$$\n\nThis is inspired by human memory: events that violate expectations (surprising events) are more memorable.\n\n#### 3. Momentum for Temporal Context\n\nA single surprising moment should influence memory for subsequent tokens. The momentum term $\\mathbf{S}$ accumulates surprise over time:\n\n$$\\mathbf{S}_t = \\eta \\mathbf{S}_{t-1} - \\theta \\nabla_\\mathbf{M} \\ell$$\n\nWhere $\\eta$ controls how much past surprise is retained (momentum decay) and $\\theta$ controls the learning rate for momentary surprise. This addresses a limitation of prior work that only considered momentary surprise, missing the flow of information across tokens.\n\n#### 4. Forgetting Mechanism\n\nTo manage memory capacity over long sequences, a forget gate $\\alpha$ allows the memory to discard old information:\n\n$$\\mathbf{M}_t = (1 - \\alpha)\\mathbf{M}_{t-1} + \\mathbf{S}_t$$\n\nWhen $\\alpha \\to 0$, the memory retains all past information. When $\\alpha \\to 1$, the memory is cleared. This data-dependent forgetting is crucial for handling sequences with millions of tokens.\n\n### Complete Update Rule\n\nCombining all components:\n\n$$\\mathbf{S}_t = \\eta \\mathbf{S}_{t-1} - \\theta (\\mathbf{M}_{t-1}\\mathbf{k}_t - \\mathbf{v}_t)\\mathbf{k}_t^\\top$$\n$$\\mathbf{M}_t = (1 - \\alpha)\\mathbf{M}_{t-1} + \\mathbf{S}_t$$\n\n### Connection to Optimization\n\nThis update rule is mathematically equivalent to gradient descent with momentum and weight decay. The $\\mathbf{S}$ term acts as momentum in optimization, while the $(1-\\alpha)$ factor acts as L2 regularization. This connection enables efficient parallelizable training using matrix operations.\n\n### Why This Matters\n\nThe surprise metric ensures the memory focuses on novel information rather than wasting capacity on predictable tokens. The momentum component tracks information flow across the sequence, addressing limitations of prior approaches that only considered instantaneous surprise. The forgetting mechanism prevents memory overflow when processing extremely long contexts. Together, these components enable Titans to scale to contexts larger than 2 million tokens while outperforming both Transformers and modern linear recurrent models on long-context tasks."
}