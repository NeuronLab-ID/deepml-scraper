{
  "description": "V3JpdGUgYSBQeXRob24gY2xhc3MgdG8gaW1wbGVtZW50IE1peGVkIFByZWNpc2lvbiBUcmFpbmluZyB0aGF0IHVzZXMgYm90aCBmbG9hdDMyIGFuZCBmbG9hdDE2IGRhdGEgdHlwZXMgdG8gb3B0aW1pemUgbWVtb3J5IHVzYWdlIGFuZCBzcGVlZC4gWW91ciBjbGFzcyBzaG91bGQgaGF2ZSBhbiBgX19pbml0X18oc2VsZiwgbG9zc19zY2FsZT0xMDI0LjApYCBtZXRob2QgdG8gaW5pdGlhbGl6ZSB3aXRoIGxvc3Mgc2NhbGluZyBmYWN0b3IuIEltcGxlbWVudCBgZm9yd2FyZChzZWxmLCB3ZWlnaHRzLCBpbnB1dHMsIHRhcmdldHMpYCB0byBwZXJmb3JtIGZvcndhcmQgcGFzcyB3aXRoIGZsb2F0MTYgY29tcHV0YXRpb24gYW5kIHJldHVybiBNZWFuIFNxdWFyZWQgRXJyb3IgKE1TRSkgbG9zcyAoc2NhbGVkKSBpbiBmbG9hdDMyLCBhbmQgYGJhY2t3YXJkKHNlbGYsIGdyYWRpZW50cylgIHRvIHVuc2NhbGUgZ3JhZGllbnRzIGFuZCBjaGVjayBmb3Igb3ZlcmZsb3cuIFVzZSBmbG9hdDE2IGZvciBjb21wdXRhdGlvbnMgYnV0IGZsb2F0MzIgZm9yIGdyYWRpZW50IGFjY3VtdWxhdGlvbi4gUmV0dXJuIGdyYWRpZW50cyBhcyBmbG9hdDMyIGFuZCBzZXQgdGhlbSB0byB6ZXJvIGlmIG92ZXJmbG93IGlzIGRldGVjdGVkLiBPbmx5IHVzZSBOdW1QeS4=",
  "id": "160",
  "test_cases": [
    {
      "test": "import numpy as np\nmp = MixedPrecision(loss_scale=1024.0)\nweights = np.array([0.5, -0.3], dtype=np.float32)\ninputs = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\ntargets = np.array([1.0, 0.0], dtype=np.float32)\nloss = mp.forward(weights, inputs, targets)\nprint(f\"Loss: {loss:.4f}\")\nprint(f\"Loss dtype: {type(loss).__name__}\")",
      "expected_output": "Loss: 665.0000\nLoss dtype: float"
    },
    {
      "test": "import numpy as np\nmp = MixedPrecision(loss_scale=1024.0)\ngrads = np.array([512.0, -256.0], dtype=np.float32)\nresult = mp.backward(grads)\nprint(f\"Gradients: {result}\")\nprint(f\"Grad dtype: {result.dtype}\")",
      "expected_output": "Gradients: [ 0.5  -0.25]\nGrad dtype: float32"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "import numpy as np\nmp = MixedPrecision(loss_scale=1024.0)\nweights = np.array([0.5, -0.3], dtype=np.float32)\ninputs = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\ntargets = np.array([1.0, 0.0], dtype=np.float32)\nloss = mp.forward(weights, inputs, targets)\nprint(f\"Loss: {loss:.4f}\")\nprint(f\"Loss dtype: {type(loss).__name__}\")\ngrads = np.array([512.0, -256.0], dtype=np.float32)\nresult = mp.backward(grads)\nprint(f\"Gradients: {result}\")\nprint(f\"Grad dtype: {result.dtype}\")",
    "output": "Loss: 665.0000\nLoss dtype: float\nGradients: [0.5 -0.25]\nGrad dtype: float32",
    "reasoning": "Forward pass converts inputs to float16, computes loss, then scales and returns as Python float (float32). Backward converts gradients to float32 and unscales. Final gradients must be float32 type."
  },
  "category": "Machine Learning",
  "starter_code": "import numpy as np\n\nclass MixedPrecision:\n    def __init__(self, loss_scale=1024.0):\n        # Initialize loss scaling factor\n        pass\n    \n    def forward(self, weights, inputs, targets):\n        # Perform forward pass with float16, return scaled loss as float32\n        pass\n    \n    def backward(self, gradients):\n        # Unscale gradients and check for overflow, return as float32\n        pass",
  "title": "Mixed Precision Training",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/komaksym",
      "name": "komaksym"
    }
  ],
  "createdAt": "September 15, 2025 at 10:08:30â€¯AM UTUTC-4",
  "description_decoded": "Write a Python class to implement Mixed Precision Training that uses both float32 and float16 data types to optimize memory usage and speed. Your class should have an `__init__(self, loss_scale=1024.0)` method to initialize with loss scaling factor. Implement `forward(self, weights, inputs, targets)` to perform forward pass with float16 computation and return Mean Squared Error (MSE) loss (scaled) in float32, and `backward(self, gradients)` to unscale gradients and check for overflow. Use float16 for computations but float32 for gradient accumulation. Return gradients as float32 and set them to zero if overflow is detected. Only use NumPy.",
  "learn_section_decoded": "# **Mixed Precision Training**\n## **1. Definition**\nMixed Precision Training is a **deep learning optimization technique** that uses both **float16** (half precision) and **float32** (single precision) data types during training to reduce memory usage and increase training speed while maintaining model accuracy.\nThe technique works by:\n- **Using float16 for forward pass computations** to save memory and increase speed\n- **Using float32 for gradient accumulation** to maintain numerical precision\n- **Applying loss scaling** to prevent gradient underflow in float16\n---\n## **2. Key Components**\n### **Mean Squared Error (MSE) Loss**\nThe loss function must be computed as Mean Squared Error:\n$$\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$$\nwhere $y_i$ is the target and $\\hat{y}_i$ is the prediction for sample $i$.\n\n### **Loss Scaling**\nTo prevent gradient underflow in float16, gradients are scaled up during the forward pass:\n$$\n\\text{scaled\\_loss} = \\text{MSE} \\times \\text{scale\\_factor}\n$$\nThen unscaled during backward pass:\n$$\n\\text{gradient} = \\frac{\\text{scaled\\_gradient}}{\\text{scale\\_factor}}\n$$\n### **Overflow Detection**\nCheck for invalid gradients (NaN or Inf) that indicate numerical overflow:\n$$\n\\text{overflow} = \\text{any}(\\text{isnan}(\\text{gradients}) \\text{ or } \\text{isinf}(\\text{gradients}))\n$$\n---\n## **3. Precision Usage**\n- **float16**: Forward pass computations, activations, temporary calculations\n- **float32**: Gradient accumulation, parameter updates, loss scaling\n- **Automatic casting**: Convert between precisions as needed\n- **Loss computation**: Use MSE as the loss function before scaling\n---\n## **4. Benefits and Applications**\n- **Memory Efficiency**: Reduces memory usage by ~50% for activations\n- **Speed Improvement**: Faster computation on modern GPUs with Tensor Cores\n- **Training Stability**: Loss scaling prevents gradient underflow\n- **Model Accuracy**: Maintains comparable accuracy to full precision training\nCommon in training large neural networks where memory is a constraint and speed is critical.\n---"
}