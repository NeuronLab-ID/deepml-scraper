{
  "description": "SW1wbGVtZW50IGEgZ3JhZGllbnQgZGVzY2VudCB0cmFpbmluZyBhbGdvcml0aG0gZm9yIGxvZ2lzdGljIHJlZ3Jlc3Npb24uIFlvdXIgdGFzayBpcyB0byBjb21wdXRlIG1vZGVsIHBhcmFtZXRlcnMgdXNpbmcgQmluYXJ5IENyb3NzIEVudHJvcHkgbG9zcyBhbmQgcmV0dXJuIHRoZSBvcHRpbWl6ZWQgY29lZmZpY2llbnRzIGFsb25nIHdpdGggbG9zcyB2YWx1ZXMgY29sbGVjdGVkIG92ZXIgaXRlcmF0aW9ucy4KClNwZWNpZmljYXRpb25zOgotIEFkZCBhIGJpYXMgdGVybSAoY29sdW1uIG9mIG9uZXMpIGFzIHRoZSBGSVJTVCBjb2x1bW4gb2YgdGhlIGZlYXR1cmUgbWF0cml4Ci0gSW5pdGlhbGl6ZSBhbGwgY29lZmZpY2llbnRzIHRvIFpFUk8KLSBVc2UgU1VNLWJhc2VkIEJDRSBsb3NzIChub3QgbWVhbik6IEwgPSAtzqNbecK3bG9nKHApICsgKDEteSnCt2xvZygxLXApXQotIFVwZGF0ZSBydWxlOiDOsiA9IM6yIC0gbHIgw5cgWF5UIMOXIChwcmVkaWN0aW9ucyAtIHkp",
  "id": "106",
  "test_cases": [
    {
      "test": "coeffs, losses = train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)\nprint(([round(c, 4) for c in coeffs], [round(l, 4) for l in losses]))",
      "expected_output": "([-0.0097, 0.0286, 0.015, 0.0135, 0.0316], [6.9315, 6.9075, 6.8837, 6.8601, 6.8367, 6.8134, 6.7904, 6.7675, 6.7448, 6.7223])"
    },
    {
      "test": "coeffs, losses = train_logreg(np.array([[ 0.76743473,  1.57921282, -0.46947439],[-0.23415337,  1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869,  0.37569802, -0.29169375],[-1.91328024,  0.24196227, -1.72491783],[-1.01283112, -0.56228753,  0.31424733],[-0.1382643 ,  0.49671415,  0.64768854],[-0.46341769,  0.54256004, -0.46572975],[-1.4123037 , -0.90802408,  1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10)\nprint(([round(c, 4) for c in coeffs], [round(l, 4) for l in losses]))",
      "expected_output": "([-0.2509, 0.9325, 1.6218, 0.6336], [6.9315, 5.5073, 4.6382, 4.0609, 3.6503, 3.3432, 3.1045, 2.9134, 2.7567, 2.6258])"
    }
  ],
  "difficulty": "hard",
  "pytorch_difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "X = np.array([[1.0, 0.5], [-0.5, -1.5], [2.0, 1.5], [-2.0, -1.0]]), y = np.array([1, 0, 1, 0]), learning_rate = 0.01, iterations = 5",
    "output": "([-0.0001, 0.128, 0.1053], [2.7726, 2.6485, 2.533, 2.4254, 2.325])",
    "reasoning": "The function adds a bias column to X (making it 4×3), initializes 3 coefficients to zero, then iteratively updates them using gradient descent. After 5 iterations, the loss decreases from 2.77 to 2.33, showing the model is learning."
  },
  "category": "Machine Learning",
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpkZWYgbG9yYV9mb3J3YXJkKAogICAgeDogdG9yY2guVGVuc29yLAogICAgVzogdG9yY2guVGVuc29yLAogICAgQTogdG9yY2guVGVuc29yLAogICAgQjogdG9yY2guVGVuc29yLAogICAgYWxwaGE6IGZsb2F0ID0gMS4wCikgLT4gdG9yY2guVGVuc29yOgogICAgIiIiCiAgICBDb21wdXRlIHRoZSBMb1JBIGZvcndhcmQgcGFzcyB1c2luZyBQeVRvcmNoLgogICAgCiAgICBBcmdzOgogICAgICAgIHg6IElucHV0IHRlbnNvciAoYmF0Y2hfc2l6ZSB4IGluX2ZlYXR1cmVzKQogICAgICAgIFc6IEZyb3plbiBwcmV0cmFpbmVkIHdlaWdodHMgKGluX2ZlYXR1cmVzIHggb3V0X2ZlYXR1cmVzKQogICAgICAgIEE6IExvUkEgbWF0cml4IEEgKHJhbmsgeCBvdXRfZmVhdHVyZXMpCiAgICAgICAgQjogTG9SQSBtYXRyaXggQiAoaW5fZmVhdHVyZXMgeCByYW5rKQogICAgICAgIGFscGhhOiBMb1JBIHNjYWxpbmcgZmFjdG9yCiAgICAgICAgCiAgICBSZXR1cm5zOgogICAgICAgIE91dHB1dCB0ZW5zb3IgKGJhdGNoX3NpemUgeCBvdXRfZmVhdHVyZXMpCiAgICAiIiIKICAgICMgWW91ciBjb2RlIGhlcmUKICAgIHBhc3M=",
  "title": "Train Logistic Regression with Gradient Descent",
  "createdAt": "December 15, 2025 at 1:11:17 PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/turkunov",
      "name": "turkunov"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "import torch\nx = torch.tensor([[1.0, 2.0, 3.0]])\nW = torch.tensor([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])\nB = torch.tensor([[0.1], [0.2], [0.3]])\nA = torch.tensor([[1.0, 2.0]])\nresult = lora_forward(x, W, A, B, alpha=1.0)\nprint([[round(v, 4) for v in row.tolist()] for row in result])",
      "expected_output": "[[5.4, 7.8]]"
    },
    {
      "test": "import torch\nx = torch.tensor([[1.0, 0.0], [0.0, 1.0]])\nW = torch.tensor([[2.0, 1.0], [1.0, 2.0]])\nB = torch.tensor([[1.0], [1.0]])\nA = torch.tensor([[1.0, 1.0]])\nresult = lora_forward(x, W, A, B, alpha=1.0)\nprint([[round(v, 4) for v in row.tolist()] for row in result])",
      "expected_output": "[[3.0, 2.0], [2.0, 3.0]]"
    },
    {
      "test": "import torch\nx = torch.tensor([[1.0, 1.0]])\nW = torch.tensor([[1.0], [1.0]])\nB = torch.tensor([[0.5], [0.5]])\nA = torch.tensor([[2.0]])\nresult = lora_forward(x, W, A, B, alpha=2.0)\nprint([[round(v, 4) for v in row.tolist()] for row in result])",
      "expected_output": "[[6.0]]"
    },
    {
      "test": "import torch\nx = torch.tensor([[1.0, 2.0]])\nW = torch.tensor([[1.0, 0.0], [0.0, 1.0]])\nB = torch.tensor([[1.0, 0.0], [0.0, 1.0]])\nA = torch.tensor([[0.5, 0.0], [0.0, 0.5]])\nresult = lora_forward(x, W, A, B, alpha=2.0)\nprint([[round(v, 4) for v in row.tolist()] for row in result])",
      "expected_output": "[[1.5, 3.0]]"
    },
    {
      "test": "import torch\nx = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\nW = torch.tensor([[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]])\nB = torch.tensor([[1.0], [2.0], [3.0]])\nA = torch.tensor([[0.1, 0.2]])\nresult = lora_forward(x, W, A, B, alpha=1.0)\nprint([[round(v, 4) for v in row.tolist()] for row in result])",
      "expected_output": "[[1.1, 1.2], [1.2, 1.4], [1.3, 1.6]]"
    }
  ],
  "learn_section": "IyMgTG9naXN0aWMgUmVncmVzc2lvbiB3aXRoIEdyYWRpZW50IERlc2NlbnQKCkxvZ2lzdGljIHJlZ3Jlc3Npb24gaXMgYSBmdW5kYW1lbnRhbCBjbGFzc2lmaWNhdGlvbiBhbGdvcml0aG0gdGhhdCBtb2RlbHMgdGhlIHByb2JhYmlsaXR5IG9mIGEgYmluYXJ5IG91dGNvbWUuCgojIyMgVGhlIE1vZGVsCgpGb3IgaW5wdXQgZmVhdHVyZXMgJFgkIGFuZCBjb2VmZmljaWVudHMgJFxiZXRhJCwgdGhlIHByZWRpY3RlZCBwcm9iYWJpbGl0eSBpczoKJCRwID0gXHNpZ21hKFhcYmV0YSkgPSBcZnJhY3sxfXsxICsgZV57LVhcYmV0YX19JCQKCndoZXJlICRcc2lnbWEkIGlzIHRoZSBzaWdtb2lkIGZ1bmN0aW9uLgoKIyMjIEJpbmFyeSBDcm9zcyBFbnRyb3B5IExvc3MgKFN1bS1CYXNlZCkKCldlIHVzZSB0aGUgKipzdW0tYmFzZWQqKiBCQ0UgbG9zcyAobm90IG1lYW4tYmFzZWQpOgokJFxtYXRoY2Fse0x9ID0gLVxzdW1fe2k9MX1ee259IFxsZWZ0WyB5X2kgXGxvZyhwX2kpICsgKDEteV9pKSBcbG9nKDEtcF9pKSBccmlnaHRdJCQKCk5vdGU6IE1lYW4tYmFzZWQgQkNFIHdvdWxkIGRpdmlkZSBieSAkbiQsIG1ha2luZyB0aGUgbG9zcyBzY2FsZS1pbmRlcGVuZGVudC4gU3VtLWJhc2VkIGxvc3Mgc2NhbGVzIHdpdGggZGF0YXNldCBzaXplLgoKIyMjIEdyYWRpZW50IERlc2NlbnQgVXBkYXRlCgpUaGUgZ3JhZGllbnQgb2YgdGhlIGxvc3Mgd2l0aCByZXNwZWN0IHRvIGNvZWZmaWNpZW50cyBpczoKJCRcbmFibGFfXGJldGEgXG1hdGhjYWx7TH0gPSBYXlQoXHNpZ21hKFhcYmV0YSkgLSB5KSQkCgpUaGUgdXBkYXRlIHJ1bGU6CiQkXGJldGFfe3QrMX0gPSBcYmV0YV90IC0gXGV0YSBcY2RvdCBYXlQoXHNpZ21hKFhcYmV0YV90KSAtIHkpJCQKCndoZXJlICRcZXRhJCBpcyB0aGUgbGVhcm5pbmcgcmF0ZS4KCiMjIyBJbXBsZW1lbnRhdGlvbiBTdGVwcwoKMS4gKipBZGQgYmlhcyB0ZXJtKio6IFByZXBlbmQgYSBjb2x1bW4gb2Ygb25lcyB0byBYIGFzIHRoZSBmaXJzdCBjb2x1bW4KMi4gKipJbml0aWFsaXplKio6IFNldCBhbGwgY29lZmZpY2llbnRzIHRvICoqemVybyoqCjMuICoqRm9yIGVhY2ggaXRlcmF0aW9uKio6CiAgIC0gQ29tcHV0ZSBwcmVkaWN0aW9uczogJFxoYXR7eX0gPSBcc2lnbWEoWFxiZXRhKSQKICAgLSBDb21wdXRlIGdyYWRpZW50OiAkZyA9IFheVChcaGF0e3l9IC0geSkkCiAgIC0gVXBkYXRlIGNvZWZmaWNpZW50czogJFxiZXRhID0gXGJldGEgLSBcZXRhIFxjZG90IGckCiAgIC0gUmVjb3JkIHN1bS1iYXNlZCBCQ0UgbG9zcwoKIyMjIEtleSBEZXRhaWxzCgotIENvZWZmaWNpZW50IGluaXRpYWxpemF0aW9uOiAqKnplcm9zKiogKHJlcXVpcmVkIGZvciByZXByb2R1Y2liaWxpdHkpCi0gTG9zcyB0eXBlOiAqKnN1bS1iYXNlZCoqIEJDRQotIEJpYXMgY29sdW1uOiBwcmVwZW5kZWQgYXMgKipmaXJzdCoqIGNvbHVtbgotIE51bWJlciBvZiBjb2VmZmljaWVudHMgPSBudW1iZXIgb2YgZmVhdHVyZXMgKyAxIChmb3IgYmlhcykK",
  "starter_code": "import numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Train logistic regression using gradient descent with BCE loss.\n    \n    Args:\n        X: Feature matrix of shape (n_samples, n_features)\n        y: Binary labels of shape (n_samples,)\n        learning_rate: Step size for gradient descent\n        iterations: Number of training iterations\n    \n    Returns:\n        Tuple of (coefficients, losses) where:\n        - coefficients: List of learned weights (bias first, then feature weights)\n        - losses: List of sum-based BCE loss values at each iteration\n    \n    Notes:\n        - Initialize all coefficients to zero\n        - Add bias column as FIRST column of X\n        - Use sum-based BCE loss: -sum(y*log(p) + (1-y)*log(1-p))\n    \"\"\"\n    # Your code here\n    pass",
  "description_decoded": "Implement a gradient descent training algorithm for logistic regression. Your task is to compute model parameters using Binary Cross Entropy loss and return the optimized coefficients along with loss values collected over iterations.\n\nSpecifications:\n- Add a bias term (column of ones) as the FIRST column of the feature matrix\n- Initialize all coefficients to ZERO\n- Use SUM-based BCE loss (not mean): L = -Σ[y·log(p) + (1-y)·log(1-p)]\n- Update rule: β = β - lr × X^T × (predictions - y)",
  "learn_section_decoded": "## Logistic Regression with Gradient Descent\n\nLogistic regression is a fundamental classification algorithm that models the probability of a binary outcome.\n\n### The Model\n\nFor input features $X$ and coefficients $\\beta$, the predicted probability is:\n$$p = \\sigma(X\\beta) = \\frac{1}{1 + e^{-X\\beta}}$$\n\nwhere $\\sigma$ is the sigmoid function.\n\n### Binary Cross Entropy Loss (Sum-Based)\n\nWe use the **sum-based** BCE loss (not mean-based):\n$$\\mathcal{L} = -\\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]$$\n\nNote: Mean-based BCE would divide by $n$, making the loss scale-independent. Sum-based loss scales with dataset size.\n\n### Gradient Descent Update\n\nThe gradient of the loss with respect to coefficients is:\n$$\\nabla_\\beta \\mathcal{L} = X^T(\\sigma(X\\beta) - y)$$\n\nThe update rule:\n$$\\beta_{t+1} = \\beta_t - \\eta \\cdot X^T(\\sigma(X\\beta_t) - y)$$\n\nwhere $\\eta$ is the learning rate.\n\n### Implementation Steps\n\n1. **Add bias term**: Prepend a column of ones to X as the first column\n2. **Initialize**: Set all coefficients to **zero**\n3. **For each iteration**:\n   - Compute predictions: $\\hat{y} = \\sigma(X\\beta)$\n   - Compute gradient: $g = X^T(\\hat{y} - y)$\n   - Update coefficients: $\\beta = \\beta - \\eta \\cdot g$\n   - Record sum-based BCE loss\n\n### Key Details\n\n- Coefficient initialization: **zeros** (required for reproducibility)\n- Loss type: **sum-based** BCE\n- Bias column: prepended as **first** column\n- Number of coefficients = number of features + 1 (for bias)\n"
}