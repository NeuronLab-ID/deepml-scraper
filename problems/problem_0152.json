{
  "description": "SW1wbGVtZW50IHRoZSBST1VHRS0xIChSZWNhbGwtT3JpZW50ZWQgVW5kZXJzdHVkeSBmb3IgR2lzdGluZyBFdmFsdWF0aW9uKSBzY29yZSB0byBldmFsdWF0ZSB0aGUgcXVhbGl0eSBvZiBhIGdlbmVyYXRlZCBzdW1tYXJ5IGJ5IGNvbXBhcmluZyBpdCB0byBhIHJlZmVyZW5jZSBzdW1tYXJ5LiBST1VHRS0xIGZvY3VzZXMgb24gdW5pZ3JhbSAoc2luZ2xlIHdvcmQpIG92ZXJsYXBzIGJldHdlZW4gdGhlIGNhbmRpZGF0ZSBhbmQgcmVmZXJlbmNlIHRleHRzLiBZb3VyIHRhc2sgaXMgdG8gd3JpdGUgYSBmdW5jdGlvbiB0aGF0IGNvbXB1dGVzIHRoZSBST1VHRS0xIHJlY2FsbCwgcHJlY2lzaW9uLCBhbmQgRjEgc2NvcmUgYmFzZWQgb24gdGhlIG51bWJlciBvZiBvdmVybGFwcGluZyB1bmlncmFtcy4=",
  "id": "152",
  "test_cases": [
    {
      "test": "print(rouge_1_score('the cat sat on the mat', 'the cat is on the mat'))",
      "expected_output": "{'precision': 0.8333333333333334, 'recall': 0.8333333333333334, 'f1': 0.8333333333333334}"
    },
    {
      "test": "print(rouge_1_score('hello there', 'hello there'))",
      "expected_output": "{'precision': 1.0, 'recall': 1.0, 'f1': 1.0}"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "rouge_1_score('the cat sat on the mat', 'the cat is on the mat')",
    "output": "{'precision': 0.8333333333333334, 'recall': 0.8333333333333334, 'f1': 0.8333333333333334}",
    "reasoning": "The reference text 'the cat sat on the mat' has 6 tokens, and the candidate text 'the cat is on the mat' has 6 tokens. The overlapping words are: 'the' (appears 2 times in reference, 2 times in candidate, so min(2,2)=2 overlap), 'cat' (1,1 → 1 overlap), 'on' (1,1 → 1 overlap), and 'mat' (1,1 → 1 overlap). Total overlap = 2+1+1+1 = 5. Precision = 5/6 ≈ 0.833 (5 overlapping words out of 6 candidate words). Recall = 5/6 ≈ 0.833 (5 overlapping words out of 6 reference words). F1 = 2×(0.833×0.833)/(0.833+0.833) = 0.833 since precision equals recall."
  },
  "category": "Machine Learning",
  "starter_code": "# Implement your function below.\n\ndef rouge_1_score(reference: str, candidate: str) -> dict:\n    \"\"\"\n    Compute ROUGE-1 score between reference and candidate texts.\n    \n    Returns a dictionary with precision, recall, and f1.\n    \"\"\"\n    # Your code here\n    pass",
  "title": "Implementing ROUGE Score",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/kartik-git",
      "name": "kartik-git"
    }
  ],
  "description_decoded": "Implement the ROUGE-1 (Recall-Oriented Understudy for Gisting Evaluation) score to evaluate the quality of a generated summary by comparing it to a reference summary. ROUGE-1 focuses on unigram (single word) overlaps between the candidate and reference texts. Your task is to write a function that computes the ROUGE-1 recall, precision, and F1 score based on the number of overlapping unigrams.",
  "learn_section_decoded": "# ROUGE-1 Score Learning Guide\n\n## Solution Explanation\n\nROUGE-1 (Recall-Oriented Understudy for Gisting Evaluation) is a fundamental metric for evaluating the quality of automatically generated summaries by comparing them to reference summaries. The \"1\" in ROUGE-1 refers to unigrams (single words), making it the most basic but widely used variant of ROUGE metrics.\n\n### Intuition\n\nImagine you're a teacher grading a student's book summary. You have a reference summary (the \"gold standard\") and want to measure how well the student's summary captures the key information. ROUGE-1 essentially counts how many important words from the reference summary appear in the student's summary.\n\nThe core idea is simple: **if a generated summary contains many of the same words as a high-quality reference summary, it's likely capturing similar content and therefore of good quality.**\n\n### Mathematical Foundation\n\nROUGE-1 is built on three fundamental components:\n\n**1. Precision (P)**\n$$P = \\frac{\\text{Number of overlapping unigrams}}{\\text{Total unigrams in generated summary}}$$\n\n**2. Recall (R)**\n$$R = \\frac{\\text{Number of overlapping unigrams}}{\\text{Total unigrams in reference summary}}$$\n\n**3. F1-Score (F)**\n$$F = \\frac{2 \\times P \\times R}{P + R}$$\n\nWhere an \"overlapping unigram\" is a word that appears in both the generated summary and the reference summary.\n\n### Step-by-Step Calculation Process\n\nLet's work through a concrete example:\n\n**Reference Summary:** \"The quick brown fox jumps over the lazy dog\"\n**Generated Summary:** \"A quick fox jumps over a lazy cat\"\n\n**Step 1: Tokenization**\n- Reference tokens: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n- Generated tokens: [\"A\", \"quick\", \"fox\", \"jumps\", \"over\", \"a\", \"lazy\", \"cat\"]\n\n**Step 2: Identify Overlapping Unigrams**\nOverlapping words (case-insensitive): [\"quick\", \"fox\", \"jumps\", \"over\", \"lazy\"]\n- Count of overlapping unigrams: 5\n\n**Step 3: Calculate Precision**\n$$P = \\frac{5}{8} = 0.625$$\n*Interpretation: 62.5% of words in the generated summary appear in the reference*\n\n**Step 4: Calculate Recall**\n$$R = \\frac{5}{9} = 0.556$$\n*Interpretation: 55.6% of words in the reference summary are captured in the generated summary*\n\n**Step 5: Calculate F1-Score**\n$$F = \\frac{2 \\times 0.625 \\times 0.556}{0.625 + 0.556} = \\frac{0.695}{1.181} = 0.588$$\n\n### Understanding the Components\n\n**Precision answers:** \"Of all the words in my generated summary, how many are actually relevant (appear in the reference)?\"\n- High precision means the generated summary doesn't contain many irrelevant words\n- Low precision suggests the summary is verbose or off-topic\n\n**Recall answers:** \"Of all the important words in the reference, how many did my generated summary capture?\"\n- High recall means the generated summary covers most key information\n- Low recall suggests the summary misses important content\n\n**F1-Score provides:** A balanced measure that penalizes both missing important information (low recall) and including irrelevant information (low precision)\n\n### Advanced Considerations\n\n**Preprocessing Steps:**\n1. **Case normalization:** Convert all text to lowercase\n2. **Tokenization:** Split text into individual words\n3. **Stop word handling:** Optionally remove common words like \"the\", \"and\", \"is\"\n4. **Stemming/Lemmatization:** Optionally reduce words to their root forms\n\n**Mathematical Variants:**\n- **ROUGE-1 Precision:** $P = \\frac{\\sum_{i} \\text{Count}_{\\text{match}}(unigram_i)}{\\sum_{i} \\text{Count}(unigram_i)}$\n- **ROUGE-1 Recall:** $R = \\frac{\\sum_{i} \\text{Count}_{\\text{match}}(unigram_i)}{\\sum_{i} \\text{Count}_{\\text{ref}}(unigram_i)}$\n\nWhere $\\text{Count}_{\\text{match}}(unigram_i)$ is the minimum of the counts of $unigram_i$ in the generated and reference summaries.\n\n### Practical Implementation Insights\n\n**Handling Multiple References:**\nWhen multiple reference summaries exist, ROUGE-1 can be calculated against each reference separately, then the maximum score is typically taken:\n\n$$\\text{ROUGE-1} = \\max_{j} \\text{ROUGE-1}(\\text{generated}, \\text{reference}_j)$$\n\n**Limitations to Consider:**\n- **Word order independence:** ROUGE-1 ignores sentence structure and word order\n- **Semantic blindness:** Synonyms and paraphrases aren't recognized\n- **Length bias:** Longer summaries may achieve higher recall simply by including more words\n\n### Real-World Applications\n\nROUGE-1 is extensively used in:\n- **Automatic summarization evaluation** (news articles, scientific papers)\n- **Machine translation quality assessment** (as a secondary metric)\n- **Question answering systems** (evaluating answer quality)\n- **Chatbot response evaluation** (measuring relevance to expected responses)"
}