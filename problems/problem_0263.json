{
  "description": "SW1wbGVtZW50IHRoZSBiaW5hcnkgY3Jvc3MtZW50cm9weSAoQkNFKSBsb3NzIGZ1bmN0aW9uLCB3aGljaCBpcyB0aGUgc3RhbmRhcmQgbG9zcyBmdW5jdGlvbiBmb3IgYmluYXJ5IGNsYXNzaWZpY2F0aW9uIHByb2JsZW1zLgoKR2l2ZW4gYSBsaXN0IG9mIHRydWUgYmluYXJ5IGxhYmVscyAoMCBvciAxKSBhbmQgYSBsaXN0IG9mIHByZWRpY3RlZCBwcm9iYWJpbGl0aWVzICh2YWx1ZXMgYmV0d2VlbiAwIGFuZCAxKSwgY29tcHV0ZSB0aGUgbWVhbiBiaW5hcnkgY3Jvc3MtZW50cm9weSBsb3NzIGFjcm9zcyBhbGwgc2FtcGxlcy4KClRoZSBmdW5jdGlvbiBzaG91bGQgaGFuZGxlIG51bWVyaWNhbCBzdGFiaWxpdHkgYnkgY2xpcHBpbmcgcHJlZGljdGVkIHByb2JhYmlsaXR5IHZhbHVlcyB0byBhdm9pZCB0YWtpbmcgdGhlIGxvZ2FyaXRobSBvZiBleGFjdGx5IDAgb3IgMSwgd2hpY2ggd291bGQgcmVzdWx0IGluIHVuZGVmaW5lZCBvciBpbmZpbml0ZSB2YWx1ZXMuIFVzZSB0aGUgcHJvdmlkZWQgZXBzaWxvbiBwYXJhbWV0ZXIgKGRlZmF1bHQgMWUtMTUpIGZvciBjbGlwcGluZy4KClRoZSBmdW5jdGlvbiBzaG91bGQgcmV0dXJuIHRoZSBtZWFuIGxvc3MgYXMgYSBzaW5nbGUgZmxvYXQgdmFsdWUu",
  "id": "263",
  "test_cases": [
    {
      "test": "result = binary_cross_entropy([1, 0, 1, 0], [0.9, 0.1, 0.8, 0.2])\nprint(round(result, 4))",
      "expected_output": "0.1643"
    },
    {
      "test": "result = binary_cross_entropy([1, 0], [0.1, 0.9])\nprint(round(result, 4))",
      "expected_output": "2.3026"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "y_true = [1, 0, 1, 0], y_pred = [0.9, 0.1, 0.8, 0.2]",
    "output": "0.1643",
    "reasoning": "For each sample, we compute: -[y * log(p) + (1-y) * log(1-p)]. Sample 0: -(1*log(0.9) + 0*log(0.1)) = 0.1054. Sample 1: -(0*log(0.1) + 1*log(0.9)) = 0.1054. Sample 2: -log(0.8) = 0.2231. Sample 3: -log(0.8) = 0.2231. Mean = (0.1054 + 0.1054 + 0.2231 + 0.2231) / 4 = 0.1643"
  },
  "category": "Deep Learning",
  "starter_code": "def binary_cross_entropy(y_true: list[float], y_pred: list[float], epsilon: float = 1e-15) -> float:\n\t\"\"\"\n\tCompute binary cross-entropy loss.\n\t\n\tArgs:\n\t\ty_true: True binary labels (0 or 1)\n\t\ty_pred: Predicted probabilities (between 0 and 1)\n\t\tepsilon: Small value for numerical stability\n\t\n\tReturns:\n\t\tMean binary cross-entropy loss\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Implement Binary Cross-Entropy Loss",
  "createdAt": "December 15, 2025 at 7:02:03â€¯AM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement the binary cross-entropy (BCE) loss function, which is the standard loss function for binary classification problems.\n\nGiven a list of true binary labels (0 or 1) and a list of predicted probabilities (values between 0 and 1), compute the mean binary cross-entropy loss across all samples.\n\nThe function should handle numerical stability by clipping predicted probability values to avoid taking the logarithm of exactly 0 or 1, which would result in undefined or infinite values. Use the provided epsilon parameter (default 1e-15) for clipping.\n\nThe function should return the mean loss as a single float value.",
  "learn_section_decoded": "## Binary Cross-Entropy Loss\n\n### What is Binary Cross-Entropy?\n\nBinary Cross-Entropy (BCE), also known as Log Loss, is the most commonly used loss function for binary classification problems. It measures how well predicted probabilities match the true binary labels.\n\n### Mathematical Definition\n\nFor a single sample with true label $y \\in \\{0, 1\\}$ and predicted probability $p \\in (0, 1)$, the binary cross-entropy is:\n\n$$BCE(y, p) = -[y \\cdot \\log(p) + (1-y) \\cdot \\log(1-p)]$$\n\nFor a dataset with $N$ samples, we compute the mean:\n\n$$BCE = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\cdot \\log(p_i) + (1-y_i) \\cdot \\log(1-p_i)]$$\n\n### Intuition\n\nThe loss function has two components:\n\n1. **When $y = 1$**: The loss is $-\\log(p)$\n   - If $p \\to 1$ (correct confident prediction), loss $\\to 0$\n   - If $p \\to 0$ (wrong confident prediction), loss $\\to \\infty$\n\n2. **When $y = 0$**: The loss is $-\\log(1-p)$\n   - If $p \\to 0$ (correct confident prediction), loss $\\to 0$\n   - If $p \\to 1$ (wrong confident prediction), loss $\\to \\infty$\n\n### Example Calculation\n\nSuppose we have:\n- True label: $y = 1$\n- Predicted probability: $p = 0.9$\n\nThe BCE is:\n$$BCE = -(1 \\cdot \\log(0.9) + 0 \\cdot \\log(0.1)) = -\\log(0.9) \\approx 0.105$$\n\nThis is a low loss because our model correctly predicted a high probability for the positive class.\n\n### Numerical Stability\n\nSince $\\log(0) = -\\infty$ and $\\log(1) = 0$, predictions of exactly 0 or 1 can cause numerical issues. We clip predictions to a small range like $[\\epsilon, 1-\\epsilon]$ where $\\epsilon$ is typically $10^{-15}$:\n\n$$p_{clipped} = \\text{clip}(p, \\epsilon, 1-\\epsilon)$$\n\n### Connection to Information Theory\n\nBinary cross-entropy originates from information theory. It measures the average number of bits needed to encode data from the true distribution $y$ when using a code optimized for the predicted distribution $p$. The minimum possible value (0) is achieved when predictions perfectly match the true labels.\n\n### Use Cases\n\n- Binary classification (spam detection, fraud detection, medical diagnosis)\n- Multi-label classification (each label treated independently)\n- As a building block in more complex loss functions"
}