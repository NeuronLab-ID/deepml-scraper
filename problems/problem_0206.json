{
  "description": "SW1wbGVtZW50IHRoZSBFdmlkZW5jZSBMb3dlciBCb3VuZCAoRUxCTykgZm9yIHZhcmlhdGlvbmFsIGluZmVyZW5jZS4gSW4gc2ltcGxlIHRlcm1zOiB3ZSB3YW50IHRvIGZpZ3VyZSBvdXQgaGlkZGVuIHZhcmlhYmxlcyAoeikgZnJvbSBvYnNlcnZlZCBkYXRhICh4KSwgYnV0IHRoZSBleGFjdCBjYWxjdWxhdGlvbiBpcyB0b28gaGFyZC4gSW5zdGVhZCwgd2UgYXBwcm94aW1hdGUgd2l0aCBhIHNpbXBsZXIgZGlzdHJpYnV0aW9uIChxKSBhbmQgbWVhc3VyZSBob3cgZ29vZCBvdXIgYXBwcm94aW1hdGlvbiBpcyB1c2luZyBFTEJPLiBIaWdoZXIgRUxCTyBtZWFucyBiZXR0ZXIgYXBwcm94aW1hdGlvbi4KCllvdSdsbCB1c2UgTW9udGUgQ2FybG8gc2FtcGxpbmcgKHRha2luZyBtYW55IHJhbmRvbSBzYW1wbGVzKSB0byBlc3RpbWF0ZSB0aGUgRUxCTyB2YWx1ZS4=",
  "id": "206",
  "test_cases": [
    {
      "test": "np.random.seed(42)\nelbo = compute_elbo([1.0], 0.5, 0.7, 0.0, 1.0, 1.0, n_samples=10000)\nprint(round(elbo, 2))",
      "expected_output": "-1.52"
    },
    {
      "test": "np.random.seed(42)\nelbo = compute_elbo([1.0], 3.0, 0.5, 0.0, 1.0, 1.0, n_samples=10000)\nprint(round(elbo, 2))",
      "expected_output": "-7.86"
    }
  ],
  "difficulty": "hard",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "x=[1.0], q_mean=0.5, q_std=0.7, prior_mean=0.0, prior_std=1.0, likelihood_std=1.0, n_samples=10000",
    "output": "-1.52",
    "reasoning": "The ELBO has three components:\n- Expected log-likelihood E_q[log p(x|z)] ≈ -1.29\n- Expected log-prior E_q[log p(z)] ≈ -1.29\n- Entropy H[q] = 0.5 * log(2πe * 0.7²) ≈ 1.06\n\nSumming: -1.29 + (-1.29) + 1.06 ≈ -1.52"
  },
  "category": "Probabilistic ML",
  "starter_code": "import numpy as np\n\ndef compute_elbo(x: list[float], q_mean: float, q_std: float, \n                 prior_mean: float, prior_std: float,\n                 likelihood_std: float, n_samples: int = 1000) -> float:\n    \"\"\"\n    Compute the Evidence Lower Bound (ELBO) for variational inference.\n    \n    Args:\n        x: Observed data points\n        q_mean: Mean of variational distribution q(z)\n        q_std: Standard deviation of variational distribution q(z)\n        prior_mean: Mean of prior distribution p(z)\n        prior_std: Standard deviation of prior distribution p(z)\n        likelihood_std: Standard deviation of likelihood p(x|z)\n        n_samples: Number of Monte Carlo samples\n    \n    Returns:\n        ELBO value (float)\n    \"\"\"\n    # Your code here\n    pass",
  "learn_section": "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",
  "title": "Variational Inference: ELBO Computation",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "createdAt": "December 15, 2025 at 9:29:06 AM UTC-0500",
  "description_decoded": "Implement the Evidence Lower Bound (ELBO) for variational inference. In simple terms: we want to figure out hidden variables (z) from observed data (x), but the exact calculation is too hard. Instead, we approximate with a simpler distribution (q) and measure how good our approximation is using ELBO. Higher ELBO means better approximation.\n\nYou'll use Monte Carlo sampling (taking many random samples) to estimate the ELBO value.",
  "learn_section_decoded": "## Understanding Variational Inference and ELBO\n\nVariational inference solves a fundamental problem in Bayesian statistics: computing posterior distributions $p(z|x)$ is often impossible because it requires intractable integrals. Instead, we approximate the posterior with a simpler distribution $q(z)$ and find the best approximation.\n\n### The Core Problem\n\nWe want the posterior:\n$$p(z|x) = \\frac{p(x|z)p(z)}{p(x)}$$\n\nBut computing $p(x) = \\int p(x|z)p(z)dz$ is intractable for complex models.\n\n### Variational Approximation\n\nChoose a tractable family of distributions (e.g., Gaussians) and find $q^*(z)$ that's closest to $p(z|x)$:\n$$q^*(z) = \\arg\\min_{q} D_{KL}(q(z) \\| p(z|x))$$\n\nSince we can't compute the KL divergence directly (it requires the unknown posterior), we maximize the ELBO instead.\n\n### Evidence Lower Bound (ELBO)\n\nThe ELBO can be decomposed as:\n$$\\text{ELBO} = \\mathbb{E}_q[\\log p(x|z)] + \\mathbb{E}_q[\\log p(z)] - \\mathbb{E}_q[\\log q(z)]$$\n\nOr equivalently:\n$$\\text{ELBO} = \\underbrace{\\mathbb{E}_q[\\log p(x|z)]}_{\\text{reconstruction}} + \\underbrace{\\mathbb{E}_q[\\log p(z)] + H[q]}_{\\text{regularization}}$$\n\nWhere $H[q] = -\\mathbb{E}_q[\\log q(z)]$ is the entropy of $q$.\n\n### Key Relationship\n\n$$\\log p(x) = \\text{ELBO}(q) + D_{KL}(q(z) \\| p(z|x))$$\n\nSince $\\log p(x)$ is constant and $D_{KL} \\geq 0$, maximizing ELBO minimizes KL divergence to the true posterior.\n\n### Computing Each Term\n\n#### 1. Expected Log-Likelihood: $\\mathbb{E}_q[\\log p(x|z)]$\n\nFor Gaussian likelihood $p(x|z) = \\mathcal{N}(x; z, \\sigma_x^2)$ with observed data $\\{x_1, ..., x_n\\}$:\n\n$$\\mathbb{E}_q[\\log p(x|z)] = \\mathbb{E}_q\\left[\\sum_{i=1}^{n} \\log p(x_i|z)\\right]$$\n\nMonte Carlo estimate:\n$$\\mathbb{E}_q[\\log p(x|z)] \\approx \\frac{1}{S}\\sum_{s=1}^{S}\\sum_{i=1}^{n} \\log p(x_i|z^{(s)}), \\quad z^{(s)} \\sim q(z)$$\n\n#### 2. Expected Log-Prior: $\\mathbb{E}_q[\\log p(z)]$\n\nFor Gaussian prior $p(z) = \\mathcal{N}(z; \\mu_0, \\sigma_0^2)$:\n\n$$\\mathbb{E}_q[\\log p(z)] \\approx \\frac{1}{S}\\sum_{s=1}^{S} \\log p(z^{(s)}), \\quad z^{(s)} \\sim q(z)$$\n\n#### 3. Entropy: $H[q]$\n\nFor Gaussian $q(z) = \\mathcal{N}(z; \\mu_q, \\sigma_q^2)$, the entropy has a closed form:\n\n$$H[q] = -\\mathbb{E}_q[\\log q(z)] = \\frac{1}{2}\\log(2\\pi e \\sigma_q^2)$$\n\n### Gaussian Log PDF\n\nFor a Gaussian with mean $\\mu$ and standard deviation $\\sigma$:\n$$\\log p(x) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}$$\n\n### Example Calculation\n\nFor $x = 1.0$, $q(z) \\sim \\mathcal{N}(0.5, 0.7^2)$, prior $p(z) \\sim \\mathcal{N}(0, 1)$, likelihood $p(x|z) \\sim \\mathcal{N}(z, 1)$:\n\n| Component | Formula | Value |\n|-----------|---------|-------|\n| $\\mathbb{E}_q[\\log p(x|z)]$ | Monte Carlo with 10000 samples | ≈ -1.29 |\n| $\\mathbb{E}_q[\\log p(z)]$ | Monte Carlo with 10000 samples | ≈ -1.29 |\n| $H[q]$ | $\\frac{1}{2}\\log(2\\pi e \\cdot 0.49)$ | ≈ 1.06 |\n| **ELBO** | Sum of above | **≈ -1.52** |\n\nA poor choice like $q(z) \\sim \\mathcal{N}(3.0, 0.5^2)$ gives ELBO ≈ -7.86, showing the importance of good variational parameters.\n\n### Applications\n\n- **Variational Autoencoders (VAEs)**: Neural networks learn both encoder $q(z|x)$ and decoder $p(x|z)$\n- **Bayesian Neural Networks**: Approximate posterior over weights for uncertainty quantification\n- **Topic Modeling**: Latent Dirichlet Allocation discovers document topics\n\n### Key Properties\n\n- ELBO is a **lower bound** on log evidence: $\\text{ELBO} \\leq \\log p(x)$\n- Higher ELBO = better approximation to true posterior\n- ELBO values are typically **negative** (log probabilities)\n- Gap between ELBO and $\\log p(x)$ equals KL divergence to true posterior"
}