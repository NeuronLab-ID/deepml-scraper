{
  "description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBjb21wdXRlcyB0aGUgZGVyaXZhdGl2ZXMgb2YgdGhyZWUgY29tbW9uIGFjdGl2YXRpb24gZnVuY3Rpb25zIChTaWdtb2lkLCBUYW5oLCBhbmQgUmVMVSkgYXQgYSBnaXZlbiBpbnB1dCB2YWx1ZSB4LiBUaGUgZnVuY3Rpb24gc2hvdWxkIHJldHVybiBhIGRpY3Rpb25hcnkgd2l0aCBrZXlzICdzaWdtb2lkJywgJ3RhbmgnLCBhbmQgJ3JlbHUnLCBjb250YWluaW5nIHRoZSByZXNwZWN0aXZlIGRlcml2YXRpdmUgdmFsdWVzLiBVbmRlcnN0YW5kaW5nIHRoZXNlIGRlcml2YXRpdmVzIGlzIGVzc2VudGlhbCBmb3IgYmFja3Byb3BhZ2F0aW9uIGluIG5ldXJhbCBuZXR3b3Jrcy4=",
  "id": "217",
  "test_cases": [
    {
      "test": "result = activation_derivatives(0.0)\nprint({k: round(v, 4) for k, v in result.items()})",
      "expected_output": "{'sigmoid': 0.25, 'tanh': 1.0, 'relu': 0.0}"
    },
    {
      "test": "result = activation_derivatives(1.0)\nprint({k: round(v, 4) for k, v in result.items()})",
      "expected_output": "{'sigmoid': 0.1966, 'tanh': 0.42, 'relu': 1.0}"
    }
  ],
  "difficulty": "easy",
  "pytorch_difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "x = 0.0",
    "output": "{'sigmoid': 0.25, 'tanh': 1.0, 'relu': 0.0}",
    "reasoning": "At x=0: Sigmoid derivative = sigmoid(0) * (1 - sigmoid(0)) = 0.5 * 0.5 = 0.25. Tanh derivative = 1 - tanh(0)^2 = 1 - 0 = 1.0. ReLU derivative = 0 since x is not greater than 0."
  },
  "category": "Deep Learning",
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpkZWYgYWN0aXZhdGlvbl9kZXJpdmF0aXZlcyh4OiBmbG9hdCkgLT4gZGljdFtzdHIsIGZsb2F0XToKICAgICIiIgogICAgQ29tcHV0ZSB0aGUgZGVyaXZhdGl2ZXMgb2YgU2lnbW9pZCwgVGFuaCwgYW5kIFJlTFUgYXQgYSBnaXZlbiBwb2ludCB4CiAgICB1c2luZyBQeVRvcmNoIGF1dG9ncmFkLgogICAgCiAgICBBcmdzOgogICAgICAgIHg6IElucHV0IHZhbHVlCiAgICAgICAgCiAgICBSZXR1cm5zOgogICAgICAgIERpY3Rpb25hcnkgd2l0aCBrZXlzICdzaWdtb2lkJywgJ3RhbmgnLCAncmVsdScgYW5kIHRoZWlyIGRlcml2YXRpdmUgdmFsdWVzCiAgICAiIiIKICAgICMgWW91ciBjb2RlIGhlcmUgLSB1c2UgYXV0b2dyYWQhCiAgICAjIEhpbnQ6IENyZWF0ZSB0ZW5zb3JzIHdpdGggcmVxdWlyZXNfZ3JhZD1UcnVlLCBhcHBseSBhY3RpdmF0aW9uLCBjYWxsIC5iYWNrd2FyZCgpCiAgICBwYXNz",
  "title": "Derivatives of Activation Functions",
  "createdAt": "November 28, 2025 at 10:13:26â€¯AM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "import torch\nresult = activation_derivatives(0.0)\nprint({k: round(v, 4) for k, v in result.items()})",
      "expected_output": "{'sigmoid': 0.25, 'tanh': 1.0, 'relu': 0.0}"
    },
    {
      "test": "import torch\nresult = activation_derivatives(1.0)\nprint({k: round(v, 4) for k, v in result.items()})",
      "expected_output": "{'sigmoid': 0.1966, 'tanh': 0.42, 'relu': 1.0}"
    },
    {
      "test": "import torch\nresult = activation_derivatives(-1.0)\nprint({k: round(v, 4) for k, v in result.items()})",
      "expected_output": "{'sigmoid': 0.1966, 'tanh': 0.42, 'relu': 0.0}"
    },
    {
      "test": "import torch\nresult = activation_derivatives(2.0)\nprint({k: round(v, 4) for k, v in result.items()})",
      "expected_output": "{'sigmoid': 0.105, 'tanh': 0.0707, 'relu': 1.0}"
    },
    {
      "test": "import torch\nresult = activation_derivatives(-2.0)\nprint({k: round(v, 4) for k, v in result.items()})",
      "expected_output": "{'sigmoid': 0.105, 'tanh': 0.0707, 'relu': 0.0}"
    }
  ],
  "learn_section": "IyMgRGVyaXZhdGl2ZXMgb2YgQWN0aXZhdGlvbiBGdW5jdGlvbnMKCkluIG5ldXJhbCBuZXR3b3JrcywgY29tcHV0aW5nIGRlcml2YXRpdmVzIG9mIGFjdGl2YXRpb24gZnVuY3Rpb25zIGlzIGNydWNpYWwgZm9yIGJhY2twcm9wYWdhdGlvbi4gRHVyaW5nIHRyYWluaW5nLCB3ZSBuZWVkIHRoZXNlIGRlcml2YXRpdmVzIHRvIGNhbGN1bGF0ZSBncmFkaWVudHMgYW5kIHVwZGF0ZSB3ZWlnaHRzLiBMZXQncyBkZXJpdmUgZWFjaCBvbmUuCgojIyMgMS4gU2lnbW9pZCBGdW5jdGlvbgoKKipEZWZpbml0aW9uOioqCiQkXHNpZ21hKHgpID0gXGZyYWN7MX17MSArIGVeey14fX0kJAoKKipEZXJpdmF0aXZlOioqClVzaW5nIHRoZSBxdW90aWVudCBydWxlIGFuZCBjaGFpbiBydWxlOgokJFxzaWdtYScoeCkgPSBcc2lnbWEoeCkgXGNkb3QgKDEgLSBcc2lnbWEoeCkpJCQKClRoaXMgZWxlZ2FudCBmb3JtIG1lYW5zIHdlIGNhbiBjb21wdXRlIHRoZSBkZXJpdmF0aXZlIGRpcmVjdGx5IGZyb20gdGhlIGZ1bmN0aW9uJ3Mgb3V0cHV0IQoKKipQcm9wZXJ0aWVzOioqCi0gTWF4aW11bSB2YWx1ZSBvZiAwLjI1IGF0IHggPSAwCi0gQXBwcm9hY2hlcyAwIGFzIHx4fCBpbmNyZWFzZXMgKHZhbmlzaGluZyBncmFkaWVudCBwcm9ibGVtKQotIEFsd2F5cyBwb3NpdGl2ZQotIFN5bW1ldHJpYyBhcm91bmQgeCA9IDAKCiMjIyAyLiBIeXBlcmJvbGljIFRhbmdlbnQgKFRhbmgpCgoqKkRlZmluaXRpb246KioKJCRcdGFuaCh4KSA9IFxmcmFje2VeeCAtIGVeey14fX17ZV54ICsgZV57LXh9fSQkCgoqKkRlcml2YXRpdmU6KioKJCRcdGFuaCcoeCkgPSAxIC0gXHRhbmheMih4KSQkCgpBbHRlcm5hdGl2ZWx5IHdyaXR0ZW4gYXM6CiQkXHRhbmgnKHgpID0gXHRleHR7c2VjaH1eMih4KSQkCgoqKlByb3BlcnRpZXM6KioKLSBNYXhpbXVtIHZhbHVlIG9mIDEuMCBhdCB4ID0gMAotIEFwcHJvYWNoZXMgMCBhcyB8eHwgaW5jcmVhc2VzIChhbHNvIHN1ZmZlcnMgZnJvbSB2YW5pc2hpbmcgZ3JhZGllbnRzKQotIEFsd2F5cyBwb3NpdGl2ZQotIFN0ZWVwZXIgZ3JhZGllbnRzIHRoYW4gc2lnbW9pZCBuZWFyIG9yaWdpbgoKIyMjIDMuIFJlTFUgKFJlY3RpZmllZCBMaW5lYXIgVW5pdCkKCioqRGVmaW5pdGlvbjoqKgokJFx0ZXh0e1JlTFV9KHgpID0gXG1heCgwLCB4KSQkCgoqKkRlcml2YXRpdmU6KioKJCRcdGV4dHtSZUxVfScoeCkgPSBcYmVnaW57Y2FzZXN9IDEgJiBcdGV4dHtpZiB9IHggPiAwIFxcIDAgJiBcdGV4dHtpZiB9IHggXGxlcSAwIFxlbmR7Y2FzZXN9JCQKCioqUHJvcGVydGllczoqKgotIE5vbi16ZXJvIGdyYWRpZW50IGZvciBhbGwgcG9zaXRpdmUgaW5wdXRzIChoZWxwcyB3aXRoIHZhbmlzaGluZyBncmFkaWVudHMpCi0gWmVybyBncmFkaWVudCBmb3IgbmVnYXRpdmUgaW5wdXRzIChjYW4gY2F1c2UgImR5aW5nIFJlTFUiIHByb2JsZW0pCi0gVGVjaG5pY2FsbHkgdW5kZWZpbmVkIGF0IHggPSAwLCBidXQgY29udmVudGlvbmFsbHkgc2V0IHRvIDAKLSBDb21wdXRhdGlvbmFsbHkgZWZmaWNpZW50CgojIyMgV2h5IERlcml2YXRpdmVzIE1hdHRlcgoKRHVyaW5nIGJhY2twcm9wYWdhdGlvbiwgdGhlIGdyYWRpZW50IG9mIHRoZSBsb3NzIHdpdGggcmVzcGVjdCB0byBhIHdlaWdodCBkZXBlbmRzIG9uOgokJFxmcmFje1xwYXJ0aWFsIEx9e1xwYXJ0aWFsIHd9ID0gXGZyYWN7XHBhcnRpYWwgTH17XHBhcnRpYWwgYX0gXGNkb3QgZicoeikgXGNkb3QgeCQkCgp3aGVyZSAkZicoeikkIGlzIHRoZSBhY3RpdmF0aW9uIGZ1bmN0aW9uJ3MgZGVyaXZhdGl2ZS4gSWYgdGhpcyBkZXJpdmF0aXZlIGlzIHZlcnkgc21hbGwgKGFzIHdpdGggc2lnbW9pZC90YW5oIGZvciBsYXJnZSB8eHwpLCBncmFkaWVudHMgdmFuaXNoIGFuZCBsZWFybmluZyBzbG93cyBkcmFtYXRpY2FsbHkuCgojIyMgVXNpbmcgQXV0b2dyYWQKCkluIHByYWN0aWNlLCBkZWVwIGxlYXJuaW5nIGZyYW1ld29ya3MgbGlrZSBQeVRvcmNoIGFuZCB0aW55Z3JhZCBjb21wdXRlIHRoZXNlIGRlcml2YXRpdmVzIGF1dG9tYXRpY2FsbHkgdXNpbmcgYXV0b2dyYWQuIEJ5IHNldHRpbmcgYHJlcXVpcmVzX2dyYWQ9VHJ1ZWAgb24gdGVuc29ycyBhbmQgY2FsbGluZyBgLmJhY2t3YXJkKClgLCB0aGUgZnJhbWV3b3JrIGNvbXB1dGVzIGdyYWRpZW50cyBmb3IgeW91IQ==",
  "starter_code": "def activation_derivatives(x: float) -> dict[str, float]:\n\t\"\"\"\n\tCompute the derivatives of Sigmoid, Tanh, and ReLU at a given point x.\n\t\n\tArgs:\n\t\tx: Input value\n\t\t\n\tReturns:\n\t\tDictionary with keys 'sigmoid', 'tanh', 'relu' and their derivative values\n\t\"\"\"\n\t# Your code here\n\tpass",
  "description_decoded": "Write a Python function that computes the derivatives of three common activation functions (Sigmoid, Tanh, and ReLU) at a given input value x. The function should return a dictionary with keys 'sigmoid', 'tanh', and 'relu', containing the respective derivative values. Understanding these derivatives is essential for backpropagation in neural networks.",
  "learn_section_decoded": "## Derivatives of Activation Functions\n\nIn neural networks, computing derivatives of activation functions is crucial for backpropagation. During training, we need these derivatives to calculate gradients and update weights. Let's derive each one.\n\n### 1. Sigmoid Function\n\n**Definition:**\n$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n\n**Derivative:**\nUsing the quotient rule and chain rule:\n$$\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$$\n\nThis elegant form means we can compute the derivative directly from the function's output!\n\n**Properties:**\n- Maximum value of 0.25 at x = 0\n- Approaches 0 as |x| increases (vanishing gradient problem)\n- Always positive\n- Symmetric around x = 0\n\n### 2. Hyperbolic Tangent (Tanh)\n\n**Definition:**\n$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n\n**Derivative:**\n$$\\tanh'(x) = 1 - \\tanh^2(x)$$\n\nAlternatively written as:\n$$\\tanh'(x) = \\text{sech}^2(x)$$\n\n**Properties:**\n- Maximum value of 1.0 at x = 0\n- Approaches 0 as |x| increases (also suffers from vanishing gradients)\n- Always positive\n- Steeper gradients than sigmoid near origin\n\n### 3. ReLU (Rectified Linear Unit)\n\n**Definition:**\n$$\\text{ReLU}(x) = \\max(0, x)$$\n\n**Derivative:**\n$$\\text{ReLU}'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}$$\n\n**Properties:**\n- Non-zero gradient for all positive inputs (helps with vanishing gradients)\n- Zero gradient for negative inputs (can cause \"dying ReLU\" problem)\n- Technically undefined at x = 0, but conventionally set to 0\n- Computationally efficient\n\n### Why Derivatives Matter\n\nDuring backpropagation, the gradient of the loss with respect to a weight depends on:\n$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot f'(z) \\cdot x$$\n\nwhere $f'(z)$ is the activation function's derivative. If this derivative is very small (as with sigmoid/tanh for large |x|), gradients vanish and learning slows dramatically.\n\n### Using Autograd\n\nIn practice, deep learning frameworks like PyTorch and tinygrad compute these derivatives automatically using autograd. By setting `requires_grad=True` on tensors and calling `.backward()`, the framework computes gradients for you!"
}