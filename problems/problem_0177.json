{
  "description": "V3JpdGUgYSBmdW5jdGlvbiB0aGF0IGFwcGxpZXMgdGhlIE11b25DbGlwIHFrLWNsaXAgc3RlcCB0byBhdHRlbnRpb24gcHJvamVjdGlvbiB3ZWlnaHRzIFdfcSBhbmQgV19rLiBHaXZlbiBpbnB1dCBmZWF0dXJlcyB4LCBhIHRocmVzaG9sZCB0LCBhbmQgYSBtaXhpbmcgcGFyYW1ldGVyIGFscGhhLCB5b3VyIGZ1bmN0aW9uIG11c3Q6ICgxKSBjb21wdXRlIHRoZSBtYXhpbXVtIHByZS1jbGlwIFFLIHNjb3JlIChzY2FsZWQgYnkgMS9zcXJ0KGRfaGVhZCkpOyAoMikgaWYgdGhpcyBtYXggc2NvcmUgZXhjZWVkcyB0LCByZXNjYWxlIFdfcSBhbmQgV19rIGJ5IGV0YV5hbHBoYSBhbmQgZXRhXigxLWFscGhhKSwgd2hlcmUgZXRhID0gdCAvIG1heF9zY29yZTsgKDMpIHJldHVybiB0aGUgKHBvc3NpYmx5KSByZXNjYWxlZCB3ZWlnaHRzLCBhIGJvb2xlYW4gaW5kaWNhdGluZyB3aGV0aGVyIGNsaXBwaW5nIG9jY3VycmVkLCBhbmQgdGhlIHBvc3QtY2xpcCBtYXggc2NvcmUuIFJvdW5kIGFsbCByZXR1cm5lZCBmbG9hdGluZyB2YWx1ZXMgdG8gNCBkZWNpbWFscyBmb3IgcmVwcm9kdWNpYmlsaXR5IGluIHRlc3RzLg==",
  "id": "177",
  "test_cases": [
    {
      "test": "import numpy as np\nW_q = [[2.0, 0.0],[0.0, 2.0]]\nW_k = [[2.0, 0.0],[0.0, 2.0]]\nx   = [[[1.0, 0.0],[0.0, 1.0]]]\nWq, Wk, c, m = muonclip_qk_clip(W_q, W_k, x, t=1.0, alpha=0.5)\n# Round again in the test so users don't manage rounding\nprint(np.round(np.array(Wq),4).tolist(), np.round(np.array(Wk),4).tolist(), bool(c), round(m,4))",
      "expected_output": "[[1.1892, 0.0], [0.0, 1.1892]] [[1.1892, 0.0], [0.0, 1.1892]] True 1.0"
    },
    {
      "test": "import numpy as np\nW_q = [[0.4, 0.0],[0.0, 0.4]]\nW_k = [[0.4, 0.0],[0.0, 0.4]]\nx   = [[[1.0, 0.0],[0.0, 1.0]]]\nWq, Wk, c, m = muonclip_qk_clip(W_q, W_k, x, t=1.0, alpha=0.5)\nprint(np.round(np.array(Wq),4).tolist(), np.round(np.array(Wk),4).tolist(), bool(c), round(m,4))",
      "expected_output": "[[0.4, 0.0], [0.0, 0.4]] [[0.4, 0.0], [0.0, 0.4]] False 0.1131"
    }
  ],
  "difficulty": "medium",
  "pytorch_difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "W_q = [[2.0, 0.0],[0.0, 2.0]]\nW_k = [[2.0, 0.0],[0.0, 2.0]]\nx   = [[[1.0, 0.0],[0.0, 1.0]]]  # (batch=1, seq=2, d_model=2)\nprint(muonclip_qk_clip(W_q, W_k, x, t=1.0, alpha=0.5))",
    "output": "([[1.1892, 0.0], [0.0, 1.1892]], [[1.1892, 0.0], [0.0, 1.1892]], True, 1.0)",
    "reasoning": "Pre-clip max score is 4/√2 ≈ 2.8284 (> 1.0). With η = 1/2.8284 and α = 0.5, both W_q and W_k are scaled by √η, bringing the max score to t."
  },
  "category": "Deep Learning",
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpAdG9yY2gubm9fZ3JhZCgpCmRlZiBtdW9uY2xpcF9xa19jbGlwKFdfcTogdG9yY2guVGVuc29yLCBXX2s6IHRvcmNoLlRlbnNvciwgeDogdG9yY2guVGVuc29yLCB0OiBmbG9hdCwgYWxwaGE6IGZsb2F0ID0gMC41LCBlcHM6IGZsb2F0ID0gMWUtNyk6CgkiIiJQeVRvcmNoIHZlcnNpb24gb2YgTXVvbkNsaXAgcWstY2xpcCAobm8gZ3JhZCkuIFJldHVybnMgKFdfcV9uZXcsIFdfa19uZXcsIGNsaXBwZWQsIG1heF9wb3N0KS4iIiIKCSMgY29tcHV0ZSBxLCBrLCBzY29yZXM7IHJlc2NhbGUgaWYgbmVlZGVkOyByb3VuZCBvdXRwdXRzCglwYXNzCg==",
  "title": "Implement MuonClip (qk-clip) for Stabilizing Attention",
  "createdAt": "August 26, 2025 at 3:21:20 PM UTUTC-4",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "import torch\nW_q = torch.tensor([[2.0, 0.0],[0.0, 2.0]])\nW_k = torch.tensor([[2.0, 0.0],[0.0, 2.0]])\nx   = torch.tensor([[[1.0, 0.0],[0.0, 1.0]]])\nWq2, Wk2, clipped, m = muonclip_qk_clip(W_q, W_k, x, t=1.0, alpha=0.5)\n# Round again inside the test\nWq2 = torch.round(Wq2, decimals=4)\nWk2 = torch.round(Wk2, decimals=4)\nprint(Wq2.tolist(), Wk2.tolist(), bool(clipped), round(m,4))",
      "expected_output": "[[1.1892000436782837, 0.0], [0.0, 1.1892000436782837]] [[1.1892000436782837, 0.0], [0.0, 1.1892000436782837]] True 1.0"
    },
    {
      "test": "import torch\nW_q = torch.tensor([[0.4, 0.0],[0.0, 0.4]])\nW_k = torch.tensor([[0.4, 0.0],[0.0, 0.4]])\nx   = torch.tensor([[[1.0, 0.0],[0.0, 1.0]]])\nWq2, Wk2, clipped, m = muonclip_qk_clip(W_q, W_k, x, t=1.0, alpha=0.5)\nWq2 = torch.round(Wq2, decimals=4)\nWk2 = torch.round(Wk2, decimals=4)\nprint(Wq2.tolist(), Wk2.tolist(), bool(clipped), round(m,4))",
      "expected_output": "[[0.4000000059604645, 0.0], [0.0, 0.4000000059604645]] [[0.4000000059604645, 0.0], [0.0, 0.4000000059604645]] False 0.1131"
    },
    {
      "test": "import torch\ntorch.manual_seed(123)\nW_q = torch.tensor([[2.0, 0.0],[0.0, 2.0]])\nW_k = torch.tensor([[2.0, 0.0],[0.0, 2.0]])\nx   = torch.randn(1, 2, 2)  # random, reproducible\nWq2, Wk2, clipped, m = muonclip_qk_clip(W_q, W_k, x, t=1.0, alpha=0.25)\nWq2 = torch.round(Wq2, decimals=4)\nWk2 = torch.round(Wk2, decimals=4)\nprint(Wq2.tolist(), Wk2.tolist(), bool(clipped), round(m,4))",
      "expected_output": "[[2.0, 0.0], [0.0, 2.0]] [[2.0, 0.0], [0.0, 2.0]] False 0.5499"
    },
    {
      "test": "import torch\n# Repeat with same seed to confirm identical rounded outputs\ntorch.manual_seed(123)\nW_q = torch.tensor([[2.0, 0.0],[0.0, 2.0]])\nW_k = torch.tensor([[2.0, 0.0],[0.0, 2.0]])\nx   = torch.randn(1, 2, 2)\nres1 = muonclip_qk_clip(W_q, W_k, x, t=1.0, alpha=0.25)\nres2 = muonclip_qk_clip(W_q, W_k, x, t=1.0, alpha=0.25)\nWq1, Wk1, c1, m1 = res1\nWq2, Wk2, c2, m2 = res2\nsame = Wq1.equal(Wq2) and Wk1.equal(Wk2) and (bool(c1)==bool(c2)) and (round(m1,4)==round(m2,4))\nprint(same)",
      "expected_output": "True"
    }
  ],
  "learn_section": "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",
  "starter_code": "import numpy as np\n\ndef muonclip_qk_clip(W_q: np.ndarray, W_k: np.ndarray, x: np.ndarray, t: float, alpha: float = 0.5, eps: float = 1e-7):\n\t\"\"\"\n\tApply MuonClip qk-clip to (W_q, W_k).\n\n\tArgs:\n\t\tW_q: (d_head, d_model) query projection weights\n\t\tW_k: (d_head, d_model) key  projection weights\n\t\tx: (batch, seq, d_model) input features\n\t\tt: threshold for max QK score (after 1/sqrt(d_head) scaling)\n\t\talpha: fraction of rescaling applied to W_q (remainder to W_k)\n\t\teps: small epsilon to avoid division by zero\n\n\tReturns:\n\t\tW_q_new (list[list[float]]), W_k_new (list[list[float]]), clipped (bool), max_post (float rounded to 4 dp)\n\t\"\"\"\n\t# compute q, k, scores; rescale if needed; round outputs\n\tpass\n",
  "description_decoded": "Write a function that applies the MuonClip qk-clip step to attention projection weights W_q and W_k. Given input features x, a threshold t, and a mixing parameter alpha, your function must: (1) compute the maximum pre-clip QK score (scaled by 1/sqrt(d_head)); (2) if this max score exceeds t, rescale W_q and W_k by eta^alpha and eta^(1-alpha), where eta = t / max_score; (3) return the (possibly) rescaled weights, a boolean indicating whether clipping occurred, and the post-clip max score. Round all returned floating values to 4 decimals for reproducibility in tests.",
  "learn_section_decoded": "## Understanding MuonClip (qk-clip)\n\nMuonClip is a mathematical safeguard for self-attention. It limits the magnitude of query-key (QK) similarity scores so that the softmax in attention does not receive excessively large logits.\n\n### Setup\nLet the model dimension be $d_{model}$ and the head dimension be $d_{head}$. For a sequence $x \\in \\mathbb{R}^{B\\times L \\times d_{model}}$ and weights\n$$\nW_q, W_k \\in \\mathbb{R}^{d_{head} \\times d_{model}},\n$$\nwe form queries and keys per head by\n$$\nQ = x W_q^\\top \\in \\mathbb{R}^{B\\times L \\times d_{head}}, \\qquad K = x W_k^\\top \\in \\mathbb{R}^{B\\times L \\times d_{head}}.\n$$\n\n### Attention scores\nScaled dot-product attention uses scores\n$$\nS = \\frac{Q K^\\top}{\\sqrt{d_{head}}} \\in \\mathbb{R}^{B\\times L \\times L}, \\qquad S_{ij} = \\frac{q_i \\cdot k_j}{\\sqrt{d_{head}}}.\n$$\nLarge $|S_{ij}|$ produce large softmax logits $\\exp(S_{ij})$, which can saturate probabilities and destabilize optimization.\n\n### The clipping rule\nCompute the maximum pre-clip score over all batches and positions\n$$\nM = \\max_{b,i,j} S_{b,ij}.\n$$\nGiven a threshold $t>0$, define\n$$\n\\eta = \\min\\!\\left(1,\\; \\frac{t}{M}\\right).\n$$\nMuonClip rescales the *weights* as\n$$\nW_q \\leftarrow \\eta^{\\alpha} W_q, \\qquad W_k \\leftarrow \\eta^{\\,1-\\alpha} W_k, \\quad \\alpha\\in[0,1].\n$$\n\n### Why this works (scaling property)\nThe rescaling induces\n$$\nQ \\leftarrow \\eta^{\\alpha}Q, \\qquad K \\leftarrow \\eta^{\\,1-\\alpha}K \\;\\; \\Rightarrow \\;\\; S \\leftarrow \\eta\\, S.\n$$\nTherefore the new maximum score satisfies $\\max S_{\\text{post}} = \\eta M \\le t$. The exponent split $\\alpha$ only decides which matrix receives more of the adjustment; **any** $\\alpha\\in[0,1]$ preserves the product scaling on $S$.\n\n### Choosing $\\alpha$ and $t$\n- $\\alpha = 1/2$ shares the scaling equally between $W_q$ and $W_k$.\n- Smaller $\\alpha$ (closer to $0$) leaves $W_q$ larger while shrinking $W_k$ more, and vice versa.\n- The threshold $t$ bounds the largest logit before softmax. Since attention uses $\\exp(S)$, capping $S$ indirectly limits the sharpness of attention distributions and helps avoid extreme gradients.\n\n### Worked miniature example\nTake $d_{head}=2$, $x=\\big[(1,0),(0,1)\\big]$, and\n$$\nW_q = W_k = 2 I_2, \\qquad S_{\\max}^{\\text{pre}} = \\frac{4}{\\sqrt{2}} = 2.8284.\n$$\nFor $t=1$ we get $\\eta = 1/2.8284 = 0.3536$. With $\\alpha=\\tfrac12$,\n$$\nW_q, W_k \\leftarrow \\sqrt{\\eta}\\, W_{q,k} = 0.5946\\, W_{q,k},\n$$\nso the post-clip scores satisfy $\\max S_{\\text{post}} = \\eta\\, S_{\\max}^{\\text{pre}} = 1$ (exactly at the threshold)."
}