{
  "description": "SW1wbGVtZW50IHRoZSB1bmJpYXNlZCBLTCBkaXZlcmdlbmNlIGVzdGltYXRvciB1c2VkIGluIEdSUE8gKEdyb3VwIFJlbGF0aXZlIFBvbGljeSBPcHRpbWl6YXRpb24pLiBUaGlzIGVzdGltYXRvciBjb21wdXRlcyB0aGUgS0wgZGl2ZXJnZW5jZSBiZXR3ZWVuIHRoZSBjdXJyZW50IHBvbGljeSBhbmQgYSByZWZlcmVuY2UgcG9saWN5IGZvciBlYWNoIHNhbXBsZSwgd2hpY2ggaXMgdGhlbiB1c2VkIGFzIGEgcmVndWxhcml6YXRpb24gdGVybSB0byBwcmV2ZW50IHRoZSBwb2xpY3kgZnJvbSBkZXZpYXRpbmcgdG9vIGZhciBmcm9tIHRoZSByZWZlcmVuY2Uu",
  "id": "225",
  "test_cases": [
    {
      "test": "import numpy as np\nresult = kl_divergence_estimator(np.array([0.5, 0.5]), np.array([0.5, 0.5]))\nprint(np.round(result, 4))",
      "expected_output": "[0. 0.]"
    },
    {
      "test": "import numpy as np\nresult = kl_divergence_estimator(np.array([0.8, 0.6]), np.array([0.4, 0.9]))\nprint(np.round(result, 4))",
      "expected_output": "[0.1931 0.0945]"
    }
  ],
  "difficulty": "easy",
  "pytorch_difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "pi_theta = np.array([0.8]), pi_ref = np.array([0.4])",
    "output": "np.array([0.1931])",
    "reasoning": "ratio = 0.4/0.8 = 0.5. KL = 0.5 - log(0.5) - 1 = 0.5 - (-0.693) - 1 = 0.193. This penalizes the policy for assigning higher probability than the reference."
  },
  "category": "Reinforcement Learning",
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpkZWYga2xfZGl2ZXJnZW5jZV9lc3RpbWF0b3IocGlfdGhldGE6IHRvcmNoLlRlbnNvciwgcGlfcmVmOiB0b3JjaC5UZW5zb3IpIC0+IHRvcmNoLlRlbnNvcjoKCSIiIgoJQ29tcHV0ZSB0aGUgdW5iaWFzZWQgS0wgZGl2ZXJnZW5jZSBlc3RpbWF0b3IgdXNpbmcgUHlUb3JjaC4KCQoJQXJnczoKCQlwaV90aGV0YTogQ3VycmVudCBwb2xpY3kgcHJvYmFiaWxpdGllcwoJCXBpX3JlZjogUmVmZXJlbmNlIHBvbGljeSBwcm9iYWJpbGl0aWVzCgkJCglSZXR1cm5zOgoJCVBlci1zYW1wbGUgS0wgZGl2ZXJnZW5jZSBlc3RpbWF0ZXMKCSIiIgoJIyBZb3VyIGNvZGUgaGVyZQoJcGFzcw==",
  "title": "KL Divergence Estimator for GRPO",
  "createdAt": "December 9, 2025 at 5:17:28â€¯PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "import torch\nresult = kl_divergence_estimator(torch.tensor([0.5, 0.5]), torch.tensor([0.5, 0.5]))\nprint(result.numpy().round(4))",
      "expected_output": "[0. 0.]"
    },
    {
      "test": "import torch\nresult = kl_divergence_estimator(torch.tensor([0.8, 0.6]), torch.tensor([0.4, 0.9]))\nprint(result.numpy().round(4))",
      "expected_output": "[0.1931 0.0945]"
    },
    {
      "test": "import torch\nresult = kl_divergence_estimator(torch.tensor([0.9]), torch.tensor([0.3]))\nprint(result.numpy().round(4))",
      "expected_output": "[0.4319]"
    },
    {
      "test": "import torch\nresult = kl_divergence_estimator(torch.tensor([0.5, 0.7, 0.3, 0.9]), torch.tensor([0.5, 0.5, 0.5, 0.5]))\nprint(result.numpy().round(4))",
      "expected_output": "[0.     0.0508 0.1558 0.1433]"
    }
  ],
  "learn_section": "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",
  "starter_code": "import numpy as np\n\ndef kl_divergence_estimator(pi_theta: np.ndarray, pi_ref: np.ndarray) -> np.ndarray:\n\t\"\"\"\n\tCompute the unbiased KL divergence estimator used in GRPO.\n\t\n\tFormula: D_KL = (pi_ref / pi_theta) - log(pi_ref / pi_theta) - 1\n\t\n\tArgs:\n\t\tpi_theta: Current policy probabilities for each sample\n\t\tpi_ref: Reference policy probabilities for each sample\n\t\t\n\tReturns:\n\t\tArray of KL divergence estimates (one per sample)\n\t\"\"\"\n\t# Your code here\n\tpass",
  "description_decoded": "Implement the unbiased KL divergence estimator used in GRPO (Group Relative Policy Optimization). This estimator computes the KL divergence between the current policy and a reference policy for each sample, which is then used as a regularization term to prevent the policy from deviating too far from the reference.",
  "learn_section_decoded": "## KL Divergence Estimator in GRPO\n\n### Why KL Divergence?\n\nIn reinforcement learning for LLMs, we want to improve the policy while keeping it close to a reference policy. This prevents the model from:\n- Collapsing to degenerate solutions\n- Forgetting useful behaviors from pretraining\n- Exploiting reward hacking strategies\n\n### The Standard KL Divergence\n\nThe standard KL divergence between two distributions is:\n\n$$D_{KL}(P || Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}$$\n\nHowever, computing this exactly requires summing over all possible outputs, which is intractable for language models.\n\n### The GRPO Estimator\n\nGRPO uses an unbiased per-sample estimator:\n\n$$D_{KL}(\\pi_\\theta || \\pi_{ref}) = \\frac{\\pi_{ref}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{ref}(o|q)}{\\pi_\\theta(o|q)} - 1$$\n\nLet $r = \\frac{\\pi_{ref}}{\\pi_\\theta}$, then:\n\n$$D_{KL} = r - \\log(r) - 1$$\n\n### Properties of This Estimator\n\n1. **Always non-negative**: Since $r - \\log(r) - 1 \\geq 0$ for all $r > 0$ (with equality only at $r = 1$)\n\n2. **Zero when policies match**: When $\\pi_\\theta = \\pi_{ref}$, we have $r = 1$, so $D_{KL} = 1 - 0 - 1 = 0$\n\n3. **Unbiased**: $\\mathbb{E}_{o \\sim \\pi_\\theta}[r - \\log(r) - 1]$ equals the true KL divergence\n\n4. **Per-sample**: Can be computed for each sampled output without summing over all possibilities\n\n### Role in GRPO Objective\n\nThe full GRPO objective includes this KL term:\n\n$$J_{GRPO} = \\mathbb{E}\\left[\\min(\\rho A, \\text{clip}(\\rho, 1-\\epsilon, 1+\\epsilon) A) - \\beta D_{KL}\\right]$$\n\nThe $\\beta$ hyperparameter controls how strongly to penalize deviation from the reference policy."
}