{
  "description": "SW4gbWFjaGluZSBsZWFybmluZyBhbmQgc3RhdGlzdGljcywgdGhlIHNvZnRtYXggZnVuY3Rpb24gaXMgYSBnZW5lcmFsaXphdGlvbiBvZiB0aGUgbG9naXN0aWMgZnVuY3Rpb24gdGhhdCBjb252ZXJ0cyBhIHZlY3RvciBvZiBzY29yZXMgaW50byBwcm9iYWJpbGl0aWVzLiBUaGUgbG9nLXNvZnRtYXggZnVuY3Rpb24gaXMgdGhlIGxvZ2FyaXRobSBvZiB0aGUgc29mdG1heCBmdW5jdGlvbiwgYW5kIGl0IGlzIG9mdGVuIHVzZWQgZm9yIG51bWVyaWNhbCBzdGFiaWxpdHkgd2hlbiBjb21wdXRpbmcgdGhlIHNvZnRtYXggb2YgbGFyZ2UgbnVtYmVycy4KCkdpdmVuIGEgMUQgbnVtcHkgYXJyYXkgb2Ygc2NvcmVzLCBpbXBsZW1lbnQgYSBQeXRob24gZnVuY3Rpb24gdG8gY29tcHV0ZSB0aGUgbG9nLXNvZnRtYXggb2YgdGhlIGFycmF5Lg==",
  "mdx_file": "d8c6fd66-323e-4376-a240-5c161919b722.mdx",
  "test_cases": [
    {
      "test": "print(log_softmax([1, 2, 3]))",
      "expected_output": "[-2.4076, -1.4076, -0.4076]"
    },
    {
      "test": "print(log_softmax([1, 1, 1]))",
      "expected_output": "[-1.0986, -1.0986, -1.0986]"
    }
  ],
  "difficulty": "easy",
  "pytorch_difficulty": "easy",
  "video": null,
  "likes": "0",
  "example": {
    "input": "A = np.array([1, 2, 3])\nprint(log_softmax(A))",
    "output": "array([-2.4076, -1.4076, -0.4076])",
    "reasoning": "The log-softmax function is applied to the input array [1, 2, 3]. The output array contains the log-softmax values for each element."
  },
  "dislikes": "0",
  "category": "Deep Learning",
  "starter_code": "import numpy as np\n\ndef log_softmax(scores: list) -> np.ndarray:\n\t# Your code here\n\tpass",
  "title": "Implementation of Log Softmax Function",
  "learn_section": "CiMjIFVuZGVyc3RhbmRpbmcgTG9nIFNvZnRtYXggRnVuY3Rpb24KClRoZSBsb2cgc29mdG1heCBmdW5jdGlvbiBpcyBhIG51bWVyaWNhbGx5IHN0YWJsZSB3YXkgb2YgY2FsY3VsYXRpbmcgdGhlIGxvZ2FyaXRobSBvZiB0aGUgc29mdG1heCBmdW5jdGlvbi4gVGhlIHNvZnRtYXggZnVuY3Rpb24gY29udmVydHMgYSB2ZWN0b3Igb2YgYXJiaXRyYXJ5IHZhbHVlcyAobG9naXRzKSBpbnRvIGEgdmVjdG9yIG9mIHByb2JhYmlsaXRpZXMsIHdoZXJlIGVhY2ggdmFsdWUgbGllcyBiZXR3ZWVuIDAgYW5kIDEsIGFuZCB0aGUgdmFsdWVzIHN1bSB0byAxLgoKIyMjIFNvZnRtYXggRnVuY3Rpb24KVGhlIHNvZnRtYXggZnVuY3Rpb24gaXMgZ2l2ZW4gYnk6CiQkClx0ZXh0e3NvZnRtYXh9KHhfaSkgPSBcZnJhY3tlXnt4X2l9fXtcc3VtX3tqPTF9Xm4gZV57eF9qfX0KJCQKCiMjIyBMb2cgU29mdG1heCBGdW5jdGlvbgpEaXJlY3RseSBhcHBseWluZyB0aGUgbG9nYXJpdGhtIHRvIHRoZSBzb2Z0bWF4IGZ1bmN0aW9uIGNhbiBsZWFkIHRvIG51bWVyaWNhbCBpbnN0YWJpbGl0eSwgZXNwZWNpYWxseSB3aGVuIGRlYWxpbmcgd2l0aCBsYXJnZSBudW1iZXJzLiBUbyBwcmV2ZW50IHRoaXMsIHdlIHVzZSB0aGUgbG9nLXNvZnRtYXggZnVuY3Rpb24sIHdoaWNoIGluY29ycG9yYXRlcyBhIHNoaWZ0IGJ5IHN1YnRyYWN0aW5nIHRoZSBtYXhpbXVtIHZhbHVlIGZyb20gdGhlIGlucHV0IHZlY3RvcjoKJCQKXHRleHR7bG9nIHNvZnRtYXh9KHhfaSkgPSB4X2kgLSBcbWF4KHgpIC0gXGxvZ1xsZWZ0KFxzdW1fe2o9MX1ebiBlXnt4X2ogLSBcbWF4KHgpfVxyaWdodCkKJCQKClRoaXMgZm9ybXVsYXRpb24gaGVscHMgdG8gYXZvaWQgb3ZlcmZsb3cgaXNzdWVzIHRoYXQgY2FuIG9jY3VyIHdoZW4gZXhwb25lbnRpYXRpbmcgbGFyZ2UgbnVtYmVycy4gVGhlIGxvZy1zb2Z0bWF4IGZ1bmN0aW9uIGlzIHBhcnRpY3VsYXJseSB1c2VmdWwgaW4gbWFjaGluZSBsZWFybmluZyBmb3IgY2FsY3VsYXRpbmcgcHJvYmFiaWxpdGllcyBpbiBhIHN0YWJsZSBtYW5uZXIsIGVzcGVjaWFsbHkgd2hlbiB1c2VkIHdpdGggY3Jvc3MtZW50cm9weSBsb3NzIGZ1bmN0aW9ucy4K",
  "contributor": [
    {
      "profile_link": "https://github.com/MKFMIKU",
      "name": "Kangfu MEI"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "from __main__ import log_softmax\nprint(log_softmax([1, 2, 3]).tolist())",
      "expected_output": "[-2.4076, -1.4076, -0.4076]"
    },
    {
      "test": "from __main__ import log_softmax\nprint(log_softmax([1, 1, 1]).tolist())",
      "expected_output": "[-1.0986, -1.0986, -1.0986]"
    }
  ],
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCmZyb20gdHlwaW5nIGltcG9ydCBMaXN0CgpkZWYgbG9nX3NvZnRtYXgoc2NvcmVzOiBMaXN0W2Zsb2F0XSkgLT4gdG9yY2guVGVuc29yOgogICAgIiIiCiAgICBDb21wdXRlIHRoZSBsb2ctc29mdG1heCBvZiBhIDFEIGxpc3Qgb2Ygc2NvcmVzIHVzaW5nIFB5VG9yY2guCiAgICBBcmdzOgogICAgICAgIHNjb3JlczogbGlzdCBvZiBmbG9hdHMKICAgIFJldHVybnM6CiAgICAgICAgdG9yY2guVGVuc29yIG9mIGxvZy1zb2Z0bWF4IHZhbHVlcwogICAgIiIiCiAgICAjIFlvdXIgY29kZSBoZXJlCiAgICBwYXNzCg==",
  "description_decoded": "In machine learning and statistics, the softmax function is a generalization of the logistic function that converts a vector of scores into probabilities. The log-softmax function is the logarithm of the softmax function, and it is often used for numerical stability when computing the softmax of large numbers.\n\nGiven a 1D numpy array of scores, implement a Python function to compute the log-softmax of the array.",
  "learn_section_decoded": "\n## Understanding Log Softmax Function\n\nThe log softmax function is a numerically stable way of calculating the logarithm of the softmax function. The softmax function converts a vector of arbitrary values (logits) into a vector of probabilities, where each value lies between 0 and 1, and the values sum to 1.\n\n### Softmax Function\nThe softmax function is given by:\n$$\n\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}\n$$\n\n### Log Softmax Function\nDirectly applying the logarithm to the softmax function can lead to numerical instability, especially when dealing with large numbers. To prevent this, we use the log-softmax function, which incorporates a shift by subtracting the maximum value from the input vector:\n$$\n\\text{log softmax}(x_i) = x_i - \\max(x) - \\log\\left(\\sum_{j=1}^n e^{x_j - \\max(x)}\\right)\n$$\n\nThis formulation helps to avoid overflow issues that can occur when exponentiating large numbers. The log-softmax function is particularly useful in machine learning for calculating probabilities in a stable manner, especially when used with cross-entropy loss functions.\n"
}