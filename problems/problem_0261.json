{
  "description": "SW1wbGVtZW50IGEgR2F1c3NpYW4gTmFpdmUgQmF5ZXMgY2xhc3NpZmllciBmcm9tIHNjcmF0Y2guIFRoZSBjbGFzc2lmaWVyIHNob3VsZCBsZWFybiBmcm9tIHRyYWluaW5nIGRhdGEgYnkgY29tcHV0aW5nIHRoZSBtZWFuLCB2YXJpYW5jZSwgYW5kIHByaW9yIHByb2JhYmlsaXR5IGZvciBlYWNoIGNsYXNzLCB0aGVuIHVzZSB0aGVzZSBwYXJhbWV0ZXJzIHRvIHByZWRpY3QgY2xhc3MgbGFiZWxzIGZvciBuZXcgdGVzdCBzYW1wbGVzLgoKWW91ciBmdW5jdGlvbiBzaG91bGQ6CjEuIENvbXB1dGUgdGhlIHByaW9yIHByb2JhYmlsaXR5IGZvciBlYWNoIGNsYXNzIGJhc2VkIG9uIHRyYWluaW5nIGRhdGEgZnJlcXVlbmNpZXMKMi4gRXN0aW1hdGUgdGhlIG1lYW4gYW5kIHZhcmlhbmNlIG9mIGVhY2ggZmVhdHVyZSBmb3IgZWFjaCBjbGFzcwozLiBVc2UgdGhlIEdhdXNzaWFuIHByb2JhYmlsaXR5IGRlbnNpdHkgZnVuY3Rpb24gdG8gY29tcHV0ZSBsaWtlbGlob29kcwo0LiBBcHBseSBCYXllcycgdGhlb3JlbSB0byBwcmVkaWN0IHRoZSBtb3N0IHByb2JhYmxlIGNsYXNzIGZvciBlYWNoIHRlc3Qgc2FtcGxlCgpUaGUgZnVuY3Rpb24gdGFrZXMgdHJhaW5pbmcgZmVhdHVyZXMgWF90cmFpbiwgdHJhaW5pbmcgbGFiZWxzIHlfdHJhaW4sIGFuZCB0ZXN0IGZlYXR1cmVzIFhfdGVzdCBhcyBpbnB1dHMsIGFuZCByZXR1cm5zIHRoZSBwcmVkaWN0ZWQgY2xhc3MgbGFiZWxzIGZvciB0aGUgdGVzdCBzYW1wbGVzLgoKTm90ZTogVXNlIGxvZyBwcm9iYWJpbGl0aWVzIHRvIGF2b2lkIG51bWVyaWNhbCB1bmRlcmZsb3cgd2hlbiBtdWx0aXBseWluZyBtYW55IHNtYWxsIHByb2JhYmlsaXRpZXMgdG9nZXRoZXIuIEFkZCBhIHNtYWxsIGVwc2lsb24gKDFlLTkpIHRvIHZhcmlhbmNlcyBmb3IgbnVtZXJpY2FsIHN0YWJpbGl0eS4=",
  "id": "261",
  "test_cases": [
    {
      "test": "print(gaussian_naive_bayes(np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [6.0, 7.0], [7.0, 8.0], [8.0, 9.0]]), np.array([0, 0, 0, 1, 1, 1]), np.array([[2.5, 3.5], [6.5, 7.5]])))",
      "expected_output": "[0 1]"
    },
    {
      "test": "print(gaussian_naive_bayes(np.array([[0.0, 0.0], [1.0, 1.0], [10.0, 10.0], [11.0, 11.0], [20.0, 20.0], [21.0, 21.0]]), np.array([0, 0, 1, 1, 2, 2]), np.array([[0.5, 0.5], [10.5, 10.5], [20.5, 20.5]])))",
      "expected_output": "[0 1 2]"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "X_train = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [6.0, 7.0], [7.0, 8.0], [8.0, 9.0]])\ny_train = np.array([0, 0, 0, 1, 1, 1])\nX_test = np.array([[2.5, 3.5], [6.5, 7.5]])",
    "output": "[0 1]",
    "reasoning": "For class 0: mean=[2,3], variance=[2/3, 2/3], prior=0.5. For class 1: mean=[7,8], variance=[2/3, 2/3], prior=0.5. Test point [2.5, 3.5] is closer to class 0's mean, so it's classified as 0. Test point [6.5, 7.5] is closer to class 1's mean, so it's classified as 1."
  },
  "category": "Machine Learning",
  "starter_code": "import numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> np.ndarray:\n\t\"\"\"\n\tImplements Gaussian Naive Bayes classifier.\n\t\n\tArgs:\n\t\tX_train: Training features (shape: N_train x D)\n\t\ty_train: Training labels (shape: N_train)\n\t\tX_test: Test features (shape: N_test x D)\n\t\n\tReturns:\n\t\tPredicted class labels for X_test (shape: N_test)\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Gaussian Naive Bayes Classifier",
  "createdAt": "December 14, 2025 at 1:18:30â€¯PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement a Gaussian Naive Bayes classifier from scratch. The classifier should learn from training data by computing the mean, variance, and prior probability for each class, then use these parameters to predict class labels for new test samples.\n\nYour function should:\n1. Compute the prior probability for each class based on training data frequencies\n2. Estimate the mean and variance of each feature for each class\n3. Use the Gaussian probability density function to compute likelihoods\n4. Apply Bayes' theorem to predict the most probable class for each test sample\n\nThe function takes training features X_train, training labels y_train, and test features X_test as inputs, and returns the predicted class labels for the test samples.\n\nNote: Use log probabilities to avoid numerical underflow when multiplying many small probabilities together. Add a small epsilon (1e-9) to variances for numerical stability.",
  "learn_section_decoded": "## Gaussian Naive Bayes Classifier\n\nGaussian Naive Bayes is a probabilistic classifier based on Bayes' theorem with the \"naive\" assumption that features are conditionally independent given the class label.\n\n### Bayes' Theorem\n\nThe classifier uses Bayes' theorem to compute the posterior probability:\n\n$$P(y|\\mathbf{x}) = \\frac{P(\\mathbf{x}|y) \\cdot P(y)}{P(\\mathbf{x})}$$\n\nwhere:\n- $P(y|\\mathbf{x})$ is the posterior probability of class $y$ given features $\\mathbf{x}$\n- $P(\\mathbf{x}|y)$ is the likelihood of features given the class\n- $P(y)$ is the prior probability of the class\n- $P(\\mathbf{x})$ is the evidence (constant for all classes)\n\n### Naive Independence Assumption\n\nThe \"naive\" assumption states that features are conditionally independent:\n\n$$P(\\mathbf{x}|y) = \\prod_{i=1}^{D} P(x_i|y)$$\n\nwhere $D$ is the number of features.\n\n### Gaussian Likelihood\n\nFor continuous features, we assume each feature follows a Gaussian (normal) distribution:\n\n$$P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y,i}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}\\right)$$\n\nwhere $\\mu_{y,i}$ and $\\sigma_{y,i}^2$ are the mean and variance of feature $i$ for class $y$.\n\n### Log Probabilities\n\nTo avoid numerical underflow when multiplying many small probabilities, we work with log probabilities:\n\n$$\\log P(y|\\mathbf{x}) \\propto \\log P(y) + \\sum_{i=1}^{D} \\log P(x_i|y)$$\n\nThe log of the Gaussian PDF becomes:\n\n$$\\log P(x_i|y) = -\\frac{1}{2}\\log(2\\pi\\sigma_{y,i}^2) - \\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2}$$\n\n### Training Phase\n\n1. **Compute priors**: $P(y=c) = \\frac{N_c}{N}$ where $N_c$ is the count of samples in class $c$\n2. **Compute means**: $\\mu_{c,i} = \\frac{1}{N_c}\\sum_{j:y_j=c} x_{j,i}$\n3. **Compute variances**: $\\sigma_{c,i}^2 = \\frac{1}{N_c}\\sum_{j:y_j=c} (x_{j,i} - \\mu_{c,i})^2$\n\n### Prediction Phase\n\nFor each test sample, compute the log posterior for each class and select the class with the highest value:\n\n$$\\hat{y} = \\arg\\max_c \\left[ \\log P(y=c) + \\sum_{i=1}^{D} \\log P(x_i|y=c) \\right]$$\n\n### Key Advantages\n\n- **Simple and fast**: Requires only computing means and variances\n- **Works well with small datasets**: Less prone to overfitting\n- **Handles high-dimensional data**: Independence assumption reduces complexity\n- **No hyperparameters**: Fully determined by the training data"
}