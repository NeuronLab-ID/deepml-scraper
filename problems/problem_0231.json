{
  "description": "SW1wbGVtZW50IHRlbXBlcmF0dXJlIGRlY2F5IHNjaGVkdWxlcnMgdXNlZCB0byBjb250cm9sIHJhbmRvbW5lc3MgaW4gbmV1cmFsIG5ldHdvcmsgdHJhaW5pbmcuIFRlbXBlcmF0dXJlIHNjYWxlcyB0aGUgc29mdG1heCBkaXN0cmlidXRpb246IGhpZ2ggdGVtcGVyYXR1cmUgcHJvZHVjZXMgbW9yZSB1bmlmb3JtIChleHBsb3JhdG9yeSkgb3V0cHV0cywgd2hpbGUgbG93IHRlbXBlcmF0dXJlIHByb2R1Y2VzIHBlYWtlZCAoZGV0ZXJtaW5pc3RpYykgb3V0cHV0cy4gSW1wbGVtZW50IGZvdXIgY29tbW9uIHNjaGVkdWxlczogbGluZWFyLCBleHBvbmVudGlhbCwgY29zaW5lIGFubmVhbGluZywgYW5kIGNvbnN0YW50LiBUaGVzZSBhcmUgY3J1Y2lhbCBmb3IgdGV4dCBnZW5lcmF0aW9uLCBHdW1iZWwtU29mdG1heCB0cmFpbmluZywgcmVpbmZvcmNlbWVudCBsZWFybmluZywgYW5kIGtub3dsZWRnZSBkaXN0aWxsYXRpb24u",
  "id": "231",
  "test_cases": [
    {
      "test": "result = temperature_decay('linear', 2.0, 500, 1000, final_temp=0.1); print(f\"{result:.4f}\")",
      "expected_output": "1.0500"
    },
    {
      "test": "result = temperature_decay('exponential', 1.0, 100, 1000, decay_rate=0.99); print(f\"{result:.6f}\")",
      "expected_output": "0.366032"
    }
  ],
  "difficulty": "medium",
  "pytorch_difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "schedule='linear', initial=2.0, step=500, total=1000, final=0.1",
    "output": "1.05",
    "reasoning": "Linear interpolation: progress=500/1000=0.5. Temperature = 2.0 - (2.0-0.1)*0.5 = 2.0 - 0.95 = 1.05. Halfway through training, temperature is halfway between initial and final values."
  },
  "category": "Deep Learning",
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCmltcG9ydCBtYXRoCgpkZWYgdGVtcGVyYXR1cmVfZGVjYXlfcHl0b3JjaCgKICAgIHNjaGVkdWxlX3R5cGU6IHN0ciwKICAgIGluaXRpYWxfdGVtcDogZmxvYXQsCiAgICBjdXJyZW50X3N0ZXA6IGludCwKICAgIHRvdGFsX3N0ZXBzOiBpbnQsCiAgICBmaW5hbF90ZW1wOiBmbG9hdCA9IDAuMDEsCiAgICBkZWNheV9yYXRlOiBmbG9hdCA9IDAuOTUKKSAtPiB0b3JjaC5UZW5zb3I6CgkiIiIKCUNvbXB1dGUgdGVtcGVyYXR1cmUgYXQgY3VycmVudCB0cmFpbmluZyBzdGVwIHVzaW5nIFB5VG9yY2guCgkKCVNhbWUgYXMgTnVtUHkgdmVyc2lvbiBidXQgcmV0dXJucyB0b3JjaC5UZW5zb3IuCglVc2VmdWwgZm9yIGludGVncmF0aW9uIHdpdGggUHlUb3JjaCB0cmFpbmluZyBsb29wcy4KCQoJQXJnczoKCQlzY2hlZHVsZV90eXBlOiAnbGluZWFyJywgJ2V4cG9uZW50aWFsJywgJ2Nvc2luZScsIG9yICdjb25zdGFudCcKCQlpbml0aWFsX3RlbXA6IFN0YXJ0aW5nIHRlbXBlcmF0dXJlCgkJY3VycmVudF9zdGVwOiBDdXJyZW50IHRyYWluaW5nIHN0ZXAKCQl0b3RhbF9zdGVwczogVG90YWwgbnVtYmVyIG9mIHN0ZXBzCgkJZmluYWxfdGVtcDogTWluaW11bSB0ZW1wZXJhdHVyZQoJCWRlY2F5X3JhdGU6IERlY2F5IHJhdGUgZm9yIGV4cG9uZW50aWFsCgkKCVJldHVybnM6CgkJVGVtcGVyYXR1cmUgYXMgdG9yY2guVGVuc29yIChzY2FsYXIpCgkJCglIaW50czoKCQktIFVzZSB0b3JjaC50ZW5zb3IoKSB0byBjcmVhdGUgdGVuc29ycwoJCS0gVXNlIHRvcmNoLmNvcygpIGZvciBjb3NpbmUKCQktIFVzZSB0b3JjaC5jbGFtcCgpIHRvIGVuZm9yY2UgbWluaW11bQoJCS0gVXNlIC5pdGVtKCkgdG8gZXh0cmFjdCBQeXRob24gZmxvYXQgaWYgbmVlZGVkCgkiIiIKCSMgWW91ciBjb2RlIGhlcmUKCXBhc3M=",
  "title": "Temperature Decay Scheduler",
  "createdAt": "December 11, 2025 at 1:49:40 PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "result = temperature_decay_pytorch('linear', 2.0, 500, 1000, final_temp=0.1); print(f\"{result.item():.4f}\")",
      "expected_output": "1.0500"
    },
    {
      "test": "result = temperature_decay_pytorch('exponential', 1.0, 100, 1000, decay_rate=0.99); print(f\"{result.item():.6f}\")",
      "expected_output": "0.366032"
    },
    {
      "test": "result = temperature_decay_pytorch('cosine', 2.0, 500, 1000, final_temp=0.0); print(f\"{result.item():.4f}\")",
      "expected_output": "1.0000"
    },
    {
      "test": "result = temperature_decay_pytorch('constant', 1.5, 999, 1000); print(f\"{result.item():.1f}\")",
      "expected_output": "1.5"
    },
    {
      "test": "result = temperature_decay_pytorch('exponential', 1.0, 20, 1000, final_temp=0.01, decay_rate=0.5); print(f\"{result.item():.2f}\")",
      "expected_output": "0.01"
    }
  ],
  "learn_section": "### Temperature Decay Scheduling

Temperature is a hyperparameter that controls randomness in neural network outputs. Temperature decay schedules gradually reduce temperature during training to transition from exploration to exploitation.

#### What is Temperature?

**Temperature (T)** scales logits before applying softmax:

$$
P(y_i) = \frac{e^{z_i/\tau}}{\sum_j e^{z_j/\tau}}
$$

Where:
- $z_i$ = logits (raw model outputs)
- $\tau$ = temperature
- $P(y_i)$ = probability of class $i$

**Effect of temperature**:
- **High temperature (T > 1)**: More uniform distribution, more randomness
- **Low temperature (T < 1)**: Peaked distribution, more deterministic
- **T = 1**: Standard softmax (no scaling)

#### Temperature Decay Schedules

**1. Linear Decay**

$$
\tau(t) = \tau_{\text{initial}} - (\tau_{\text{initial}} - \tau_{\text{final}}) \cdot \frac{t}{T}
$$

Where:
- $t$ = current step
- $T$ = total steps

**Properties**:
- Constant decay rate
- Predictable progression
- Simple to implement

**Example**: $\tau_{\text{initial}} = 2.0$, $\tau_{\text{final}} = 0.1$, $t = 500$, $T = 1000$

$$
\tau(500) = 2.0 - (2.0 - 0.1) \cdot 0.5 = 2.0 - 0.95 = 1.05
$$

**2. Exponential Decay**

$$
\tau(t) = \max(\tau_{\text{initial}} \cdot \gamma^t, \tau_{\text{final}})
$$

Where $\gamma$ is the decay rate (e.g., 0.99)

**Properties**:
- Fast initial decay
- Slow later decay
- Needs minimum temperature (floor)

**Example**: $\tau_{\text{initial}} = 1.0$, $\gamma = 0.99$, $t = 100$

$$
\tau(100) = 1.0 \times 0.99^{100} \approx 0.366
$$

**3. Cosine Annealing**

$$
\tau(t) = \tau_{\text{final}} + \frac{1}{2}(\tau_{\text{initial}} - \tau_{\text{final}}) \left(1 + \cos\left(\frac{\pi t}{T}\right)\right)
$$

**Properties**:
- Smooth decay
- Slow at start and end
- Fast in middle
- SOTA in many applications

**Example**: $\tau_{\text{initial}} = 2.0$, $\tau_{\text{final}} = 0.0$, $t = 500$, $T = 1000$

Progress: $\frac{500}{1000} = 0.5$

$$
\cos(\pi \cdot 0.5) = \cos\left(\frac{\pi}{2}\right) = 0
$$

$$
\tau(500) = 0.0 + \frac{1}{2}(2.0 - 0.0)(1 + 0) = 1.0
$$

**4. Constant (No Decay)**

$$
\tau(t) = \tau_{\text{initial}} \quad \forall t
$$

Used as baseline or when temperature is not critical.

#### Applications

**1. Text Generation**

Control diversity in language model outputs.

**Example logits**: [2.0, 1.0, 0.5] for three tokens

**Temperature = 0.1** (low, deterministic):
- Probabilities: [99.995%, 0.005%, 0.0%]
- Nearly always picks highest logit
- Use: Factual question answering

**Temperature = 2.0** (high, random):
- Probabilities: [48%, 29%, 23%]
- More uniform distribution
- Use: Creative writing, brainstorming

**Decay schedule**: Start with T=2.0 for exploration, decay to T=0.5 for quality.

**2. Gumbel-Softmax**

Enables gradient-based learning with discrete latent variables.

**Standard problem**: Discrete sampling is non-differentiable

**Gumbel-Softmax trick**:
$$
y_i = \frac{\exp((\log(\pi_i) + g_i)/\tau)}{\sum_j \exp((\log(\pi_j) + g_j)/\tau)}
$$

Where $g_i$ is Gumbel noise: $g = -\log(-\log(u))$, $u \sim \text{Uniform}(0,1)$

**Temperature decay**:
- **Start high (T=5.0)**: Soft, gradients flow easily
- **Decay**: Gradually become more discrete
- **End low (T=0.1)**: Nearly one-hot, behaves like discrete sample

**Use cases**:
- Variational Autoencoders (VAEs) with discrete latents
- Neural architecture search
- Discrete bottleneck layers

**3. Reinforcement Learning**

Boltzmann exploration policy:

$$
\pi(a|s) = \frac{\exp(Q(s,a)/\tau)}{\sum_{a'} \exp(Q(s,a')/\tau)}
$$

**Temperature decay**:
- **Early training (T=2.0)**: High exploration, try many actions
- **Mid training (T=1.0)**: Balanced exploration-exploitation
- **Late training (T=0.1)**: Exploitation, near-greedy policy

**Example Q-values**: [0.8, 0.5, 0.3, 0.1]

**Temperature = 2.0** (explore):
- Action probs: [0.30, 0.26, 0.23, 0.21]
- Fairly uniform, tries all actions

**Temperature = 0.1** (exploit):
- Action probs: [0.92, 0.05, 0.02, 0.01]
- Strongly prefers best action

**4. Knowledge Distillation**

Student network learns from teacher's soft targets.

**Hard labels**: [0, 0, 1, 0] - only target class

**Soft targets** (teacher with T=3):
- Logits: [5.0, 2.0, 6.0, 1.0]
- Soft probs: [0.22, 0.03, 0.73, 0.01]
- Reveals: Class 3 is best, class 1 is similar, class 2 is dissimilar

**Benefits of high temperature**:
- Preserves relative relationships between classes
- Student learns richer representation
- Improved generalization

**Typical approach**: Use T=3-10 during distillation, T=1 for final predictions

#### Choosing a Schedule

**Linear**:
- Use when: Fixed training duration, simple baseline
- Pros: Predictable, easy to understand
- Cons: May be too aggressive early or too slow late

**Exponential**:
- Use when: Want fast initial decay
- Pros: Quick early adaptation
- Cons: Can decay too fast, may hit floor early

**Cosine**:
- Use when: General purpose, SOTA approach
- Pros: Smooth, well-balanced, widely successful
- Cons: Slightly more complex

**Constant**:
- Use when: Temperature not critical, baseline comparison
- Pros: No tuning needed
- Cons: No adaptation

#### Schedule Comparison

Training for 1000 steps, T_initial=2.0, T_final=0.1

| Step | Linear | Exponential (γ=0.995) | Cosine | Constant |
|------|--------|----------------------|--------|----------|
| 0 | 2.00 | 2.00 | 2.00 | 2.00 |
| 250 | 1.53 | 0.57 | 1.72 | 2.00 |
| 500 | 1.05 | 0.16 | 1.05 | 2.00 |
| 750 | 0.58 | 0.05 | 0.38 | 2.00 |
| 1000 | 0.10 | 0.01 | 0.10 | 2.00 |

**Observations**:
- Exponential decays fastest (may be too aggressive)
- Linear is steady and predictable
- Cosine is smooth with slow start/end
- Constant provides upper bound baseline

#### Implementation Considerations

**1. Minimum Temperature**

Always enforce a floor to prevent numerical issues:

$$
\tau(t) = \max(\tau_{\text{computed}}, \tau_{\text{min}})
$$

Typical $\tau_{\text{min}} = 0.01$ to avoid division by zero.

**2. Step vs Epoch**

Can decay per step or per epoch:
- **Per step**: Smoother, more gradual
- **Per epoch**: Coarser, easier to track

**3. Warmup Period**

Sometimes keep temperature constant initially:
- First 10% of training: T = T_initial
- Remaining 90%: Apply decay schedule

**4. Cyclic Schedules**

Restart temperature periodically (less common):
- Can help escape local minima
- Used in some adversarial training

#### Practical Training Example

**Task**: Train discrete VAE with Gumbel-Softmax

**Setup**:
- Total steps: 100,000 (100 epochs × 1000 steps)
- Schedule: Cosine annealing
- T_initial: 2.0 (soft, easy gradients)
- T_final: 0.1 (discrete, realistic samples)

**Temperature at key points**:
- Step 0 (Epoch 0): T = 2.00 - Very soft
- Step 25,000 (Epoch 25): T = 1.72 - Still exploratory  
- Step 50,000 (Epoch 50): T = 1.05 - Transitioning
- Step 75,000 (Epoch 75): T = 0.38 - Getting discrete
- Step 100,000 (Epoch 100): T = 0.10 - Nearly discrete

**Benefits**:
- Early: Soft samples, stable gradients, wide exploration
- Middle: Balanced, learning meaningful patterns
- Late: Discrete samples, realistic outputs

#### Common Pitfalls

**1. Decaying too fast**: Model doesn't explore enough
- Solution: Use slower schedule (cosine) or higher decay rate

**2. Decaying too slow**: Never reaches exploitation
- Solution: Ensure final temperature is low enough (< 0.5)

**3. No minimum temperature**: Numerical instability
- Solution: Always enforce T_min (e.g., 0.01)

**4. Wrong schedule for task**: Linear may be too simple
- Solution: Try cosine as default, tune if needed

#### Summary

**Temperature decay schedules**:
- Linear: $\tau(t) = \tau_0 - (\tau_0 - \tau_f) \cdot t/T$
- Exponential: $\tau(t) = \tau_0 \cdot \gamma^t$
- Cosine: $\tau(t) = \tau_f + \frac{1}{2}(\tau_0 - \tau_f)(1 + \cos(\pi t/T))$
- Constant: $\tau(t) = \tau_0$

**Key applications**: Text generation, Gumbel-Softmax, RL exploration, knowledge distillation

**General advice**: Start with cosine annealing (T_initial=2.0, T_final=0.1), adjust based on task

**Goal**: Smooth transition from exploration (high T) to exploitation (low T)",
  "starter_code": "import numpy as np\n\ndef temperature_decay(\n    schedule_type: str,\n    initial_temp: float,\n    current_step: int,\n    total_steps: int,\n    final_temp: float = 0.01,\n    decay_rate: float = 0.95\n) -> float:\n\t\"\"\"\n\tCompute temperature at current training step using decay schedule.\n\t\n\tTemperature controls randomness in neural network outputs:\n\t- High temperature: More random, more exploration\n\t- Low temperature: More deterministic, more exploitation\n\t\n\tArgs:\n\t\tschedule_type: Decay schedule type\n\t\t  'linear': Steady linear decrease\n\t\t  'exponential': Fast early decay, slow later\n\t\t  'cosine': Smooth cosine curve\n\t\t  'constant': No decay\n\t\tinitial_temp: Starting temperature\n\t\tcurrent_step: Current training step (0 to total_steps)\n\t\ttotal_steps: Total number of training steps\n\t\tfinal_temp: Minimum temperature (floor)\n\t\tdecay_rate: Decay rate per step (for exponential)\n\t\n\tReturns:\n\t\tTemperature value at current step\n\t\"\"\"\n\t# Your code here\n\tpass",
  "description_decoded": "Implement temperature decay schedulers used to control randomness in neural network training. Temperature scales the softmax distribution: high temperature produces more uniform (exploratory) outputs, while low temperature produces peaked (deterministic) outputs. Implement four common schedules: linear, exponential, cosine annealing, and constant. These are crucial for text generation, Gumbel-Softmax training, reinforcement learning, and knowledge distillation.",
  "learn_section_decoded": "### Temperature Decay Scheduling\n\nTemperature is a hyperparameter that controls randomness in neural network outputs. Temperature decay schedules gradually reduce temperature during training to transition from exploration to exploitation.\n\n#### What is Temperature?\n\n**Temperature (T)** scales logits before applying softmax:\n\n$$\nP(y_i) = \\frac{e^{z_i/\\tau}}{\\sum_j e^{z_j/\\tau}}\n$$\n\nWhere:\n- $z_i$ = logits (raw model outputs)\n- $\\tau$ = temperature\n- $P(y_i)$ = probability of class $i$\n\n**Effect of temperature**:\n- **High temperature (T > 1)**: More uniform distribution, more randomness\n- **Low temperature (T < 1)**: Peaked distribution, more deterministic\n- **T = 1**: Standard softmax (no scaling)\n\n#### Temperature Decay Schedules\n\n**1. Linear Decay**\n\n$$\n\\tau(t) = \\tau_{\\text{initial}} - (\\tau_{\\text{initial}} - \\tau_{\\text{final}}) \\cdot \\frac{t}{T}\n$$\n\nWhere:\n- $t$ = current step\n- $T$ = total steps\n\n**Properties**:\n- Constant decay rate\n- Predictable progression\n- Simple to implement\n\n**Example**: $\\tau_{\\text{initial}} = 2.0$, $\\tau_{\\text{final}} = 0.1$, $t = 500$, $T = 1000$\n\n$$\n\\tau(500) = 2.0 - (2.0 - 0.1) \\cdot 0.5 = 2.0 - 0.95 = 1.05\n$$\n\n**2. Exponential Decay**\n\n$$\n\\tau(t) = \\max(\\tau_{\\text{initial}} \\cdot \\gamma^t, \\tau_{\\text{final}})\n$$\n\nWhere $\\gamma$ is the decay rate (e.g., 0.99)\n\n**Properties**:\n- Fast initial decay\n- Slow later decay\n- Needs minimum temperature (floor)\n\n**Example**: $\\tau_{\\text{initial}} = 1.0$, $\\gamma = 0.99$, $t = 100$\n\n$$\n\\tau(100) = 1.0 \\times 0.99^{100} \\approx 0.366\n$$\n\n**3. Cosine Annealing**\n\n$$\n\\tau(t) = \\tau_{\\text{final}} + \\frac{1}{2}(\\tau_{\\text{initial}} - \\tau_{\\text{final}}) \\left(1 + \\cos\\left(\\frac{\\pi t}{T}\\right)\\right)\n$$\n\n**Properties**:\n- Smooth decay\n- Slow at start and end\n- Fast in middle\n- SOTA in many applications\n\n**Example**: $\\tau_{\\text{initial}} = 2.0$, $\\tau_{\\text{final}} = 0.0$, $t = 500$, $T = 1000$\n\nProgress: $\\frac{500}{1000} = 0.5$\n\n$$\n\\cos(\\pi \\cdot 0.5) = \\cos\\left(\\frac{\\pi}{2}\\right) = 0\n$$\n\n$$\n\\tau(500) = 0.0 + \\frac{1}{2}(2.0 - 0.0)(1 + 0) = 1.0\n$$\n\n**4. Constant (No Decay)**\n\n$$\n\\tau(t) = \\tau_{\\text{initial}} \\quad \\forall t\n$$\n\nUsed as baseline or when temperature is not critical.\n\n#### Applications\n\n**1. Text Generation**\n\nControl diversity in language model outputs.\n\n**Example logits**: [2.0, 1.0, 0.5] for three tokens\n\n**Temperature = 0.1** (low, deterministic):\n- Probabilities: [99.995%, 0.005%, 0.0%]\n- Nearly always picks highest logit\n- Use: Factual question answering\n\n**Temperature = 2.0** (high, random):\n- Probabilities: [48%, 29%, 23%]\n- More uniform distribution\n- Use: Creative writing, brainstorming\n\n**Decay schedule**: Start with T=2.0 for exploration, decay to T=0.5 for quality.\n\n**2. Gumbel-Softmax**\n\nEnables gradient-based learning with discrete latent variables.\n\n**Standard problem**: Discrete sampling is non-differentiable\n\n**Gumbel-Softmax trick**:\n$$\ny_i = \\frac{\\exp((\\log(\\pi_i) + g_i)/\\tau)}{\\sum_j \\exp((\\log(\\pi_j) + g_j)/\\tau)}\n$$\n\nWhere $g_i$ is Gumbel noise: $g = -\\log(-\\log(u))$, $u \\sim \\text{Uniform}(0,1)$\n\n**Temperature decay**:\n- **Start high (T=5.0)**: Soft, gradients flow easily\n- **Decay**: Gradually become more discrete\n- **End low (T=0.1)**: Nearly one-hot, behaves like discrete sample\n\n**Use cases**:\n- Variational Autoencoders (VAEs) with discrete latents\n- Neural architecture search\n- Discrete bottleneck layers\n\n**3. Reinforcement Learning**\n\nBoltzmann exploration policy:\n\n$$\n\\pi(a|s) = \\frac{\\exp(Q(s,a)/\\tau)}{\\sum_{a'} \\exp(Q(s,a')/\\tau)}\n$$\n\n**Temperature decay**:\n- **Early training (T=2.0)**: High exploration, try many actions\n- **Mid training (T=1.0)**: Balanced exploration-exploitation\n- **Late training (T=0.1)**: Exploitation, near-greedy policy\n\n**Example Q-values**: [0.8, 0.5, 0.3, 0.1]\n\n**Temperature = 2.0** (explore):\n- Action probs: [0.30, 0.26, 0.23, 0.21]\n- Fairly uniform, tries all actions\n\n**Temperature = 0.1** (exploit):\n- Action probs: [0.92, 0.05, 0.02, 0.01]\n- Strongly prefers best action\n\n**4. Knowledge Distillation**\n\nStudent network learns from teacher's soft targets.\n\n**Hard labels**: [0, 0, 1, 0] - only target class\n\n**Soft targets** (teacher with T=3):\n- Logits: [5.0, 2.0, 6.0, 1.0]\n- Soft probs: [0.22, 0.03, 0.73, 0.01]\n- Reveals: Class 3 is best, class 1 is similar, class 2 is dissimilar\n\n**Benefits of high temperature**:\n- Preserves relative relationships between classes\n- Student learns richer representation\n- Improved generalization\n\n**Typical approach**: Use T=3-10 during distillation, T=1 for final predictions\n\n#### Choosing a Schedule\n\n**Linear**:\n- Use when: Fixed training duration, simple baseline\n- Pros: Predictable, easy to understand\n- Cons: May be too aggressive early or too slow late\n\n**Exponential**:\n- Use when: Want fast initial decay\n- Pros: Quick early adaptation\n- Cons: Can decay too fast, may hit floor early\n\n**Cosine**:\n- Use when: General purpose, SOTA approach\n- Pros: Smooth, well-balanced, widely successful\n- Cons: Slightly more complex\n\n**Constant**:\n- Use when: Temperature not critical, baseline comparison\n- Pros: No tuning needed\n- Cons: No adaptation\n\n#### Schedule Comparison\n\nTraining for 1000 steps, T_initial=2.0, T_final=0.1\n\n| Step | Linear | Exponential (γ=0.995) | Cosine | Constant |\n|------|--------|----------------------|--------|----------|\n| 0 | 2.00 | 2.00 | 2.00 | 2.00 |\n| 250 | 1.53 | 0.57 | 1.72 | 2.00 |\n| 500 | 1.05 | 0.16 | 1.05 | 2.00 |\n| 750 | 0.58 | 0.05 | 0.38 | 2.00 |\n| 1000 | 0.10 | 0.01 | 0.10 | 2.00 |\n\n**Observations**:\n- Exponential decays fastest (may be too aggressive)\n- Linear is steady and predictable\n- Cosine is smooth with slow start/end\n- Constant provides upper bound baseline\n\n#### Implementation Considerations\n\n**1. Minimum Temperature**\n\nAlways enforce a floor to prevent numerical issues:\n\n$$\n\\tau(t) = \\max(\\tau_{\\text{computed}}, \\tau_{\\text{min}})\n$$\n\nTypical $\\tau_{\\text{min}} = 0.01$ to avoid division by zero.\n\n**2. Step vs Epoch**\n\nCan decay per step or per epoch:\n- **Per step**: Smoother, more gradual\n- **Per epoch**: Coarser, easier to track\n\n**3. Warmup Period**\n\nSometimes keep temperature constant initially:\n- First 10% of training: T = T_initial\n- Remaining 90%: Apply decay schedule\n\n**4. Cyclic Schedules**\n\nRestart temperature periodically (less common):\n- Can help escape local minima\n- Used in some adversarial training\n\n#### Practical Training Example\n\n**Task**: Train discrete VAE with Gumbel-Softmax\n\n**Setup**:\n- Total steps: 100,000 (100 epochs × 1000 steps)\n- Schedule: Cosine annealing\n- T_initial: 2.0 (soft, easy gradients)\n- T_final: 0.1 (discrete, realistic samples)\n\n**Temperature at key points**:\n- Step 0 (Epoch 0): T = 2.00 - Very soft\n- Step 25,000 (Epoch 25): T = 1.72 - Still exploratory  \n- Step 50,000 (Epoch 50): T = 1.05 - Transitioning\n- Step 75,000 (Epoch 75): T = 0.38 - Getting discrete\n- Step 100,000 (Epoch 100): T = 0.10 - Nearly discrete\n\n**Benefits**:\n- Early: Soft samples, stable gradients, wide exploration\n- Middle: Balanced, learning meaningful patterns\n- Late: Discrete samples, realistic outputs\n\n#### Common Pitfalls\n\n**1. Decaying too fast**: Model doesn't explore enough\n- Solution: Use slower schedule (cosine) or higher decay rate\n\n**2. Decaying too slow**: Never reaches exploitation\n- Solution: Ensure final temperature is low enough (< 0.5)\n\n**3. No minimum temperature**: Numerical instability\n- Solution: Always enforce T_min (e.g., 0.01)\n\n**4. Wrong schedule for task**: Linear may be too simple\n- Solution: Try cosine as default, tune if needed\n\n#### Summary\n\n**Temperature decay schedules**:\n- Linear: $\\tau(t) = \\tau_0 - (\\tau_0 - \\tau_f) \\cdot t/T$\n- Exponential: $\\tau(t) = \\tau_0 \\cdot \\gamma^t$\n- Cosine: $\\tau(t) = \\tau_f + \\frac{1}{2}(\\tau_0 - \\tau_f)(1 + \\cos(\\pi t/T))$\n- Constant: $\\tau(t) = \\tau_0$\n\n**Key applications**: Text generation, Gumbel-Softmax, RL exploration, knowledge distillation\n\n**General advice**: Start with cosine annealing (T_initial=2.0, T_final=0.1), adjust based on task\n\n**Goal**: Smooth transition from exploration (high T) to exploitation (low T)"
}