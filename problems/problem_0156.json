{
  "description": "SW1wbGVtZW50IGEgUHl0aG9uIGZ1bmN0aW9uIHRoYXQgYXBwbGllcyB0aGUgKipTd2lHTFUgYWN0aXZhdGlvbiBmdW5jdGlvbioqIHRvIGEgTnVtUHkgYXJyYXkuIEFzc3VtZSB0aGUgaW5wdXQgYXJyYXkgaGFzIGFscmVhZHkgYmVlbiBwYXNzZWQgdGhyb3VnaCBhIGxpbmVhciBwcm9qZWN0aW9uIGFuZCBoYXMgc2hhcGUgYChiYXRjaF9zaXplLCAyZClgLiBSb3VuZCBlYWNoIG91dHB1dCB0byBmb3VyIGRlY2ltYWwgcGxhY2VzIGFuZCByZXR1cm4gdGhlIHJlc3VsdCBhcyBhIE51bVB5IGFycmF5IG9mIHRoZSBzaGFwZSBgKGJhdGNoX3NpemUsIGQpYC4=",
  "id": "156",
  "test_cases": [
    {
      "test": "print(np.round(SwiGLU(np.zeros((1, 4))), 4))",
      "expected_output": "[[0., 0.]]"
    },
    {
      "test": "print(np.round(SwiGLU(np.array([[1.0, -1.0, 2.0, -2.0]])), 4))",
      "expected_output": "[[1.7616, 0.2384]]"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "np.array([[1, -1, 1000, -1000]])",
    "output": "[[1000., 0.]]",
    "reasoning": "The input is of shape (1, 4), so it is split into x1 = [1, -1] and x2 = [1000, -1000]. The sigmoid of 1000 is approximately 1, and the sigmoid of -1000 is approximately 0 due to saturation. Thus, Swish(1000) ≈ 1000 x 1 = 1000 and Swish(-1000) ≈ -1000 x 0 = 0. Then, SwiGLU = x1 * Swish(x2) = [1 x 1000, -1 x 0] = [1000, 0]."
  },
  "category": "Deep Learning",
  "starter_code": "import numpy as np\n\ndef SwiGLU(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Args:\n        x: np.ndarray of shape (batch_size, 2d)\n    Returns:\n        np.ndarray of shape (batch_size, d)\n    \"\"\"\n    # Your code here\n    return scores",
  "title": "Implement SwiGLU activation function",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/PT-10",
      "name": "PT-10"
    }
  ],
  "description_decoded": "Implement a Python function that applies the **SwiGLU activation function** to a NumPy array. Assume the input array has already been passed through a linear projection and has shape `(batch_size, 2d)`. Round each output to four decimal places and return the result as a NumPy array of the shape `(batch_size, d)`.",
  "learn_section_decoded": "## Understanding the SwiGLU Activation Function\n\nAs the name suggests the SwiGLU activation function is a combination of two activations - Swish (implemented as SiLU in PyTorch) and GLU (Gated Linear Unit). It is important that we understand Swish and GLU because SwiGLU inherits properties from both — the smooth self-gating behavior of Swish, the decoupled gating structure of GLU.\n\n### Swish Activation (Self-Gating)\n\n**Swish**, introduced by Google Brain, is a smooth, self-gated activation function defined as:\n\n$$\n\\text{Swish}(x) = x \\cdot \\sigma(x)\n$$\n\nwhere the sigmoid function is:\n\n$$\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n$$\n\nIn Swish, the same input $x$ is used to:\n  - **Compute the gate**: $\\sigma(x)$\n  - **Modulate itself**: $x \\cdot \\sigma(x)$\n\nThis is called **self-gating** — the input both **creates** and **passes through** the gate. \\\n**Note:** When written in a PyTorch forward loop, it looks something like -\n```bash\nimport torch.nn.functional as F\n\ndef forward(self, x):\n   x1 = self.fc1(x)   # x1 = Wx + b where W, b are learnable params\n   output = F.silu(x) # output = x1 * sigmoid(x1) \n   return output      # output = (Wx + b) * sigmoid(Wx + b)\n```\nThis essentially means that the gate is learnable, and the model learns the best shape of the activation function.\n\n### Gated Linear Unit (GLU)\n\n**GLU**, introduced in *Language Modeling with Gated Convolutional Networks* (Dauphin et al., 2017), is a gated activation mechanism defined as:\n\n$$\n\\text{GLU}(x_1, x_2) = x_1 \\cdot \\sigma(x_2)\n$$\n\nHere:\n- $x_1$ is the **input signal**.\n- $x_2$ is used to **compute the gate** via the sigmoid function.\n\nIn practice, both $x_1$ and $x_2$ are obtained by **splitting the output of a single linear layer**:\n\n```bash\nimport torch.nn.functional as F\n\ndef forward(self, x):\n   x_proj = self.fc1(x)                \n   x1, x2 = x_proj.chunk(2, dim=-1)    # x1 = Wx + b, x2 = Vx + c\n   output = x1 * torch.sigmoid(x2)     # GLU = x1 · σ(x2)\n   return output\n```\nSo GLU can be rewritten as:\n\n$$\n\\text{GLU}(x) = x_1 \\cdot \\sigma(x_2)\n$$\nwhere:\n$$x_1 = W x + b$$\n $$x_2 = V x + c$$\n\n\nThis is a learned, cross-gating mechanism — the model learns different parameters for the signal and the gate.\n\n\n## SwiGLU\n\nWith Swish and GLU out of the way, it becomes very easy to understand **SwiGLU**. It is defined as:\n\n$$\n\\text{SwiGLU}(x) = x_1 \\cdot \\text{Swish}(x_2)\n$$\n\nWhere:\n- $x_1, x_2$ are typically obtained by splitting a linear projection of the input (inspired by GLU).\n\n- $\\text{Swish}(x_2) = x_2 \\cdot \\sigma(x_2)$ is the self-gated activation.\n\nSo putting it together:\n\n$$\n\\text{SwiGLU}(x) = x_1 \\cdot (x_2 \\cdot \\sigma(x_2))\n$$\n\nThis combines the **signal-gate decoupling** of GLU with the **smooth self-gating** of Swish, and is used in the feed-forward blocks of large-scale models like Google's PaLM, Meta's LLaMA.\n\n\n### Why Does It Work?\n> Noam Shazeer, the author in his paper writes: \"We offer no explanation as to why these architectures seem to work; we attribute their success, as all else, to divine benevolence.\"\nThe improvement in performance have only been proven *emprically* by observing faster convergence during training"
}