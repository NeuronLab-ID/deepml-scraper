{
  "description": "SW1wbGVtZW50IHRoZSBBZGFncmFkIG9wdGltaXplciB1cGRhdGUgc3RlcCBmdW5jdGlvbi4gWW91ciBmdW5jdGlvbiBzaG91bGQgdGFrZSB0aGUgY3VycmVudCBwYXJhbWV0ZXIgdmFsdWUsIGdyYWRpZW50LCBhbmQgYWNjdW11bGF0ZWQgc3F1YXJlZCBncmFkaWVudHMgYXMgaW5wdXRzLCBhbmQgcmV0dXJuIHRoZSB1cGRhdGVkIHBhcmFtZXRlciB2YWx1ZSBhbmQgbmV3IGFjY3VtdWxhdGVkIHNxdWFyZWQgZ3JhZGllbnRzLiBUaGUgZnVuY3Rpb24gc2hvdWxkIGFsc28gaGFuZGxlIHNjYWxhciBhbmQgYXJyYXkgaW5wdXRzLCBhbmQgaW5jbHVkZSBwcm9wZXIgaW5wdXQgdmFsaWRhdGlvbi4=",
  "id": "145",
  "test_cases": [
    {
      "test": "print(adagrad_optimizer(1., 0.5, 1., 0.01, 1e-8))",
      "expected_output": "(0.99553, 1.25)"
    },
    {
      "test": "print(adagrad_optimizer(np.array([1., 2.]), np.array([0.1, 0.2]), np.array([1., 1.]), 0.01, 1e-8))",
      "expected_output": "array([0.999, 1.99804]), array([1.01, 1.04])"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "parameter = 1.0, grad = 0.1, G = 1.0",
    "output": "(0.999, 1.01)",
    "reasoning": "The Adagrad optimizer computes updated values for the parameter and the accumulated squared gradients. With input values parameter=1.0, grad=0.1, and G=1.0, the updated parameter becomes 0.999 and the updated G becomes 1.01."
  },
  "category": "Deep Learning",
  "starter_code": "import numpy as np\n\ndef adagrad_optimizer(parameter, grad, G, learning_rate=0.01, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adagrad optimizer.\n    Adapts the learning rate for each parameter based on the historical gradients.\n    Args:\n        parameter: Current parameter value\n        grad: Current gradient\n        G: Accumulated squared gradients\n        learning_rate: Learning rate (default=0.01)\n        epsilon: Small constant for numerical stability (default=1e-8)\n    Returns:\n        tuple: (updated_parameter, updated_G)\n    \"\"\"\n    # Your code here\n    return np.round(parameter, 5), np.round(G, 5)",
  "title": "Adagrad Optimizer",
  "learn_section": "IyBJbXBsZW1lbnRpbmcgQWRhZ3JhZCBPcHRpbWl6ZXIKCiMjIEludHJvZHVjdGlvbgpBZGFncmFkIChBZGFwdGl2ZSBHcmFkaWVudCBBbGdvcml0aG0pIGlzIGFuIG9wdGltaXphdGlvbiBhbGdvcml0aG0gdGhhdCBhZGFwdHMgdGhlIGxlYXJuaW5nIHJhdGUgdG8gZWFjaCBwYXJhbWV0ZXIsIHBlcmZvcm1pbmcgbGFyZ2VyIHVwZGF0ZXMgZm9yIGluZnJlcXVlbnQgcGFyYW1ldGVycyBhbmQgc21hbGxlciB1cGRhdGVzIGZvciBmcmVxdWVudCBvbmVzLiBUaGlzIG1ha2VzIGl0IHBhcnRpY3VsYXJseSB3ZWxsLXN1aXRlZCBmb3IgZGVhbGluZyB3aXRoIHNwYXJzZSBkYXRhLgoKIyMgTGVhcm5pbmcgT2JqZWN0aXZlcwotIFVuZGVyc3RhbmQgaG93IEFkYWdyYWQgb3B0aW1pemVyIHdvcmtzCi0gTGVhcm4gdG8gaW1wbGVtZW50IGFkYXB0aXZlIGxlYXJuaW5nIHJhdGVzCi0gR2FpbiBwcmFjdGljYWwgZXhwZXJpZW5jZSB3aXRoIGdyYWRpZW50LWJhc2VkIG9wdGltaXphdGlvbgoKIyMgVGhlb3J5CkFkYWdyYWQgYWRhcHRzIHRoZSBsZWFybmluZyByYXRlIGZvciBlYWNoIHBhcmFtZXRlciBiYXNlZCBvbiB0aGUgaGlzdG9yaWNhbCBncmFkaWVudHMuIFRoZSBrZXkgZXF1YXRpb25zIGFyZToKCiRHX3QgPSBHX3t0LTF9ICsgZ190XjIkIChBY2N1bXVsYXRlZCBzcXVhcmVkIGdyYWRpZW50cykKCiRcdGhldGFfdCA9IFx0aGV0YV97dC0xfSAtIFxkZnJhY3tcYWxwaGF9e1xzcXJ0e0dfdH0gKyBcZXBzaWxvbn0gXGNkb3QgZ190JCAoUGFyYW1ldGVyIHVwZGF0ZSkKCldoZXJlOgotICRHX3QkIGlzIHRoZSBzdW0gb2Ygc3F1YXJlZCBncmFkaWVudHMgdXAgdG8gdGltZSBzdGVwIHQKLSAkXGFscGhhJCBpcyB0aGUgaW5pdGlhbCBsZWFybmluZyByYXRlCi0gJFxlcHNpbG9uJCBpcyBhIHNtYWxsIGNvbnN0YW50IGZvciBudW1lcmljYWwgc3RhYmlsaXR5Ci0gJGdfdCQgaXMgdGhlIGdyYWRpZW50IGF0IHRpbWUgc3RlcCB0CgpSZWFkIG1vcmUgYXQ6CgoxLiBEdWNoaSwgSi4sIEhhemFuLCBFLiwgJiBTaW5nZXIsIFkuICgyMDExKS4gQWRhcHRpdmUgc3ViZ3JhZGllbnQgbWV0aG9kcyBmb3Igb25saW5lIGxlYXJuaW5nIGFuZCBzdG9jaGFzdGljIG9wdGltaXphdGlvbi4gSm91cm5hbCBvZiBNYWNoaW5lIExlYXJuaW5nIFJlc2VhcmNoLCAxMiwgMjEyMeKAkzIxNTkuIFtQREZdKGh0dHBzOi8vd3d3LmptbHIub3JnL3BhcGVycy92b2x1bWUxMi9kdWNoaTExYS9kdWNoaTExYS5wZGYpCjIuIFJ1ZGVyLCBTLiAoMjAxNykuIEFuIG92ZXJ2aWV3IG9mIGdyYWRpZW50IGRlc2NlbnQgb3B0aW1pemF0aW9uIGFsZ29yaXRobXMuIFthclhpdjoxNjA5LjA0NzQ3XShodHRwczovL2FyeGl2Lm9yZy9wZGYvMTYwOS4wNDc0NykKCgojIyBQcm9ibGVtIFN0YXRlbWVudApJbXBsZW1lbnQgdGhlIEFkYWdyYWQgb3B0aW1pemVyIHVwZGF0ZSBzdGVwIGZ1bmN0aW9uLiBZb3VyIGZ1bmN0aW9uIHNob3VsZCB0YWtlIHRoZSBjdXJyZW50IHBhcmFtZXRlciB2YWx1ZSwgZ3JhZGllbnQsIGFuZCBhY2N1bXVsYXRlZCBzcXVhcmVkIGdyYWRpZW50cyBhcyBpbnB1dHMsIGFuZCByZXR1cm4gdGhlIHVwZGF0ZWQgcGFyYW1ldGVyIHZhbHVlIGFuZCBuZXcgYWNjdW11bGF0ZWQgc3F1YXJlZCBncmFkaWVudHMuCgojIyMgSW5wdXQgRm9ybWF0ClRoZSBmdW5jdGlvbiBzaG91bGQgYWNjZXB0OgotIHBhcmFtZXRlcjogQ3VycmVudCBwYXJhbWV0ZXIgdmFsdWUKLSBncmFkOiBDdXJyZW50IGdyYWRpZW50Ci0gRzogQWNjdW11bGF0ZWQgc3F1YXJlZCBncmFkaWVudHMKLSBsZWFybmluZ19yYXRlOiBMZWFybmluZyByYXRlIChkZWZhdWx0PTAuMDEpCi0gZXBzaWxvbjogU21hbGwgY29uc3RhbnQgZm9yIG51bWVyaWNhbCBzdGFiaWxpdHkgKGRlZmF1bHQ9MWUtOCkKCiMjIyBPdXRwdXQgRm9ybWF0ClJldHVybiB0dXBsZTogKHVwZGF0ZWRfcGFyYW1ldGVyLCB1cGRhdGVkX0cpCgojIyBFeGFtcGxlCmBgYHB5dGhvbgojIEV4YW1wbGUgdXNhZ2U6CnBhcmFtZXRlciA9IDEuMApncmFkID0gMC4xCkcgPSAxLjAKCm5ld19wYXJhbSwgbmV3X0cgPSBhZGFncmFkX29wdGltaXplcihwYXJhbWV0ZXIsIGdyYWQsIEcpCmBgYAoKIyMgVGlwcwotIEluaXRpYWxpemUgRyBhcyB6ZXJvcwotIFVzZSBudW1weSBmb3IgbnVtZXJpY2FsIG9wZXJhdGlvbnMKLSBUZXN0IHdpdGggYm90aCBzY2FsYXIgYW5kIGFycmF5IGlucHV0cwoKLS0t",
  "contributor": [
    {
      "profile_link": "https://github.com/mavleo96",
      "name": "Vijayabharathi Murugan"
    }
  ],
  "description_decoded": "Implement the Adagrad optimizer update step function. Your function should take the current parameter value, gradient, and accumulated squared gradients as inputs, and return the updated parameter value and new accumulated squared gradients. The function should also handle scalar and array inputs, and include proper input validation.",
  "learn_section_decoded": "# Implementing Adagrad Optimizer\n\n## Introduction\nAdagrad (Adaptive Gradient Algorithm) is an optimization algorithm that adapts the learning rate to each parameter, performing larger updates for infrequent parameters and smaller updates for frequent ones. This makes it particularly well-suited for dealing with sparse data.\n\n## Learning Objectives\n- Understand how Adagrad optimizer works\n- Learn to implement adaptive learning rates\n- Gain practical experience with gradient-based optimization\n\n## Theory\nAdagrad adapts the learning rate for each parameter based on the historical gradients. The key equations are:\n\n$G_t = G_{t-1} + g_t^2$ (Accumulated squared gradients)\n\n$\\theta_t = \\theta_{t-1} - \\dfrac{\\alpha}{\\sqrt{G_t} + \\epsilon} \\cdot g_t$ (Parameter update)\n\nWhere:\n- $G_t$ is the sum of squared gradients up to time step t\n- $\\alpha$ is the initial learning rate\n- $\\epsilon$ is a small constant for numerical stability\n- $g_t$ is the gradient at time step t\n\nRead more at:\n\n1. Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12, 2121â€“2159. [PDF](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n2. Ruder, S. (2017). An overview of gradient descent optimization algorithms. [arXiv:1609.04747](https://arxiv.org/pdf/1609.04747)\n\n\n## Problem Statement\nImplement the Adagrad optimizer update step function. Your function should take the current parameter value, gradient, and accumulated squared gradients as inputs, and return the updated parameter value and new accumulated squared gradients.\n\n### Input Format\nThe function should accept:\n- parameter: Current parameter value\n- grad: Current gradient\n- G: Accumulated squared gradients\n- learning_rate: Learning rate (default=0.01)\n- epsilon: Small constant for numerical stability (default=1e-8)\n\n### Output Format\nReturn tuple: (updated_parameter, updated_G)\n\n## Example\n```python\n# Example usage:\nparameter = 1.0\ngrad = 0.1\nG = 1.0\n\nnew_param, new_G = adagrad_optimizer(parameter, grad, G)\n```\n\n## Tips\n- Initialize G as zeros\n- Use numpy for numerical operations\n- Test with both scalar and array inputs\n\n---"
}