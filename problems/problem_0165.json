{
  "description": "V3JpdGUgYSBmdW5jdGlvbiB0byBjb21wdXRlIHRoZSBkaXNjb3VudGVkIHJldHVybiBmb3IgYSBzZXF1ZW5jZSBvZiByZXdhcmRzIGdpdmVuIGEgZGlzY291bnQgZmFjdG9yIGdhbW1hLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSBsaXN0IG9yIE51bVB5IGFycmF5IG9mIHJld2FyZHMgYW5kIGEgZGlzY291bnQgZmFjdG9yIGdhbW1hICgwIDwgZ2FtbWEgPD0gMSkgYW5kIHJldHVybiB0aGUgc2NhbGFyIHZhbHVlIG9mIHRoZSB0b3RhbCBkaXNjb3VudGVkIHJldHVybi4gT25seSB1c2UgTnVtUHku",
  "id": "165",
  "test_cases": [
    {
      "test": "import numpy as np\nrewards = [1, 1, 1, 1, 1]\ngamma = 0.9\nprint(round(discounted_return(rewards, gamma), 4))",
      "expected_output": "4.0951"
    },
    {
      "test": "import numpy as np\nrewards = [5, 0, 0, 0, 2]\ngamma = 0.5\nprint(round(discounted_return(rewards, gamma), 4))",
      "expected_output": "5.125"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "rewards = [1, 1, 1]\ngamma = 0.5\nprint(discounted_return(rewards, gamma))",
    "output": "1.75",
    "reasoning": "Discounted return: 1*1 + 1*0.5 + 1*0.25 = 1 + 0.5 + 0.25 = 1.75"
  },
  "category": "Reinforcement Learning",
  "starter_code": "import numpy as np\n\ndef discounted_return(rewards, gamma):\n    \"\"\"\n    Compute the total discounted return for a sequence of rewards.\n    Args:\n        rewards (list or np.ndarray): List or array of rewards [r_0, r_1, ..., r_T-1]\n        gamma (float): Discount factor (0 < gamma <= 1)\n    Returns:\n        float: Total discounted return\n    \"\"\"\n    # Your code here\n    pass",
  "title": "Compute Discounted Return",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "description_decoded": "Write a function to compute the discounted return for a sequence of rewards given a discount factor gamma. The function should take a list or NumPy array of rewards and a discount factor gamma (0 < gamma <= 1) and return the scalar value of the total discounted return. Only use NumPy.",
  "learn_section_decoded": "# **Discounted Return in Reinforcement Learning**\n\nThe **discounted return** is a core concept in reinforcement learning that quantifies the total expected reward an agent can achieve from a sequence of actions, while accounting for the fact that immediate rewards are typically more valuable than distant future rewards.\n\n---\n\n## **Definition**\nGiven a sequence of rewards $[r_0, r_1, \\ldots, r_{T-1}]$ and a discount factor $\\gamma$ ($0 < \\gamma \\leq 1$), the **discounted return** $G$ is:\n\n$$\nG = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\dots + \\gamma^{T-1} r_{T-1}\n$$\n\nOr, more generally:\n\n$$\nG = \\sum_{t=0}^{T-1} \\gamma^t r_t\n$$\n\n- $r_t$: reward at time step $t$\n- $\\gamma$: discount factor (how much future rewards are “worth” relative to immediate rewards)\n\n---\n\n## **Why Discount Future Rewards?**\n- **Uncertainty:** Future rewards are less certain than immediate rewards.\n- **Preference:** Most decision-makers prefer immediate rewards over delayed ones.\n- **Mathematical Convenience:** Ensures the sum converges even for infinite sequences (if $\\gamma < 1$).\n\n---\n\n## **Properties**\n- If $\\gamma = 1$, the agent values all rewards equally (undiscounted return).\n- If $\\gamma = 0.9$, the agent heavily favors immediate rewards over distant ones.\n- The smaller $\\gamma$ is, the less impact future rewards have on the total return.\n\n---\n\n## **Practical Example**\nSuppose $rewards = [2, 0, 0, 3]$ and $\\gamma = 0.5$:\n\n$$\nG = 2 * 1 + 0 * 0.5 + 0 * 0.25 + 3 * 0.125 = 2 + 0 + 0 + 0.375 = 2.375\n$$\n\n---\n\n## **Summary**\nThe discounted return captures the “present value” of a sequence of rewards, providing a crucial foundation for learning and planning in reinforcement learning."
}