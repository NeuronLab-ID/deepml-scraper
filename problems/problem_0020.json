{
  "description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBpbXBsZW1lbnRzIHRoZSBkZWNpc2lvbiB0cmVlIGxlYXJuaW5nIGFsZ29yaXRobSBmb3IgY2xhc3NpZmljYXRpb24uIFRoZSBmdW5jdGlvbiBzaG91bGQgdXNlIHJlY3Vyc2l2ZSBiaW5hcnkgc3BsaXR0aW5nIGJhc2VkIG9uIGVudHJvcHkgYW5kIGluZm9ybWF0aW9uIGdhaW4gdG8gYnVpbGQgYSBkZWNpc2lvbiB0cmVlLiBJdCBzaG91bGQgdGFrZSBhIGxpc3Qgb2YgZXhhbXBsZXMgKGVhY2ggZXhhbXBsZSBpcyBhIGRpY3Qgb2YgYXR0cmlidXRlLXZhbHVlIHBhaXJzKSBhbmQgYSBsaXN0IG9mIGF0dHJpYnV0ZSBuYW1lcyBhcyBpbnB1dCwgYW5kIHJldHVybiBhIG5lc3RlZCBkaWN0aW9uYXJ5IHJlcHJlc2VudGluZyB0aGUgZGVjaXNpb24gdHJlZS4=",
  "mdx_file": "1ea17459-d896-4362-9a3b-9b87bca0abc3.mdx",
  "tinygrad_difficulty": "hard",
  "tinygrad_starter_code": "aW1wb3J0IG1hdGgKZnJvbSBjb2xsZWN0aW9ucyBpbXBvcnQgQ291bnRlcgpmcm9tIHR5cGluZyBpbXBvcnQgTGlzdCwgRGljdCwgQW55LCBVbmlvbgoKCmRlZiBsZWFybl9kZWNpc2lvbl90cmVlX3RnKAogICAgZXhhbXBsZXM6IExpc3RbRGljdFtzdHIsIEFueV1dLAogICAgYXR0cmlidXRlczogTGlzdFtzdHJdLAogICAgdGFyZ2V0X2F0dHI6IHN0cgopIC0+IFVuaW9uW0RpY3Rbc3RyLCBBbnldLCBBbnldOgogICAgIiIiCiAgICBMZWFybiBhIGRlY2lzaW9uIHRyZWUgdXNpbmcgSUQzIHdpdGggdGlueWdyYWQgZm9yIGVudHJvcHkvZ2Fpbi4KICAgIFJldHVybnMgYSBuZXN0ZWQgZGljdCB0cmVlIG9yIGEgY2xhc3MgbGFiZWwuCiAgICAiIiIKICAgICMgWW91ciBpbXBsZW1lbnRhdGlvbiBoZXJlCiAgICBwYXNzCg==",
  "test_cases": [
    {
      "test": "print(learn_decision_tree([\n    {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'},\n    {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}\n], ['Outlook', 'Wind'], 'PlayTennis'))",
      "expected_output": "{'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"
    }
  ],
  "pytorch_difficulty": "hard",
  "video": null,
  "likes": "0",
  "difficulty": "hard",
  "example": {
    "input": "examples = [\n                    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'},\n                    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'},\n                    {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n                    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'}\n                ],\n                attributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']",
    "output": "{\n            'Outlook': {\n                'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}},\n                'Overcast': 'Yes',\n                'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}\n            }\n        }",
    "reasoning": "Using the given examples, the decision tree algorithm determines that 'Outlook' is the best attribute to split the data initially. When 'Outlook' is 'Overcast', the outcome is always 'Yes', so it becomes a leaf node. In cases of 'Sunny' and 'Rain', it further splits based on 'Humidity' and 'Wind', respectively. The resulting tree structure is able to classify the training examples with the attributes 'Outlook', 'Temperature', 'Humidity', and 'Wind'."
  },
  "dislikes": "0",
  "category": "Machine Learning",
  "starter_code": "def learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n\t# Your code here\n\treturn decision_tree",
  "learn_section": "CiMjIERlY2lzaW9uIFRyZWUgTGVhcm5pbmcgQWxnb3JpdGhtCgpUaGUgZGVjaXNpb24gdHJlZSBsZWFybmluZyBhbGdvcml0aG0gaXMgYSBtZXRob2QgdXNlZCBmb3IgY2xhc3NpZmljYXRpb24gdGhhdCBwcmVkaWN0cyB0aGUgdmFsdWUgb2YgYSB0YXJnZXQgdmFyaWFibGUgYmFzZWQgb24gc2V2ZXJhbCBpbnB1dCB2YXJpYWJsZXMuIEVhY2ggaW50ZXJuYWwgbm9kZSBvZiB0aGUgdHJlZSBjb3JyZXNwb25kcyB0byBhbiBpbnB1dCB2YXJpYWJsZSwgYW5kIGVhY2ggbGVhZiBub2RlIGNvcnJlc3BvbmRzIHRvIGEgY2xhc3MgbGFiZWwuCgojIyMgQWxnb3JpdGhtIE92ZXJ2aWV3ClRoZSByZWN1cnNpdmUgYmluYXJ5IHNwbGl0dGluZyBzdGFydHMgYnkgc2VsZWN0aW5nIHRoZSBhdHRyaWJ1dGUgdGhhdCBiZXN0IHNlcGFyYXRlcyB0aGUgZXhhbXBsZXMgYWNjb3JkaW5nIHRvIHRoZSBlbnRyb3B5IGFuZCBpbmZvcm1hdGlvbiBnYWluLCBjYWxjdWxhdGVkIGFzIGZvbGxvd3M6CgojIyMgRW50cm9weQokJApIKFgpID0gLVxzdW0gcCh4KSBcbG9nXzIgcCh4KQokJAoKIyMjIEluZm9ybWF0aW9uIEdhaW4KJCQKSUcoRCwgQSkgPSBIKEQpIC0gXHN1bSBcZnJhY3t8RF92fH17fER8fSBIKERfdikKJCQKCiMjIyBFeHBsYW5hdGlvbiBvZiBUZXJtcwotICoqRW50cm9weSBcKCBIKFgpIFwpKio6IE1lYXN1cmVzIHRoZSBpbXB1cml0eSBvciBkaXNvcmRlciBvZiB0aGUgc2V0LgotICoqSW5mb3JtYXRpb24gR2FpbiBcKCBJRyhELCBBKSBcKSoqOiBSZXByZXNlbnRzIHRoZSByZWR1Y3Rpb24gaW4gZW50cm9weSBhZnRlciBzcGxpdHRpbmcgdGhlIGRhdGFzZXQgXCggRCBcKSBvbiBhdHRyaWJ1dGUgXCggQSBcKS4KLSAqKlwoIERfdiBcKSoqOiBUaGUgc3Vic2V0IG9mIFwoIEQgXCkgZm9yIHdoaWNoIGF0dHJpYnV0ZSBcKCBBIFwpIGhhcyB2YWx1ZSBcKCB2IFwpLgoKIyMjIFByb2Nlc3MKMS4gKipTZWxlY3QgQXR0cmlidXRlKio6IENob29zZSB0aGUgYXR0cmlidXRlIHdpdGggdGhlIGhpZ2hlc3QgaW5mb3JtYXRpb24gZ2Fpbi4KMi4gKipTcGxpdCBEYXRhc2V0Kio6IERpdmlkZSB0aGUgZGF0YXNldCBiYXNlZCBvbiB0aGUgdmFsdWVzIG9mIHRoZSBzZWxlY3RlZCBhdHRyaWJ1dGUuCjMuICoqUmVjdXJzaW9uKio6IFJlcGVhdCB0aGUgcHJvY2VzcyBmb3IgZWFjaCBzdWJzZXQgdW50aWw6CiAgIC0gQWxsIGRhdGEgaXMgcGVyZmVjdGx5IGNsYXNzaWZpZWQsIG9yCiAgIC0gTm8gcmVtYWluaW5nIGF0dHJpYnV0ZXMgY2FuIGJlIHVzZWQgdG8gbWFrZSBhIHNwbGl0LgoKVGhpcyByZWN1cnNpdmUgcHJvY2VzcyBjb250aW51ZXMgdW50aWwgdGhlIGRlY2lzaW9uIHRyZWUgY2FuIG5vIGxvbmdlciBiZSBzcGxpdCBmdXJ0aGVyIG9yIGFsbCBleGFtcGxlcyBoYXZlIGJlZW4gY2xhc3NpZmllZC4KCg==",
  "title": "Decision Tree Learning",
  "contributor": null,
  "pytorch_test_cases": [
    {
      "test": "print(learn_decision_tree([\n    {'Outlook': 'Sunny',   'Wind': 'Weak',   'PlayTennis': 'No'},\n    {'Outlook': 'Overcast','Wind': 'Strong', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain',    'Wind': 'Weak',   'PlayTennis': 'Yes'},\n    {'Outlook': 'Sunny',   'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Sunny',   'Wind': 'Weak',   'PlayTennis': 'Yes'},\n    {'Outlook': 'Overcast','Wind': 'Weak',   'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain',    'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Rain',    'Wind': 'Weak',   'PlayTennis': 'Yes'}\n], ['Outlook', 'Wind'], 'PlayTennis'))",
      "expected_output": "{'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"
    }
  ],
  "tinygrad_test_cases": [
    {
      "test": "print(learn_decision_tree_tg([\n    {'Outlook': 'Sunny',   'Wind': 'Weak',   'PlayTennis': 'No'},\n    {'Outlook': 'Overcast','Wind': 'Strong', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain',    'Wind': 'Weak',   'PlayTennis': 'Yes'},\n    {'Outlook': 'Sunny',   'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Sunny',   'Wind': 'Weak',   'PlayTennis': 'Yes'},\n    {'Outlook': 'Overcast','Wind': 'Weak',   'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain',    'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Rain',    'Wind': 'Weak',   'PlayTennis': 'Yes'}\n], ['Outlook', 'Wind'], 'PlayTennis'))",
      "expected_output": "{'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"
    }
  ],
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCmltcG9ydCBtYXRoCmZyb20gY29sbGVjdGlvbnMgaW1wb3J0IENvdW50ZXIKZnJvbSB0eXBpbmcgaW1wb3J0IExpc3QsIERpY3QsIEFueSwgVW5pb24KCgpkZWYgY2FsY3VsYXRlX2VudHJvcHkobGFiZWxzOiBMaXN0W0FueV0pIC0+IGZsb2F0OgogICAgIiIiCiAgICBDb21wdXRlIHRoZSBTaGFubm9uIGVudHJvcHkgb2YgdGhlIGxpc3Qgb2YgbGFiZWxzLgogICAgbGFiZWxzOiBsaXN0IG9mIGFueSBoYXNoYWJsZSBpdGVtcy4KICAgIFJldHVybnMgYSBQeXRob24gZmxvYXQuCiAgICAiIiIKICAgICMgWW91ciBpbXBsZW1lbnRhdGlvbiBoZXJlCiAgICBwYXNzCgoKZGVmIGNhbGN1bGF0ZV9pbmZvcm1hdGlvbl9nYWluKAogICAgZXhhbXBsZXM6IExpc3RbRGljdFtzdHIsIEFueV1dLAogICAgYXR0cjogc3RyLAogICAgdGFyZ2V0X2F0dHI6IHN0cgopIC0+IGZsb2F0OgogICAgIiIiCiAgICBDb21wdXRlIGluZm9ybWF0aW9uIGdhaW4gZm9yIHNwbGl0dGluZyBgZXhhbXBsZXNgIG9uIGBhdHRyYCB3LnIudC4gYHRhcmdldF9hdHRyYC4KICAgIFJldHVybnMgYSBQeXRob24gZmxvYXQuCiAgICAiIiIKICAgICMgWW91ciBpbXBsZW1lbnRhdGlvbiBoZXJlCiAgICBwYXNzCgoKZGVmIG1ham9yaXR5X2NsYXNzKAogICAgZXhhbXBsZXM6IExpc3RbRGljdFtzdHIsIEFueV1dLAogICAgdGFyZ2V0X2F0dHI6IHN0cgopIC0+IEFueToKICAgICIiIgogICAgUmV0dXJuIHRoZSBtb3N0IGNvbW1vbiB2YWx1ZSBvZiBgdGFyZ2V0X2F0dHJgIGluIGBleGFtcGxlc2AuCiAgICAiIiIKICAgICMgWW91ciBpbXBsZW1lbnRhdGlvbiBoZXJlCiAgICBwYXNzCgoKZGVmIGxlYXJuX2RlY2lzaW9uX3RyZWUoCiAgICBleGFtcGxlczogTGlzdFtEaWN0W3N0ciwgQW55XV0sCiAgICBhdHRyaWJ1dGVzOiBMaXN0W3N0cl0sCiAgICB0YXJnZXRfYXR0cjogc3RyCikgLT4gVW5pb25bRGljdFtzdHIsIEFueV0sIEFueV06CiAgICAiIiIKICAgIExlYXJuIGEgZGVjaXNpb24gdHJlZSB1c2luZyB0aGUgSUQzIGFsZ29yaXRobS4KICAgIFJldHVybnMgZWl0aGVyIGEgbmVzdGVkIGRpY3QgcmVwcmVzZW50aW5nIHRoZSB0cmVlIG9yIGEgY2xhc3MgbGFiZWwgYXQgdGhlIGxlYXZlcy4KICAgICIiIgogICAgIyBZb3VyIGltcGxlbWVudGF0aW9uIGhlcmUKICAgIHBhc3MK",
  "description_decoded": "Write a Python function that implements the decision tree learning algorithm for classification. The function should use recursive binary splitting based on entropy and information gain to build a decision tree. It should take a list of examples (each example is a dict of attribute-value pairs) and a list of attribute names as input, and return a nested dictionary representing the decision tree.",
  "learn_section_decoded": "\n## Decision Tree Learning Algorithm\n\nThe decision tree learning algorithm is a method used for classification that predicts the value of a target variable based on several input variables. Each internal node of the tree corresponds to an input variable, and each leaf node corresponds to a class label.\n\n### Algorithm Overview\nThe recursive binary splitting starts by selecting the attribute that best separates the examples according to the entropy and information gain, calculated as follows:\n\n### Entropy\n$$\nH(X) = -\\sum p(x) \\log_2 p(x)\n$$\n\n### Information Gain\n$$\nIG(D, A) = H(D) - \\sum \\frac{|D_v|}{|D|} H(D_v)\n$$\n\n### Explanation of Terms\n- **Entropy \\( H(X) \\)**: Measures the impurity or disorder of the set.\n- **Information Gain \\( IG(D, A) \\)**: Represents the reduction in entropy after splitting the dataset \\( D \\) on attribute \\( A \\).\n- **\\( D_v \\)**: The subset of \\( D \\) for which attribute \\( A \\) has value \\( v \\).\n\n### Process\n1. **Select Attribute**: Choose the attribute with the highest information gain.\n2. **Split Dataset**: Divide the dataset based on the values of the selected attribute.\n3. **Recursion**: Repeat the process for each subset until:\n   - All data is perfectly classified, or\n   - No remaining attributes can be used to make a split.\n\nThis recursive process continues until the decision tree can no longer be split further or all examples have been classified.\n\n",
  "tinygrad_starter_code_decoded": "import math\nfrom collections import Counter\nfrom typing import List, Dict, Any, Union\n\n\ndef learn_decision_tree_tg(\n    examples: List[Dict[str, Any]],\n    attributes: List[str],\n    target_attr: str\n) -> Union[Dict[str, Any], Any]:\n    \"\"\"\n    Learn a decision tree using ID3 with tinygrad for entropy/gain.\n    Returns a nested dict tree or a class label.\n    \"\"\"\n    # Your implementation here\n    pass\n"
}