{
  "description": "SW1wbGVtZW50IHRoZSBncmFkaWVudCBiYW5kaXQgYWxnb3JpdGhtIGZvciBhY3Rpb24gc2VsZWN0aW9uIGluIGEgbXVsdGktYXJtZWQgYmFuZGl0IHNldHRpbmcuIFdyaXRlIGEgY2xhc3MgR3JhZGllbnRCYW5kaXQgdGhhdCBtYWludGFpbnMgYSBzZXQgb2YgYWN0aW9uIHByZWZlcmVuY2VzIGFuZCB1cGRhdGVzIHRoZW0gYWZ0ZXIgZWFjaCByZXdhcmQuIFRoZSBjbGFzcyBzaG91bGQgcHJvdmlkZSBhIG1ldGhvZCBgc2VsZWN0X2FjdGlvbigpYCB0byBzYW1wbGUgYW4gYWN0aW9uIHVzaW5nIHRoZSBzb2Z0bWF4IG9mIHByZWZlcmVuY2VzLCBhbmQgYSBtZXRob2QgYHVwZGF0ZShhY3Rpb24sIHJld2FyZClgIHRvIHVwZGF0ZSBwcmVmZXJlbmNlcyB1c2luZyB0aGUgZ3JhZGllbnQgYXNjZW50IHVwZGF0ZSBydWxlLiBVc2Ugb25seSBOdW1QeS4=",
  "id": "163",
  "test_cases": [
    {
      "test": "import numpy as np\nnp.random.seed(0)\ngb = GradientBandit(num_actions=3, alpha=0.1)\na1 = gb.select_action()\ngb.update(a1, reward=1.0)\na2 = gb.select_action()\ngb.update(a2, reward=0.0)\nprint(a1, a2)",
      "expected_output": "1 2"
    },
    {
      "test": "import numpy as np\ngb = GradientBandit(num_actions=2, alpha=0.2)\nacts = [gb.select_action() for _ in range(10)]\nprint(len(acts), set(acts).issubset({0,1}))",
      "expected_output": "10 True"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "import numpy as np\ngb = GradientBandit(num_actions=3, alpha=0.1)\na = gb.select_action()\ngb.update(a, reward=1.0)\nprobs = gb.softmax()\nprint(np.round(probs, 2).tolist())",
    "output": "[0.32, 0.34, 0.34]",
    "reasoning": "After a positive reward, the selected action's preference is increased, boosting its softmax probability."
  },
  "category": "Reinforcement Learning",
  "starter_code": "import numpy as np\n\nclass GradientBandit:\n    def __init__(self, num_actions, alpha=0.1):\n        \"\"\"\n        num_actions (int): Number of possible actions\n        alpha (float): Step size for preference updates\n        \"\"\"\n        self.num_actions = num_actions\n        self.alpha = alpha\n        self.preferences = np.zeros(num_actions)\n        self.avg_reward = 0.0\n        self.time = 0\n    def softmax(self):\n        # Compute softmax probabilities from preferences\n        pass\n    def select_action(self):\n        # Sample an action according to the softmax distribution\n        pass\n    def update(self, action, reward):\n        # Update action preferences using the gradient ascent update\n        pass",
  "title": "Gradient Bandit Action Selection",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "description_decoded": "Implement the gradient bandit algorithm for action selection in a multi-armed bandit setting. Write a class GradientBandit that maintains a set of action preferences and updates them after each reward. The class should provide a method `select_action()` to sample an action using the softmax of preferences, and a method `update(action, reward)` to update preferences using the gradient ascent update rule. Use only NumPy.",
  "learn_section_decoded": "# **Gradient Bandits**\n\nGradient Bandit algorithms are a family of action-selection methods for multi-armed bandit problems. Instead of estimating action values, they maintain a set of *preferences* for each action and use these to generate a probability distribution over actions via the softmax function. The algorithm then updates these preferences directly to increase the likelihood of selecting actions that yield higher rewards.\n\n---\n\n## **Algorithm Outline**\n\n1. **Preferences** ($H_a$): For each action $a$, keep a real-valued preference $H_a$ (initialized to zero).\n2. **Action Probabilities (Softmax):** At each timestep, choose action $a$ with probability:\n\n$$\nP(a) = \\frac{e^{H_a}}{\\sum_j e^{H_j}}\n$$\n\n3. **Preference Update Rule:** After receiving reward $R_t$ for selected action $A_t$, update preferences as:\n\n$$\nH_a \\leftarrow H_a + \\alpha \\cdot (R_t - \\bar{R_t}) \\cdot (1 - P(a)), \\text{ if } a = A_t\n$$\n$$\nH_a \\leftarrow H_a - \\alpha \\cdot (R_t - \\bar{R_t}) \\cdot P(a), \\text{ if } a \\neq A_t\n$$\nWhere:\n- $\\bar{R_t}$ is the running average reward (baseline, helps reduce variance)\n- $\\alpha$ is the step size\n\n---\n\n## **Key Properties**\n- Uses *softmax* probabilities for exploration (all actions get non-zero probability)\n- Action preferences directly drive probability updates\n- The baseline $\\bar{R_t}$ stabilizes learning and reduces update variance\n- More likely to select actions with higher expected reward\n\n---\n\n## **When to Use Gradient Bandits?**\n- Problems where the best action changes over time (non-stationary)\n- Situations requiring continuous, adaptive exploration\n- Settings where value estimates are unreliable or less stable\n\n---\n\n## **Summary**\nGradient bandit methods offer a principled way to learn action preferences by maximizing expected reward via gradient ascent. Their use of the softmax function ensures robust, probabilistic exploration and efficient learning from feedback."
}