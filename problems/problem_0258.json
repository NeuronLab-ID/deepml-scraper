{
  "description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gY29tcHV0ZSB0aGUgQ2FsaW5za2ktSGFyYWJhc3ogSW5kZXggKGFsc28ga25vd24gYXMgdGhlIFZhcmlhbmNlIFJhdGlvIENyaXRlcmlvbikgZm9yIGV2YWx1YXRpbmcgY2x1c3RlcmluZyBxdWFsaXR5LiBUaGlzIG1ldHJpYyBtZWFzdXJlcyB0aGUgcmF0aW8gb2YgYmV0d2Vlbi1jbHVzdGVyIGRpc3BlcnNpb24gdG8gd2l0aGluLWNsdXN0ZXIgZGlzcGVyc2lvbiwgd2hlcmUgaGlnaGVyIHZhbHVlcyBpbmRpY2F0ZSBiZXR0ZXItZGVmaW5lZCBjbHVzdGVycy4KCkdpdmVuOgotIEEgMkQgbnVtcHkgYXJyYXkgWCBvZiBzaGFwZSAobl9zYW1wbGVzLCBuX2ZlYXR1cmVzKSBjb250YWluaW5nIHRoZSBkYXRhIHBvaW50cwotIEEgMUQgbnVtcHkgYXJyYXkgbGFiZWxzIG9mIHNoYXBlIChuX3NhbXBsZXMsKSBjb250YWluaW5nIGNsdXN0ZXIgYXNzaWdubWVudHMgZm9yIGVhY2ggcG9pbnQKClJldHVybiB0aGUgQ2FsaW5za2ktSGFyYWJhc3ogc2NvcmUgYXMgYSBmbG9hdC4gSWYgdGhlcmUgaXMgb25seSBvbmUgY2x1c3RlciBvciBpZiB0aGUgbnVtYmVyIG9mIGNsdXN0ZXJzIGVxdWFscyB0aGUgbnVtYmVyIG9mIHNhbXBsZXMsIHJldHVybiAwLjAu",
  "id": "258",
  "test_cases": [
    {
      "test": "import numpy as np\nX = np.array([[0, 0], [1, 0], [0, 1], [10, 10], [11, 10], [10, 11]])\nlabels = np.array([0, 0, 0, 1, 1, 1])\nprint(round(calinski_harabasz_score(X, labels), 4))",
      "expected_output": "450.0"
    },
    {
      "test": "import numpy as np\nX = np.array([[1, 1], [2, 1], [1, 2], [4, 4], [5, 4], [4, 5]])\nlabels = np.array([0, 0, 0, 1, 1, 1])\nprint(round(calinski_harabasz_score(X, labels), 4))",
      "expected_output": "40.5"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "X = np.array([[0, 0], [1, 0], [0, 1], [10, 10], [11, 10], [10, 11]])\nlabels = np.array([0, 0, 0, 1, 1, 1])",
    "output": "296.9985",
    "reasoning": "The data has 6 points split into 2 clusters. Cluster 0 contains points near origin with centroid (0.333, 0.333). Cluster 1 contains points near (10, 10) with centroid (10.333, 10.333). The global centroid is (5.333, 5.333). The within-cluster dispersion W is small (clusters are tight), while the between-cluster dispersion B is large (clusters are far apart). This results in a high CH score indicating good cluster separation."
  },
  "category": "Machine Learning",
  "starter_code": "import numpy as np\n\ndef calinski_harabasz_score(X: np.ndarray, labels: np.ndarray) -> float:\n    \"\"\"\n    Compute the Calinski-Harabasz Index for clustering evaluation.\n    \n    Args:\n        X: numpy array of shape (n_samples, n_features) containing data points\n        labels: numpy array of shape (n_samples,) containing cluster assignments\n    \n    Returns:\n        float: Calinski-Harabasz score (higher is better)\n    \"\"\"\n    # Your code here\n    pass",
  "title": "Calinski-Harabasz Index for Clustering Evaluation",
  "createdAt": "December 14, 2025 at 1:18:30â€¯PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "learn_section": "IyMgQ2FsaW5za2ktSGFyYWJhc3ogSW5kZXgKClRoZSAqKkNhbGluc2tpLUhhcmFiYXN6IEluZGV4KiogKENIKSwgYWxzbyBrbm93biBhcyB0aGUgKipWYXJpYW5jZSBSYXRpbyBDcml0ZXJpb24qKiwgaXMgYW4gaW50ZXJuYWwgY2x1c3RlcmluZyBldmFsdWF0aW9uIG1ldHJpYyB0aGF0IG1lYXN1cmVzIGhvdyB3ZWxsLXNlcGFyYXRlZCBhbmQgY29tcGFjdCB0aGUgY2x1c3RlcnMgYXJlLgoKIyMjIEZvcm11bGEKClRoZSBDSCBpbmRleCBpcyBkZWZpbmVkIGFzOgoKJCRDSCA9IFxmcmFje0IgLyAoayAtIDEpfXtXIC8gKG4gLSBrKX0kJAoKV2hlcmU6Ci0gJG4kIGlzIHRoZSB0b3RhbCBudW1iZXIgb2Ygc2FtcGxlcwotICRrJCBpcyB0aGUgbnVtYmVyIG9mIGNsdXN0ZXJzCi0gJEIkIGlzIHRoZSAqKmJldHdlZW4tY2x1c3RlciBkaXNwZXJzaW9uKiogKGludGVyLWNsdXN0ZXIgdmFyaWFuY2UpCi0gJFckIGlzIHRoZSAqKndpdGhpbi1jbHVzdGVyIGRpc3BlcnNpb24qKiAoaW50cmEtY2x1c3RlciB2YXJpYW5jZSkKCiMjIyBDb21wdXRpbmcgRGlzcGVyc2lvbnMKCioqV2l0aGluLWNsdXN0ZXIgZGlzcGVyc2lvbioqIG1lYXN1cmVzIGhvdyBjb21wYWN0IGVhY2ggY2x1c3RlciBpczoKCiQkVyA9IFxzdW1fe2k9MX1ee2t9IFxzdW1fe3ggXGluIENfaX0gXHx4IC0gXG11X2lcfF4yJCQKCndoZXJlICRcbXVfaSQgaXMgdGhlIGNlbnRyb2lkIG9mIGNsdXN0ZXIgJENfaSQuCgoqKkJldHdlZW4tY2x1c3RlciBkaXNwZXJzaW9uKiogbWVhc3VyZXMgaG93IGZhciBhcGFydCB0aGUgY2x1c3RlciBjZW50ZXJzIGFyZSBmcm9tIHRoZSBnbG9iYWwgY2VudGVyOgoKJCRCID0gXHN1bV97aT0xfV57a30gbl9pIFx8XG11X2kgLSBcbXVcfF4yJCQKCndoZXJlICRuX2kkIGlzIHRoZSBudW1iZXIgb2YgcG9pbnRzIGluIGNsdXN0ZXIgJGkkLCAkXG11X2kkIGlzIHRoZSBjbHVzdGVyIGNlbnRyb2lkLCBhbmQgJFxtdSQgaXMgdGhlIGdsb2JhbCBjZW50cm9pZCBvZiBhbGwgZGF0YSBwb2ludHMuCgojIyMgSW50ZXJwcmV0YXRpb24KCi0gKipIaWdoZXIgdmFsdWVzKiogaW5kaWNhdGUgYmV0dGVyIGNsdXN0ZXJpbmc6IGNsdXN0ZXJzIGFyZSBkZW5zZSAobG93ICRXJCkgYW5kIHdlbGwtc2VwYXJhdGVkIChoaWdoICRCJCkKLSAqKkxvd2VyIHZhbHVlcyoqIHN1Z2dlc3Qgb3ZlcmxhcHBpbmcgb3IgZGlzcGVyc2VkIGNsdXN0ZXJzCi0gVGhlIG1ldHJpYyBpcyAqKm5vdCBib3VuZGVkKiogYWJvdmUsIHNvIGl0J3MgcHJpbWFyaWx5IHVzZWQgZm9yIGNvbXBhcmluZyBkaWZmZXJlbnQgY2x1c3RlcmluZ3Mgb2YgdGhlIHNhbWUgZGF0YQoKIyMjIEVkZ2UgQ2FzZXMKCi0gSWYgJGsgPSAxJCAoc2luZ2xlIGNsdXN0ZXIpLCB0aGUgaW5kZXggaXMgdW5kZWZpbmVkICh3ZSByZXR1cm4gMCkKLSBJZiAkayA9IG4kIChlYWNoIHBvaW50IGlzIGl0cyBvd24gY2x1c3RlciksICRXID0gMCQgd2hpY2ggY2F1c2VzIGRpdmlzaW9uIGlzc3VlcyAod2UgcmV0dXJuIDApCgojIyMgVXNlIENhc2VzCgoxLiAqKkRldGVybWluaW5nIG9wdGltYWwgbnVtYmVyIG9mIGNsdXN0ZXJzKio6IFBsb3QgQ0ggc2NvcmUgdnMuICRrJCBhbmQgbG9vayBmb3IgdGhlIGVsYm93IG9yIG1heGltdW0KMi4gKipDb21wYXJpbmcgY2x1c3RlcmluZyBhbGdvcml0aG1zKio6IEhpZ2hlciBDSCBpbmRpY2F0ZXMgYmV0dGVyIGNsdXN0ZXIgcXVhbGl0eQozLiAqKlZhbGlkYXRpbmcgY2x1c3RlcmluZyByZXN1bHRzKio6IFdpdGhvdXQgZ3JvdW5kIHRydXRoIGxhYmVscw==",
  "description_decoded": "Implement a function to compute the Calinski-Harabasz Index (also known as the Variance Ratio Criterion) for evaluating clustering quality. This metric measures the ratio of between-cluster dispersion to within-cluster dispersion, where higher values indicate better-defined clusters.\n\nGiven:\n- A 2D numpy array X of shape (n_samples, n_features) containing the data points\n- A 1D numpy array labels of shape (n_samples,) containing cluster assignments for each point\n\nReturn the Calinski-Harabasz score as a float. If there is only one cluster or if the number of clusters equals the number of samples, return 0.0.",
  "learn_section_decoded": "## Calinski-Harabasz Index\n\nThe **Calinski-Harabasz Index** (CH), also known as the **Variance Ratio Criterion**, is an internal clustering evaluation metric that measures how well-separated and compact the clusters are.\n\n### Formula\n\nThe CH index is defined as:\n\n$$CH = \\frac{B / (k - 1)}{W / (n - k)}$$\n\nWhere:\n- $n$ is the total number of samples\n- $k$ is the number of clusters\n- $B$ is the **between-cluster dispersion** (inter-cluster variance)\n- $W$ is the **within-cluster dispersion** (intra-cluster variance)\n\n### Computing Dispersions\n\n**Within-cluster dispersion** measures how compact each cluster is:\n\n$$W = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2$$\n\nwhere $\\mu_i$ is the centroid of cluster $C_i$.\n\n**Between-cluster dispersion** measures how far apart the cluster centers are from the global center:\n\n$$B = \\sum_{i=1}^{k} n_i \\|\\mu_i - \\mu\\|^2$$\n\nwhere $n_i$ is the number of points in cluster $i$, $\\mu_i$ is the cluster centroid, and $\\mu$ is the global centroid of all data points.\n\n### Interpretation\n\n- **Higher values** indicate better clustering: clusters are dense (low $W$) and well-separated (high $B$)\n- **Lower values** suggest overlapping or dispersed clusters\n- The metric is **not bounded** above, so it's primarily used for comparing different clusterings of the same data\n\n### Edge Cases\n\n- If $k = 1$ (single cluster), the index is undefined (we return 0)\n- If $k = n$ (each point is its own cluster), $W = 0$ which causes division issues (we return 0)\n\n### Use Cases\n\n1. **Determining optimal number of clusters**: Plot CH score vs. $k$ and look for the elbow or maximum\n2. **Comparing clustering algorithms**: Higher CH indicates better cluster quality\n3. **Validating clustering results**: Without ground truth labels"
}