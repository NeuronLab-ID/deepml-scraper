{
  "description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBwZXJmb3JtcyBNaW4tTWF4IE5vcm1hbGl6YXRpb24gb24gYSBsaXN0IG9mIG51bWJlcnMsIHNjYWxpbmcgYWxsIHZhbHVlcyB0byB0aGUgcmFuZ2UgWzAsIDFdLiBUaGUgbWluaW11bSB2YWx1ZSBpbiB0aGUgbGlzdCBzaG91bGQgYmVjb21lIDAsIHRoZSBtYXhpbXVtIHNob3VsZCBiZWNvbWUgMSwgYW5kIGFsbCBvdGhlciB2YWx1ZXMgc2hvdWxkIGJlIHNjYWxlZCBwcm9wb3J0aW9uYWxseSBiZXR3ZWVuIHRoZW0uCgpNaW4tTWF4IG5vcm1hbGl6YXRpb24gZW5zdXJlcyB0aGF0IGFsbCBmZWF0dXJlcyBjb250cmlidXRlIGVxdWFsbHkgdG8gYSBtb2RlbCBieSB0cmFuc2Zvcm1pbmcgdGhlbSB0byBhIGNvbW1vbiBzY2FsZS4KCklmIGFsbCB2YWx1ZXMgaW4gdGhlIGlucHV0IGFyZSBpZGVudGljYWwsIHJldHVybiBhIGxpc3Qgb2YgemVyb3MgdG8gYXZvaWQgZGl2aXNpb24gYnkgemVyby4=",
  "id": "112",
  "test_cases": [
    {
      "test": "print([round(x, 4) for x in min_max([1, 2, 3, 4, 5])])",
      "expected_output": "[0.0, 0.25, 0.5, 0.75, 1.0]"
    },
    {
      "test": "print([round(x, 4) for x in min_max([30, 45, 56, 70, 88])])",
      "expected_output": "[0.0, 0.2586, 0.4483, 0.6897, 1.0]"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "min_max([1, 2, 3, 4, 5])",
    "output": "[0.0, 0.25, 0.5, 0.75, 1.0]",
    "reasoning": "The minimum value (1) becomes 0.0, the maximum value (5) becomes 1.0, and the values in between are scaled proportionally. For instance, 3 is exactly halfway between 1 and 5, so it becomes 0.5."
  },
  "category": "Data Preprocessing",
  "starter_code": "def min_max(x: list[float]) -> list[float]:\n    \"\"\"\n    Perform Min-Max normalization to scale values to [0, 1].\n    \n    Args:\n        x: A list of numerical values\n    \n    Returns:\n        A new list with values normalized to [0, 1]\n    \"\"\"\n    # Your code here\n    pass",
  "title": "Min-Max Normalization of Feature Values",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/Noth2006",
      "name": "Noth2006"
    }
  ],
  "createdAt": "December 15, 2025 at 9:34:13â€¯AM UTC-0500",
  "description_decoded": "Implement a function that performs Min-Max Normalization on a list of numbers, scaling all values to the range [0, 1]. The minimum value in the list should become 0, the maximum should become 1, and all other values should be scaled proportionally between them.\n\nMin-Max normalization ensures that all features contribute equally to a model by transforming them to a common scale.\n\nIf all values in the input are identical, return a list of zeros to avoid division by zero.",
  "learn_section_decoded": "## Understanding Min-Max Normalization\n\nMin-Max Normalization is a technique used to rescale numerical data to the range $[0, 1]$.\n\n### Formula\n\n$$\nX' = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n$$\n\nWhere:\n- $X$ is the original value\n- $X_{\\min}$ is the minimum value in the dataset\n- $X_{\\max}$ is the maximum value in the dataset\n- $X'$ is the normalized value\n\n### Why Normalize?\n\n1. **Equal contribution**: Ensures all features have equal importance regardless of their original scale\n2. **Algorithm requirements**: Many ML algorithms (k-NN, neural networks, SVM) perform better with normalized data\n3. **Faster convergence**: Gradient descent converges faster when features are on similar scales\n4. **Distance metrics**: Prevents features with larger ranges from dominating distance calculations\n\n### Special Case: Identical Values\n\nIf all elements in the input are identical, then $X_{\\max} = X_{\\min}$, which would cause division by zero. In this case, we return an array of zeros.\n\n### Example Walkthrough\n\nGiven the input `[1, 2, 3, 4, 5]`:\n\n1. Find minimum: $X_{\\min} = 1$\n2. Find maximum: $X_{\\max} = 5$\n3. Calculate range: $X_{\\max} - X_{\\min} = 4$\n4. Normalize each value:\n\n$$\n\\begin{aligned}\n&\\frac{1 - 1}{4} = 0.0 \\\\\n&\\frac{2 - 1}{4} = 0.25 \\\\\n&\\frac{3 - 1}{4} = 0.5 \\\\\n&\\frac{4 - 1}{4} = 0.75 \\\\\n&\\frac{5 - 1}{4} = 1.0\n\\end{aligned}\n$$\n\nResult: `[0.0, 0.25, 0.5, 0.75, 1.0]`\n\n### Implementation Notes\n\n- Create a **new list** rather than modifying the input (avoid side effects)\n- Handle the edge case where all values are identical\n\n### Min-Max vs Z-Score Normalization\n\n| Property | Min-Max | Z-Score |\n|----------|---------|--------|\n| Range | $[0, 1]$ | Unbounded |\n| Sensitive to outliers | Yes | Less so |\n| Preserves zero | No | Yes |\n| Use case | Bounded algorithms | Statistical analysis |\n"
}