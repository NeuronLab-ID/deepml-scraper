{
  "description": "SW1wbGVtZW50IGdyYWRpZW50IGNsaXBwaW5nIGJ5IGdsb2JhbCBub3JtLiBHaXZlbiBhIGxpc3Qgb2YgZ3JhZGllbnQgYXJyYXlzIChyZXByZXNlbnRpbmcgZ3JhZGllbnRzIGZvciBkaWZmZXJlbnQgcGFyYW1ldGVycykgYW5kIGEgbWF4aW11bSBub3JtIHRocmVzaG9sZCwgY29tcHV0ZSB0aGUgZ2xvYmFsIEwyIG5vcm0gYWNyb3NzIGFsbCBncmFkaWVudHMuIElmIHRoaXMgZ2xvYmFsIG5vcm0gZXhjZWVkcyB0aGUgdGhyZXNob2xkLCBzY2FsZSBkb3duIGFsbCBncmFkaWVudHMgcHJvcG9ydGlvbmFsbHkgc28gdGhhdCB0aGUgZ2xvYmFsIG5vcm0gZXF1YWxzIHRoZSB0aHJlc2hvbGQuIFJldHVybiB0aGUgY2xpcHBlZCBncmFkaWVudHMgbWFpbnRhaW5pbmcgdGhlIG9yaWdpbmFsIHN0cnVjdHVyZS4=",
  "id": "197",
  "test_cases": [
    {
      "test": "print([[round(x, 4) for x in arr] for arr in clip_gradients_by_global_norm([[1.0, 2.0], [3.0, 4.0]], 10.0)])",
      "expected_output": "[[1.0, 2.0], [3.0, 4.0]]"
    },
    {
      "test": "print([[round(x, 4) for x in arr] for arr in clip_gradients_by_global_norm([[3.0, 4.0], [0.0, 0.0]], 1.0)])",
      "expected_output": "[[0.6, 0.8], [0.0, 0.0]]"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "gradients=[[3.0, 4.0], [0.0, 0.0]], max_norm=1.0",
    "output": "[[0.6, 0.8], [0.0, 0.0]]",
    "reasoning": "The global norm is $\\sqrt{3^2 + 4^2 + 0^2 + 0^2} = \\sqrt{25} = 5.0$. Since $5.0 > 1.0$, we need to clip. The scaling factor is $\\frac{1.0}{5.0} = 0.2$. Each gradient is multiplied by 0.2: $[3.0 \\times 0.2, 4.0 \\times 0.2] = [0.6, 0.8]$ and $[0.0 \\times 0.2, 0.0 \\times 0.2] = [0.0, 0.0]$."
  },
  "category": "Optimization",
  "starter_code": "def clip_gradients_by_global_norm(gradients: list[list[float]], max_norm: float) -> list[list[float]]:\n\t\"\"\"\n\tClip gradients by global norm.\n\t\n\tArgs:\n\t\tgradients: List of gradient arrays\n\t\tmax_norm: Maximum allowed global norm\n\t\n\tReturns:\n\t\tList of clipped gradient arrays\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Gradient Clipping by Global Norm",
  "createdAt": "November 9, 2025 at 3:53:05 PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement gradient clipping by global norm. Given a list of gradient arrays (representing gradients for different parameters) and a maximum norm threshold, compute the global L2 norm across all gradients. If this global norm exceeds the threshold, scale down all gradients proportionally so that the global norm equals the threshold. Return the clipped gradients maintaining the original structure.",
  "learn_section_decoded": "### Understanding Gradient Clipping by Global Norm\n\nGradient clipping is a crucial technique in training deep neural networks, especially recurrent neural networks (RNNs), to prevent the exploding gradient problem. Global norm clipping is one of the most effective approaches.\n\n#### The Problem: Exploding Gradients\n\nDuring backpropagation, gradients can become extremely large, causing:\n- Numerical instability (NaN or Inf values)\n- Large, erratic parameter updates\n- Training divergence\n- Model failure to converge\n\n#### Global Norm Clipping Solution\n\nInstead of clipping each gradient individually, global norm clipping considers all gradients together:\n\n**Step 1: Compute Global Norm**\n\nThe global norm is the L2 norm of all gradients combined:\n\n$$\n\\|g\\|_{global} = \\sqrt{\\sum_{i=1}^{n} \\sum_{j=1}^{m_i} g_{i,j}^2}\n$$\n\nWhere:\n- $g_{i,j}$ represents the $j$-th element of the $i$-th gradient array\n- $n$ is the number of gradient arrays (one per parameter group)\n- $m_i$ is the length of the $i$-th gradient array\n\n**Step 2: Compute Clipping Coefficient**\n\n$$\nc = \\frac{\\text{max_norm}}{\\|g\\|_{global}}\n$$\n\n**Step 3: Scale Gradients (if needed)**\n\nIf $c < 1$ (i.e., global norm exceeds threshold):\n\n$$\ng'_{i,j} = c \\cdot g_{i,j}\n$$\n\nOtherwise, gradients remain unchanged.\n\n#### Key Properties\n\n1. **Preserves Direction**: Clipping scales all gradients by the same factor, maintaining the overall direction of the update.\n\n2. **Proportional Scaling**: Larger gradients are scaled down more in absolute terms, but all gradients maintain their relative magnitudes.\n\n3. **Threshold Control**: The `max_norm` parameter directly controls the maximum allowed gradient magnitude.\n\n#### Example Walkthrough\n\nGiven:\n- Gradients: `[[3.0, 4.0], [0.0, 0.0]]`\n- Max norm: `1.0`\n\n**Computation:**\n\n1. Global norm: $\\sqrt{3^2 + 4^2 + 0^2 + 0^2} = \\sqrt{25} = 5.0$\n\n2. Clipping coefficient: $c = \\frac{1.0}{5.0} = 0.2$\n\n3. Since $c < 1$, we clip:\n   - First array: $[3.0 \\times 0.2, 4.0 \\times 0.2] = [0.6, 0.8]$\n   - Second array: $[0.0 \\times 0.2, 0.0 \\times 0.2] = [0.0, 0.0]$\n\n4. Verify: New global norm = $\\sqrt{0.6^2 + 0.8^2} = \\sqrt{0.36 + 0.64} = \\sqrt{1.0} = 1.0$ ✓\n\n#### Practical Considerations\n\n- **Common max_norm values**: 1.0 to 5.0 for RNNs, 0.5 to 2.0 for transformers\n- **When to use**: Essential for RNNs, LSTMs, GRUs; helpful for deep networks\n- **Computational cost**: Minimal - just one additional pass through gradients\n- **Alternative**: Gradient clipping by value (clips each gradient independently)\n\n#### Comparison with Other Methods\n\n| Method | Scope | Preserves Direction | Use Case |\n|--------|-------|-------------------|----------|\n| Global Norm | All gradients | Yes | Preferred for most cases |\n| By Value | Individual elements | No | Simple but less effective |\n| By Norm (per-parameter) | Per parameter | Partially | When parameters have very different scales |\n\nGlobal norm clipping is the recommended approach in modern deep learning frameworks like PyTorch (`torch.nn.utils.clip_grad_norm_`) and TensorFlow (`tf.clip_by_global_norm`)."
}