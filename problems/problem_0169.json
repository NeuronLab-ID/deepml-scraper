{
  "description": "SW1wbGVtZW50IGEgc2luZ2xlIHVwZGF0ZSBzdGVwIG9mIHRoZSBBZGFtVyBvcHRpbWl6ZXIgZm9yIGEgcGFyYW1ldGVyIHZlY3RvciAkdyQgYW5kIGl0cyBncmFkaWVudHMgJGckLiBBZGFtVyBpcyBhIHZhcmlhbnQgb2YgdGhlIEFkYW0gb3B0aW1pemVyIHRoYXQgZGVjb3VwbGVzIHdlaWdodCBkZWNheSBmcm9tIHRoZSBncmFkaWVudCB1cGRhdGUsIGxlYWRpbmcgdG8gYmV0dGVyIGdlbmVyYWxpemF0aW9uLgoKKipZb3VyIFRhc2s6KioKV3JpdGUgYSBmdW5jdGlvbiBgYWRhbXdfdXBkYXRlKHcsIGcsIG0sIHYsIHQsIGxyLCBiZXRhMSwgYmV0YTIsIGVwc2lsb24sIHdlaWdodF9kZWNheSlgIHRoYXQgcGVyZm9ybXMgb25lIHVwZGF0ZSBzdGVwIGZvciB0aGUgcGFyYW1ldGVyIHZlY3RvciAkdyQgd2l0aCBpdHMgZ3JhZGllbnQgJGckIHVzaW5nIEFkYW1XLiBUaGUgZnVuY3Rpb24gc2hvdWxkOgotIFVwZGF0ZSB0aGUgZmlyc3QgbW9tZW50ICRtJCBhbmQgc2Vjb25kIG1vbWVudCAkdiQgKG1vdmluZyBhdmVyYWdlcyBvZiBncmFkaWVudHMgYW5kIHNxdWFyZWQgZ3JhZGllbnRzKQotIEFwcGx5IGJpYXMgY29ycmVjdGlvbiBmb3IgJG0kIGFuZCAkdiQKLSBBcHBseSB0aGUgQWRhbVcgdXBkYXRlIHJ1bGUgKHdpdGggZGVjb3VwbGVkIHdlaWdodCBkZWNheSkKLSBSZXR1cm4gdGhlIHVwZGF0ZWQgcGFyYW1ldGVyIHZlY3RvciBhbmQgdGhlIG5ldyB2YWx1ZXMgb2YgJG0kIGFuZCAkdiQKCioqQXJndW1lbnRzOioqCi0gYHdgOiBOdW1QeSBhcnJheSwgY3VycmVudCBwYXJhbWV0ZXIgdmVjdG9yCi0gYGdgOiBOdW1QeSBhcnJheSwgZ3JhZGllbnQgdmVjdG9yIChzYW1lIHNoYXBlIGFzIGB3YCkKLSBgbWA6IE51bVB5IGFycmF5LCBmaXJzdCBtb21lbnQgdmVjdG9yIChzYW1lIHNoYXBlIGFzIGB3YCkKLSBgdmA6IE51bVB5IGFycmF5LCBzZWNvbmQgbW9tZW50IHZlY3RvciAoc2FtZSBzaGFwZSBhcyBgd2ApCi0gYHRgOiBJbnRlZ2VyLCBjdXJyZW50IHRpbWUgc3RlcCAoc3RhcnRpbmcgZnJvbSAxKQotIGBscmA6IExlYXJuaW5nIHJhdGUgKGZsb2F0KQotIGBiZXRhMWA6IERlY2F5IHJhdGUgZm9yIHRoZSBmaXJzdCBtb21lbnQgKGZsb2F0KQotIGBiZXRhMmA6IERlY2F5IHJhdGUgZm9yIHRoZSBzZWNvbmQgbW9tZW50IChmbG9hdCkKLSBgZXBzaWxvbmA6IFNtYWxsIGNvbnN0YW50IGZvciBudW1lcmljYWwgc3RhYmlsaXR5IChmbG9hdCkKLSBgd2VpZ2h0X2RlY2F5YDogV2VpZ2h0IGRlY2F5IGNvZWZmaWNpZW50IChmbG9hdCkKCg==",
  "id": "169",
  "test_cases": [
    {
      "test": "import numpy as np\nw = np.array([1.0, 2.0])\ng = np.array([0.1, -0.2])\nm = np.zeros(2)\nv = np.zeros(2)\nt = 1\nlr = 0.01\nbeta1 = 0.9\nbeta2 = 0.999\nepsilon = 1e-8\nweight_decay = 0.1\nw_new, m_new, v_new = adamw_update(w, g, m, v, t, lr, beta1, beta2, epsilon, weight_decay)\nprint(np.round(w_new, 4))",
      "expected_output": "[0.989, 2.008]"
    },
    {
      "test": "import numpy as np\nw = np.array([1.5, -1.0])\ng = np.array([-0.3, 0.5])\nm = np.zeros(2)\nv = np.zeros(2)\nt = 1\nlr = 0.005\nbeta1 = 0.9\nbeta2 = 0.999\nepsilon = 1e-8\nweight_decay = 0.01\nw_new, m_new, v_new = adamw_update(w, g, m, v, t, lr, beta1, beta2, epsilon, weight_decay)\nprint(np.round(w_new, 4))",
      "expected_output": "[ 1.5049, -1.0049]"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "import numpy as np\nw = np.array([1.0, 2.0])\ng = np.array([0.1, -0.2])\nm = np.zeros(2)\nv = np.zeros(2)\nt = 1\nlr = 0.01\nbeta1 = 0.9\nbeta2 = 0.999\nepsilon = 1e-8\nweight_decay = 0.1\nw_new, m_new, v_new = adamw_update(w, g, m, v, t, lr, beta1, beta2, epsilon, weight_decay)\nprint(np.round(w_new, 4))",
    "output": "[0.989 2.008]",
    "reasoning": "After applying AdamW update, the weights are moved in the negative gradient direction and decayed by 1%. The result is [0.989, 2.001]."
  },
  "category": "Optimization",
  "starter_code": "import numpy as np\n\ndef adamw_update(w, g, m, v, t, lr, beta1, beta2, epsilon, weight_decay):\n    \"\"\"\n    Perform one AdamW optimizer step.\n    Args:\n      w: parameter vector (np.ndarray)\n      g: gradient vector (np.ndarray)\n      m: first moment vector (np.ndarray)\n      v: second moment vector (np.ndarray)\n      t: integer, current time step\n      lr: float, learning rate\n      beta1: float, beta1 parameter\n      beta2: float, beta2 parameter\n      epsilon: float, small constant\n      weight_decay: float, weight decay coefficient\n    Returns:\n      w_new, m_new, v_new\n    \"\"\"\n    # Your code here\n    pass",
  "title": "Implement AdamW Optimizer Step",
  "learn_section": "IyAqKkFkYW1XIE9wdGltaXplcioqCgpBZGFtVyBpcyBhIHZhcmlhbnQgb2YgdGhlIEFkYW0gb3B0aW1pemVyIHRoYXQgZGVjb3VwbGVzIHdlaWdodCBkZWNheSBmcm9tIHRoZSBncmFkaWVudC1iYXNlZCB1cGRhdGUsIHByb3ZpZGluZyBiZXR0ZXIgcmVndWxhcml6YXRpb24gYW5kIGdlbmVyYWxpemF0aW9uLCBlc3BlY2lhbGx5IGZvciB0cmFpbmluZyBkZWVwIG5ldXJhbCBuZXR3b3Jrcy4KCi0gKipBZGFtOioqIEFkYW0gY29tYmluZXMgbW9tZW50dW0gKG1vdmluZyBhdmVyYWdlIG9mIGdyYWRpZW50cykgYW5kIFJNU3Byb3AgKG1vdmluZyBhdmVyYWdlIG9mIHNxdWFyZWQgZ3JhZGllbnRzKSBmb3IgYWRhcHRpdmUgbGVhcm5pbmcgcmF0ZXMuCi0gKipXZWlnaHQgRGVjYXk6KiogUmVndWxhcml6ZXMgYnkgcGVuYWxpemluZyBsYXJnZSB3ZWlnaHRzLiBJbiBjbGFzc2ljIEFkYW0sIHdlaWdodCBkZWNheSBpcyBpbXBsZW1lbnRlZCBhcyAkTF8yJCByZWd1bGFyaXphdGlvbiAoYWRkZWQgdG8gdGhlIGdyYWRpZW50KS4gSW4gQWRhbVcsIHdlaWdodCBkZWNheSBpcyBhcHBsaWVkICoqZGlyZWN0bHkqKiB0byB0aGUgd2VpZ2h0cywgZGVjb3VwbGVkIGZyb20gdGhlIGdyYWRpZW50IHVwZGF0ZS4KCioqQWRhbVcgVXBkYXRlIFJ1bGU6KioKCjEuIFVwZGF0ZSB0aGUgbW92aW5nIGF2ZXJhZ2VzIChmaXJzdCBtb21lbnQgJG0kIGFuZCBzZWNvbmQgbW9tZW50ICR2JCk6CiAgICQkbV90ID0gXGJldGFfMSBtX3t0LTF9ICsgKDEgLSBcYmV0YV8xKSBnX3QkJAogICAkJHZfdCA9IFxiZXRhXzIgdl97dC0xfSArICgxIC0gXGJldGFfMikgZ190XjIkJAoyLiBCaWFzIGNvcnJlY3Rpb246CiAgICQkXGhhdHttfV90ID0gbV90IC8gKDEgLSBcYmV0YV8xXnQpJCQKICAgJCRcaGF0e3Z9X3QgPSB2X3QgLyAoMSAtIFxiZXRhXzJedCkkJAozLiBQYXJhbWV0ZXIgdXBkYXRlIHdpdGggZGVjb3VwbGVkIHdlaWdodCBkZWNheToKICAgJCR3IFxsZWZ0YXJyb3cgdyAtIFxldGEgXGNkb3QgXHRleHR7d2VpZ2h0XF9kZWNheX0gXGNkb3QgdyQkCiAgICQkdyBcbGVmdGFycm93IHcgLSBcZXRhIFxjZG90IFxmcmFje1xoYXR7bX1fdH17XHNxcnR7XGhhdHt2fV90fSArIFxlcHNpbG9ufSQkCgpUaGlzIGRlY291cGxlZCB1cGRhdGUgaXMgY3J1Y2lhbCBmb3IgY29ycmVjdCByZWd1bGFyaXphdGlvbiBpbiBtb2Rlcm4gZGVlcCBsZWFybmluZy4KCioqUmVmZXJlbmNlczoqKgotIFtMb3NoY2hpbG92ICYgSHV0dGVyLCAyMDE5LiBEZWNvdXBsZWQgV2VpZ2h0IERlY2F5IFJlZ3VsYXJpemF0aW9uIChBZGFtVyldKGh0dHBzOi8vYXJ4aXYub3JnL2Ficy8xNzExLjA1MTAxKQoKQWRhbVcgaXMgbm93IHRoZSBkZWZhdWx0IG9wdGltaXplciBpbiBtYW55IGRlZXAgbGVhcm5pbmcgZnJhbWV3b3JrcyAoZS5nLiwgUHlUb3JjaCku",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "description_decoded": "Implement a single update step of the AdamW optimizer for a parameter vector $w$ and its gradients $g$. AdamW is a variant of the Adam optimizer that decouples weight decay from the gradient update, leading to better generalization.\n\n**Your Task:**\nWrite a function `adamw_update(w, g, m, v, t, lr, beta1, beta2, epsilon, weight_decay)` that performs one update step for the parameter vector $w$ with its gradient $g$ using AdamW. The function should:\n- Update the first moment $m$ and second moment $v$ (moving averages of gradients and squared gradients)\n- Apply bias correction for $m$ and $v$\n- Apply the AdamW update rule (with decoupled weight decay)\n- Return the updated parameter vector and the new values of $m$ and $v$\n\n**Arguments:**\n- `w`: NumPy array, current parameter vector\n- `g`: NumPy array, gradient vector (same shape as `w`)\n- `m`: NumPy array, first moment vector (same shape as `w`)\n- `v`: NumPy array, second moment vector (same shape as `w`)\n- `t`: Integer, current time step (starting from 1)\n- `lr`: Learning rate (float)\n- `beta1`: Decay rate for the first moment (float)\n- `beta2`: Decay rate for the second moment (float)\n- `epsilon`: Small constant for numerical stability (float)\n- `weight_decay`: Weight decay coefficient (float)\n\n",
  "learn_section_decoded": "# **AdamW Optimizer**\n\nAdamW is a variant of the Adam optimizer that decouples weight decay from the gradient-based update, providing better regularization and generalization, especially for training deep neural networks.\n\n- **Adam:** Adam combines momentum (moving average of gradients) and RMSprop (moving average of squared gradients) for adaptive learning rates.\n- **Weight Decay:** Regularizes by penalizing large weights. In classic Adam, weight decay is implemented as $L_2$ regularization (added to the gradient). In AdamW, weight decay is applied **directly** to the weights, decoupled from the gradient update.\n\n**AdamW Update Rule:**\n\n1. Update the moving averages (first moment $m$ and second moment $v$):\n   $$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n   $$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n2. Bias correction:\n   $$\\hat{m}_t = m_t / (1 - \\beta_1^t)$$\n   $$\\hat{v}_t = v_t / (1 - \\beta_2^t)$$\n3. Parameter update with decoupled weight decay:\n   $$w \\leftarrow w - \\eta \\cdot \\text{weight\\_decay} \\cdot w$$\n   $$w \\leftarrow w - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n\nThis decoupled update is crucial for correct regularization in modern deep learning.\n\n**References:**\n- [Loshchilov & Hutter, 2019. Decoupled Weight Decay Regularization (AdamW)](https://arxiv.org/abs/1711.05101)\n\nAdamW is now the default optimizer in many deep learning frameworks (e.g., PyTorch)."
}