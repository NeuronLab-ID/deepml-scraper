{
  "description": "SW1wbGVtZW50IGF0dGVudGlvbiBoZWFkIHBydW5pbmcgZm9yIHRyYW5zZm9ybWVyIG1vZGVscyB0byByZWR1Y2UgaW5mZXJlbmNlIGNvc3QuIEdpdmVuIGF0dGVudGlvbiB3ZWlnaHRzIGZyb20gYWxsIGhlYWRzLCBpbXBvcnRhbmNlIHNjb3JlcywgYW5kIHBydW5pbmcgcmF0aW8sIHJlbW92ZSB0aGUgbGVhc3QgaW1wb3J0YW50IGhlYWRzIHdoaWxlIG1haW50YWluaW5nIG1vZGVsIHBlcmZvcm1hbmNlLiBIZWFkIHBydW5pbmcgZXhwbG9pdHMgcmVkdW5kYW5jeSBpbiB0cmFuc2Zvcm1lcnMgLSByZXNlYXJjaCBzaG93cyA1MCUgb2YgQkVSVCBoZWFkcyBjYW4gYmUgcmVtb3ZlZCB3aXRoIDwxJSBhY2N1cmFjeSBsb3NzLiBSZXR1cm5zIHBydW5lZCBhdHRlbnRpb24gd2VpZ2h0cyBhbmQgaW5kaWNlcyBvZiBrZXB0IGhlYWRzLiBVc2VkIGluIERpc3RpbEJFUlQsIFRpbnlCRVJULCBhbmQgcHJvZHVjdGlvbiBzZXJ2aW5nLg==",
  "id": "233",
  "test_cases": [
    {
      "test": "np.random.seed(42); attn = np.random.rand(4, 3, 3); importance = np.array([0.8, 0.3, 0.9, 0.2]); pruned, kept = prune_attention_heads(attn, importance, 0.5); print(kept)",
      "expected_output": "[0, 2]"
    },
    {
      "test": "np.random.seed(42); attn = np.random.rand(8, 4, 4); importance = np.array([0.5, 0.9, 0.1, 0.8, 0.6, 0.2, 0.7, 0.3]); pruned, kept = prune_attention_heads(attn, importance, 0.75); print(kept)",
      "expected_output": "[1, 3]"
    }
  ],
  "difficulty": "medium",
  "pytorch_difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "attn=(4,3,3), importance=[0.8,0.3,0.9,0.2], ratio=0.5",
    "output": "([pruned_2x3x3], [0, 2])",
    "reasoning": "Keep 4*(1-0.5)=2 heads. Rank by importance: head 2 (0.9), head 0 (0.8), head 1 (0.3), head 3 (0.2). Keep top 2: indices [0,2] (sorted). Prune heads 1 and 3. Shape (4,3,3)→(2,3,3)."
  },
  "category": "Deep Learning",
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpkZWYgcHJ1bmVfYXR0ZW50aW9uX2hlYWRzX3B5dG9yY2goCiAgICBhdHRlbnRpb25fd2VpZ2h0czogdG9yY2guVGVuc29yLAogICAgaGVhZF9pbXBvcnRhbmNlX3Njb3JlczogdG9yY2guVGVuc29yLAogICAgcHJ1bmluZ19yYXRpbzogZmxvYXQKKSAtPiB0dXBsZVt0b3JjaC5UZW5zb3IsIGxpc3RbaW50XV06CgkiIiIKCVBydW5lIGF0dGVudGlvbiBoZWFkcyB1c2luZyBQeVRvcmNoLgoJCglBcmdzOgoJCWF0dGVudGlvbl93ZWlnaHRzOiBTaGFwZSAobnVtX2hlYWRzLCBzZXFfbGVuLCBzZXFfbGVuKQoJCWhlYWRfaW1wb3J0YW5jZV9zY29yZXM6IFNoYXBlIChudW1faGVhZHMsKQoJCXBydW5pbmdfcmF0aW86IEZyYWN0aW9uIHRvIHBydW5lICgwLjAgdG8gMS4wKQoJCglSZXR1cm5zOgoJCVR1cGxlIG9mIChwcnVuZWRfYXR0ZW50aW9uX3dlaWdodHMsIGtlcHRfaGVhZF9pbmRpY2VzKQoJIiIiCgkjIFlvdXIgY29kZSBoZXJlCglwYXNz",
  "title": "Inference Head Pruning for Transformers",
  "starter_code": "import numpy as np\n\ndef prune_attention_heads(\n    attention_weights: np.ndarray,\n    head_importance_scores: np.ndarray,\n    pruning_ratio: float\n) -> tuple[np.ndarray, list[int]]:\n\t\"\"\"\n\tPrune less important attention heads to reduce inference cost.\n\t\n\tTransformer models have redundant heads. Research shows 50% of\n\tBERT heads can be removed with <1% accuracy loss.\n\t\n\tArgs:\n\t\tattention_weights: Attention weights from all heads\n\t\t  Shape: (num_heads, seq_len, seq_len)\n\t\thead_importance_scores: Importance score per head\n\t\t  Shape: (num_heads,)\n\t\t  Higher score = more important\n\t\tpruning_ratio: Fraction of heads to prune\n\t\t  Range: 0.0 (keep all) to 1.0 (prune all)\n\t\n\tReturns:\n\t\tTuple of (pruned_attention_weights, kept_head_indices):\n\t\t- pruned_attention_weights: Reduced attention matrices\n\t\t- kept_head_indices: List of preserved head indices\n\t\"\"\"\n\t# Your code here\n\tpass",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "torch.manual_seed(42); attn = torch.rand(4, 3, 3); importance = torch.tensor([0.8, 0.3, 0.9, 0.2]); pruned, kept = prune_attention_heads_pytorch(attn, importance, 0.5); print(kept)",
      "expected_output": "[0, 2]"
    },
    {
      "test": "torch.manual_seed(42); attn = torch.rand(8, 4, 4); importance = torch.tensor([0.5, 0.9, 0.1, 0.8, 0.6, 0.2, 0.7, 0.3]); pruned, kept = prune_attention_heads_pytorch(attn, importance, 0.75); print(kept)",
      "expected_output": "[1, 3]"
    },
    {
      "test": "torch.manual_seed(42); attn = torch.rand(6, 2, 2); importance = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6]); pruned, kept = prune_attention_heads_pytorch(attn, importance, 0.0); print(kept)",
      "expected_output": "[0, 1, 2, 3, 4, 5]"
    },
    {
      "test": "torch.manual_seed(42); attn = torch.rand(8, 5, 5); importance = torch.tensor([0.2, 0.95, 0.3, 0.1, 0.4, 0.5, 0.6, 0.7]); pruned, kept = prune_attention_heads_pytorch(attn, importance, 0.875); print(kept)",
      "expected_output": "[1]"
    },
    {
      "test": "torch.manual_seed(42); attn = torch.rand(12, 8, 8); importance = torch.tensor([0.4, 0.1, 0.6, 0.8, 0.3, 0.9, 0.2, 0.7, 0.5, 0.95, 0.15, 0.85]); pruned, kept = prune_attention_heads_pytorch(attn, importance, 0.5); print(pruned.shape)",
      "expected_output": "torch.Size([6, 8, 8])"
    }
  ],
  "createdAt": "December 11, 2025 at 3:13:49 PM UTC-0500",
  "learn_section": "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",
  "description_decoded": "Implement attention head pruning for transformer models to reduce inference cost. Given attention weights from all heads, importance scores, and pruning ratio, remove the least important heads while maintaining model performance. Head pruning exploits redundancy in transformers - research shows 50% of BERT heads can be removed with <1% accuracy loss. Returns pruned attention weights and indices of kept heads. Used in DistilBERT, TinyBERT, and production serving.",
  "learn_section_decoded": "**Inference Head Pruning for Transformers**\n\nAttention head pruning reduces computational cost by removing less important heads from transformer models while maintaining performance.\n\n**The Redundancy Problem**\n\nResearch findings (Voita et al., 2019; Michel et al., 2019):\n- BERT-base has 144 heads (12 layers * 12 heads)\n- Can prune 50% of heads with less than 1% accuracy loss\n- Many heads learn similar patterns\n- Some heads contribute minimally\n\nRedundancy exists due to over-parameterization, overlapping specialization, and robustness during training.\n\n**Head Pruning Algorithm**\n\nGiven: attention weights (H, L, L), importance scores (H,), pruning ratio r\n\nStep 1: Calculate heads to keep: k = H * (1 - r)\nStep 2: Sort heads by importance (descending)\nStep 3: Select top-k heads\nStep 4: Return pruned weights and kept indices\n\n**Example**\n\n4 heads with importance [0.8, 0.3, 0.9, 0.2], prune 50%\n\nHeads to keep: 4 * (1 - 0.5) = 2\nRanking: Head 2 (0.9), Head 0 (0.8), Head 1 (0.3), Head 3 (0.2)\nKeep top 2: indices [0, 2] (sorted)\nResult: Shape (4, L, L) becomes (2, L, L), 2x speedup\n\n**Measuring Head Importance**\n\n**Gradient-based** (most common): Compute gradients with respect to attention outputs during validation. Importance = mean absolute gradient. Higher gradient means more influence.\n\n**Attention entropy**: Measure how focused patterns are. Low entropy (focused) suggests importance. High entropy (dispersed) suggests less structure.\n\n**Ablation study** (most accurate but expensive): Remove each head and measure accuracy drop. Larger drop indicates more importance.\n\n**Computational Benefits**\n\nFLOPs for multi-head attention: H * L^2 * d_h\n\nExample with BERT-base, sequence length 128:\n- Original: 12 heads * 128^2 * 64 = 12.6M FLOPs\n- Pruned 50%: 6 heads * 128^2 * 64 = 6.3M FLOPs\n- Speedup: 2.0x\n\nMemory for attention weights: H * L^2 * 4 bytes\n- Original: 12 * 128^2 * 4 = 768 KB\n- Pruned 50%: 6 * 128^2 * 4 = 384 KB\n- Reduction: 50%\n\n**Accuracy vs Speedup Trade-off**\n\nPruning 0%: 0% accuracy loss, 1.0x speed, baseline\nPruning 25%: 0.1% accuracy loss, 1.15x speed\nPruning 50%: 0.8% accuracy loss, 1.5x speed (sweet spot)\nPruning 75%: 2.5% accuracy loss, 2.0x speed\nPruning 90%: 8.0% accuracy loss, 3.0x speed\n\nSweet spot is 50% pruning: 1.5x faster with minimal accuracy loss.\n\n**Head Specialization**\n\nDifferent heads learn different patterns:\n\nPosition heads: Attend to specific tokens like [CLS] or [SEP], important for classification\nSyntactic heads: Capture grammar structure, critical for linguistic understanding\nSemantic heads: Focus on meaning relationships, important for reasoning tasks\nRare heads: Specialized for edge cases, often pruned without major loss\n\n**Pruning Strategies**\n\n**Uniform**: Same ratio across all layers, simple and effective\n\n**Layer-wise**: Different ratios per layer, keep more in critical layers (early and late), prune more from middle\n\n**Task-specific**: Adapt to task needs - QA keeps reasoning heads, NER keeps entity pattern heads\n\n**Pruning Pipeline**\n\nPhase 1 - Calibration: Measure importance on validation data (minutes to hours)\nPhase 2 - Selection: Choose pruning ratio and select top-k heads (seconds)\nPhase 3 - Modification: Remove pruned heads, optionally fine-tune (minutes)\nPhase 4 - Deployment: Export and validate (minutes)\n\n**Real-World Applications**\n\nDistilBERT: Uses head pruning, 40% fewer parameters, 60% faster, 97% of BERT performance\n\nTinyBERT: Combines head and layer pruning, 7.5x smaller, 9x faster, 96% of BERT performance\n\nProduction serving: Google and Meta prune 30-50% of heads, significantly reducing inference cost\n\n**Comparison with Other Methods**\n\nHead Pruning: Fast, simple, no retraining needed. Speedup 1.5-2x with 50% pruning. Limited to attention computation.\n\nQuantization: Reduces memory and computation, combines well with pruning. Speedup 2-4x. May need specialized hardware.\n\nKnowledge Distillation: Can compress entire model but requires full retraining. Speedup 2-10x depending on student size.\n\nLayer Pruning: Removes entire layers, larger speedup potential but higher accuracy loss. Speedup 1.5-3x.\n\n**Implementation Notes**\n\nReturn pruned attention weights (reduced matrices) and kept head indices (for reproducibility).\n\nAlways return sorted indices for consistency and easier debugging.\n\nEdge cases: Pruning ratio 0.0 keeps all heads, ratio 1.0 should keep at least 1 head.\n\nIntegration requires modifying attention layer to use only kept heads and updating projection dimensions.\n\n**Common Pitfalls**\n\nPruning without calibration: Results in high accuracy loss. Always measure importance on validation data.\n\nToo aggressive pruning: Causes significant drops. Start with 25-50% and tune gradually.\n\nNot fine-tuning: Suboptimal performance. Fine-tune for 1-2 epochs after pruning.\n\nUniform across layers: May lose critical capabilities. Use layer-wise importance when possible.\n\n**Summary**\n\nHead pruning removes less important attention heads based on importance scores. Process: measure importance, rank heads, keep top-k, remove pruned heads. Typical result: 1.5-2x faster inference with less than 1% accuracy loss at 50% pruning. Used in DistilBERT, TinyBERT, and production serving."
}