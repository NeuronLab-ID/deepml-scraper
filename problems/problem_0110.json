{
  "description": "RGV2ZWxvcCBhIGZ1bmN0aW9uIHRvIGNvbXB1dGUgdGhlIE1FVEVPUiBzY29yZSBmb3IgZXZhbHVhdGluZyBtYWNoaW5lIHRyYW5zbGF0aW9uIHF1YWxpdHkuIEdpdmVuIGEgcmVmZXJlbmNlIHRyYW5zbGF0aW9uIGFuZCBhIGNhbmRpZGF0ZSB0cmFuc2xhdGlvbiwgY2FsY3VsYXRlIHRoZSBzY29yZSBiYXNlZCBvbiB1bmlncmFtIG1hdGNoZXMsIHByZWNpc2lvbiwgcmVjYWxsLCBGLW1lYW4sIGFuZCBhIHBlbmFsdHkgZm9yIHdvcmQgb3JkZXIgZnJhZ21lbnRhdGlvbi4=",
  "id": "110",
  "test_cases": [
    {
      "test": "print(round(meteor_score('The dog barks at the moon', 'The dog barks at the moon'),3))",
      "expected_output": "0.998"
    },
    {
      "test": "print(round(meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky'),3))",
      "expected_output": "0.625"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky')",
    "output": "0.625",
    "reasoning": "The function identifies 4 unigram matches ('rain', 'gently'/'gentle', 'from', 'sky'), computes precision (4/6) and recall (4/5), calculates an F-mean, and then apply a small penalty for two chunks."
  },
  "category": "NLP",
  "starter_code": "import numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\t# Your code here\n\tpass",
  "title": "Evaluate Translation Quality with METEOR Score",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/saitiger",
      "name": "saitiger"
    }
  ],
  "description_decoded": "Develop a function to compute the METEOR score for evaluating machine translation quality. Given a reference translation and a candidate translation, calculate the score based on unigram matches, precision, recall, F-mean, and a penalty for word order fragmentation.",
  "learn_section_decoded": "METEOR(Metric for Evaluation of Translation with Explicit ORdering) is a metric generally used for \nmachine translation and evaluating the text output of generative AI models. METEOR build was introduced to address \nthe limitations in earlier metrics like BLEU.\n\n## Key Characteristics\n- Considers semantic similarity beyond exact word matching\n- Accounts for word order and translation variations\n- Provides more human-aligned translation assessment\n\n# Implementation \n1. **Tokenization**\n\n2. **Frequency of matching words** : Matching needs to be exact\n\n3. **Calculate Precision, Recall and F-mean**\n```\n   F_mean = (Precision * Recall) / \n   (alpha * Precision + (1 - alpha) * Recall)\n```\n   - alpha typically set to 0.9\n   - Balances precision and recall\n\n4. **Fragmentation Penalty**\n   ```\n   Chunks = Count of contiguous matched word sequences\n   Penalty = gamma * (Chunks / Matches)^β\n   ```\n   - beta controls penalty weight (typically 3)\n   - gamma limits maximum penalty (typically 0.5)\n\n5. **Final METEOR Score**\n   ```\n   METEOR = F_mean * (1 - Penalty)\n   ```\n   - Ranges from 0 (no match) to 1 (perfect match)\n\n**__Note__** : The [paper](https://aclanthology.org/W05-0909/) that introduced the metric doesn't have the parameters (alpha,β, and gamma) as tunable parameters, but implementation in other libraries like NLTK offers this flexibility.\n\n# Example \n\n- Reference: \"The quick brown fox jumps over the lazy dog\"\n- Candidate: \"A quick brown fox jumps over a lazy dog\"\n\n### 1. Tokenization\n- Reference Tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n- Candidate Tokens: ['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'a', 'lazy', 'dog']\n\n### 2. Unigram Matching\n- Matching tokens: ['quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog']\n- Matches: 7\n\n### 3. Unigram Precision and Recall Calculation\n- Precision = Matches / Candidate Length = 7 / 9 ~ 0.778\n\n- Recall = Matches / Reference Length = 7 / 9 ~ 0.778\n\n### 4. F-mean Calculation (alpha = 0.9)\n```\nF_mean = (Precision * Recall) / \n(alpha * Precision + (1 - alpha) * Recall)\n       = (0.778 * 0.778) / (0.9 * 0.778 + (1 - 0.9) * 0.778)\n       = 0.606 / (0.7 + 0.078)\n       = 0.606 / 0.778\n       ≈ 0.779\n```\n\n### 5. Chunk Calculation\n- Contiguous matched sequences:\n  1. ['quick', 'brown', 'fox']\n  2. ['jumps', 'over']\n  3. ['lazy', 'dog']\n- Number of Chunks: 3\n- Total Number of Unigram Matches: 7\n\n### 6. Penalty Calculation (betta = 3, gamma = 0.5)\n```\nPenalty = gamma * \n(Number of Chunks / Total Number of Unigram Matches)^betta\n        = 0.5 * (3 / 7)^3\n        = 0.5 * (0.429)^3\n        ≈ 0.039\n```\n\n### 7. Final METEOR Score\n```\nMETEOR = F_mean * (1 - Penalty)\n       = 0.779 * (1 - 0.039)\n       = 0.779 * 0.961\n       ≈ 0.749\n```"
}