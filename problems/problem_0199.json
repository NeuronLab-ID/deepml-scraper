{
  "description": "SW1wbGVtZW50IGFuIGVhcmx5IHN0b3BwaW5nIGZ1bmN0aW9uIHRoYXQgdGFrZXMgYSBsaXN0IG9mIHZhbGlkYXRpb24gbG9zc2VzIGFuZCBkZXRlcm1pbmVzIGF0IGVhY2ggZXBvY2ggd2hldGhlciB0cmFpbmluZyBzaG91bGQgc3RvcC4gVGhlIGZ1bmN0aW9uIHNob3VsZCB0cmFjayB3aGV0aGVyIHRoZSB2YWxpZGF0aW9uIGxvc3MgaGFzIGltcHJvdmVkIGJ5IGF0IGxlYXN0IGEgbWluaW11bSBkZWx0YSB0aHJlc2hvbGQuIElmIHRoZSBsb3NzIGRvZXMgbm90IGltcHJvdmUgZm9yIGEgc3BlY2lmaWVkIG51bWJlciBvZiBjb25zZWN1dGl2ZSBlcG9jaHMgKHBhdGllbmNlKSwgaXQgc2hvdWxkIHNpZ25hbCB0aGF0IHRyYWluaW5nIHNob3VsZCBzdG9wIGF0IHRoYXQgcG9pbnQuIFJldHVybiBhIGxpc3Qgb2YgYm9vbGVhbiB2YWx1ZXMsIG9uZSBmb3IgZWFjaCBlcG9jaCwgaW5kaWNhdGluZyB3aGV0aGVyIHRyYWluaW5nIHNob3VsZCBzdG9wLg==",
  "id": "199",
  "test_cases": [
    {
      "test": "print(early_stopping([0.5, 0.4, 0.3, 0.2], patience=3, min_delta=0.01))",
      "expected_output": "[False, False, False, False]"
    },
    {
      "test": "print(early_stopping([0.5, 0.4, 0.39, 0.39, 0.39], patience=3, min_delta=0.01))",
      "expected_output": "[False, False, False, False, True]"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "val_losses=[0.5, 0.4, 0.39, 0.39, 0.39], patience=3, min_delta=0.01",
    "output": "[False, False, False, False, True]",
    "reasoning": "Epoch 0: First loss (0.5) is recorded as best, return False. Epoch 1: Loss improves to 0.4 (improvement ≥ 0.01), best updated to 0.4, counter=0, return False. Epoch 2: Loss 0.39 improves by only 0.01 (0.39 not < 0.39), counter becomes 1, return False. Epoch 3: Loss stays at 0.39, counter becomes 2, return False. Epoch 4: Loss stays at 0.39, counter becomes 3. Since counter (3) ≥ patience (3), return True."
  },
  "category": "Machine Learning",
  "starter_code": "def early_stopping(val_losses: list[float], patience: int = 5, min_delta: float = 0.0) -> list[bool]:\n\t\"\"\"\n\tDetermine at each epoch whether training should stop based on validation loss.\n\t\n\tArgs:\n\t\tval_losses: List of validation losses at each epoch\n\t\tpatience: Number of epochs to wait for improvement before stopping\n\t\tmin_delta: Minimum change in validation loss to qualify as improvement\n\t\n\tReturns:\n\t\tList of booleans indicating whether to stop at each epoch\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Early Stopping Based on Validation Loss Plateau",
  "createdAt": "November 9, 2025 at 11:10:01 PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "learn_section": "### Understanding Early Stopping

Early stopping is a form of regularization used to prevent overfitting during neural network training. Instead of training for a fixed number of epochs, early stopping monitors the model's performance on a validation set and stops training when performance stops improving.

#### The Problem: Overfitting During Training

As neural networks train, they initially learn generalizable patterns from the data. However, with continued training, they may begin to memorize noise and specific details of the training set. This manifests as:

- Training loss continuously decreasing
- Validation loss initially decreasing, then plateauing or increasing
- Growing gap between training and validation performance
- Poor generalization to new, unseen data

Early stopping addresses this by halting training at the point where validation performance is optimal, before overfitting occurs.

#### Mathematical Framework

**Best Loss Tracking**

At each epoch $t$, we maintain a record of the best (minimum) validation loss observed:

$$
L_{best}(t) = \min_{i=1}^{t} L_{val}(i)
$$

Where $L_{val}(i)$ represents the validation loss at epoch $i$.

**Improvement Criterion with Minimum Delta**

To qualify as a meaningful improvement, the new validation loss must be strictly better than the best loss by at least a threshold $\delta$ (min_delta):

$$
\text{Improvement at epoch } t: \quad L_{val}(t) < L_{best}(t-1) - \delta
$$

The minimum delta $\delta$ serves to filter out insignificant fluctuations that don't represent true improvement. Without this threshold, tiny random variations in loss could incorrectly reset the stopping criterion.

**Patience Counter**

We maintain a counter $c(t)$ that tracks consecutive epochs without significant improvement:

$$
c(t) = \begin{cases}
0 & \text{if } L_{val}(t) < L_{best}(t-1) - \delta \\
c(t-1) + 1 & \text{otherwise}
\end{cases}
$$

This counter resets to zero whenever we observe a meaningful improvement, but increments when the loss fails to improve by at least $\delta$.

**Stopping Criterion**

Training should stop at epoch $t$ when:

$$
c(t) \geq P
$$

Where $P$ is the patience parameter, representing how many consecutive non-improving epochs we tolerate before stopping.

#### Key Parameters and Their Effects

**Patience ($P$)**

The patience parameter controls how long we wait for improvement:

- **Low patience** (e.g., 2-3 epochs): Stops training quickly, may terminate too early if validation loss is noisy
- **High patience** (e.g., 15-20 epochs): Allows more time for improvement, but may permit some overfitting
- **Typical values**: 5-10 epochs for most applications

Mathematically, patience represents the maximum allowable value of consecutive non-improving steps:

$$
P = \max \{ k : L_{val}(t+i) \geq L_{best}(t) - \delta, \forall i \in [1,k] \}
$$

**Minimum Delta ($\delta$)**

The minimum delta threshold determines what constitutes "improvement":

- **$\delta = 0$**: Any decrease in loss counts as improvement (sensitive to noise)
- **$\delta > 0$**: Loss must decrease by at least $\delta$ to count (more robust)
- **Typical values**: 0.001 to 0.01 for normalized losses, scaled appropriately for unnormalized losses

For relative improvements, $\delta$ can be defined as a fraction of the current best loss:

$$
\delta_{relative} = \epsilon \cdot L_{best}
$$

Where $\epsilon$ is a small percentage (e.g., 0.01 for 1% improvement).

#### Conceptual Example

Consider training with patience $P = 3$ and min_delta $\delta = 0.01$:

Suppose we observe validation losses: $[0.5, 0.4, 0.39, 0.39, 0.39]$

- **Epoch 0**: First epoch, initialize $L_{best} = 0.5$, counter $c = 0$
- **Epoch 1**: Loss $0.4 < 0.5 - 0.01 = 0.49$ → Improvement! Update $L_{best} = 0.4$, reset $c = 0$
- **Epoch 2**: Loss $0.39 \not< 0.4 - 0.01 = 0.39$ → No improvement (exactly at threshold), increment $c = 1$
- **Epoch 3**: Loss $0.39 \not< 0.39$ → No improvement, increment $c = 2$
- **Epoch 4**: Loss $0.39 \not< 0.39$ → No improvement, increment $c = 3$

Since $c = 3 \geq P = 3$, we signal to stop training at epoch 4.

#### Why This Works: The Bias-Variance Tradeoff

Early stopping implicitly regularizes by limiting model complexity through training time:

$$
\text{Generalization Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
$$

As training progresses:
- **Bias** decreases (model fits training data better)
- **Variance** increases (model becomes more sensitive to training data specifics)

Early stopping finds the optimal point where:

$$
t^* = \arg\min_{t} \mathbb{E}[L_{test}(t)]
$$

This typically occurs before the model has fully minimized training loss, preventing the variance component from dominating.

#### Relationship to L2 Regularization

Interestingly, early stopping has a mathematical connection to L2 regularization. For simple linear models with gradient descent, early stopping approximates the effect of L2 regularization:

$$
w(t) \approx w_{L2} \quad \text{where} \quad w_{L2} = \arg\min_w \left[ L(w) + \lambda \|w\|^2 \right]
$$

The relationship between training steps and regularization strength is:

$$
\lambda \propto \frac{1}{t}
$$

Thus, stopping early is equivalent to stronger regularization, while training longer is equivalent to weaker regularization.

#### Practical Considerations

**Validation Set Size**: The reliability of early stopping depends on having a sufficiently large validation set to provide stable loss estimates. With small validation sets, consider:
- Increasing patience to account for noise
- Using k-fold cross-validation
- Averaging over multiple training runs

**Loss Trajectory Patterns**: Different loss trajectories require different patience values:
- **Smooth decreasing**: Lower patience works well
- **Noisy/fluctuating**: Higher patience needed
- **Sudden drops**: May need to track both recent and global best

**Learning Rate Schedules**: Early stopping interacts with learning rate schedules:
- Sudden learning rate reductions may appear as loss plateaus
- Consider resetting patience counter after scheduled learning rate changes
- Or increase patience when using aggressive learning rate decay

#### When to Use Early Stopping

Early stopping is particularly effective for:

1. **Deep networks**: Where overfitting is likely and optimal epoch count is unknown
2. **Limited data**: When validation set is small but representative
3. **Computational constraints**: Automatically finding good stopping point without extensive hyperparameter search
4. **High-capacity models**: Where model can easily memorize training data

Be cautious with:
- Very small validation sets (unreliable stopping signals)
- Highly stochastic training (noisy loss curves)
- Models that require long training to converge (may stop too early)

#### Extensions and Variations

**Multiple Metrics**: Instead of just validation loss, monitor:

$$
\text{Stop when} \quad c_{loss}(t) \geq P_{loss} \quad \text{OR} \quad c_{metric}(t) \geq P_{metric}
$$

**Restore Best Weights**: Save model parameters at best validation loss and restore them after stopping.

**Delayed Start**: Don't begin monitoring until after a warm-up period to allow initial training dynamics to stabilize.

Early stopping is a simple yet powerful technique that provides automatic regularization, making it one of the most widely used methods in deep learning practice.",
  "description_decoded": "Implement an early stopping function that takes a list of validation losses and determines at each epoch whether training should stop. The function should track whether the validation loss has improved by at least a minimum delta threshold. If the loss does not improve for a specified number of consecutive epochs (patience), it should signal that training should stop at that point. Return a list of boolean values, one for each epoch, indicating whether training should stop.",
  "learn_section_decoded": "### Understanding Early Stopping\n\nEarly stopping is a form of regularization used to prevent overfitting during neural network training. Instead of training for a fixed number of epochs, early stopping monitors the model's performance on a validation set and stops training when performance stops improving.\n\n#### The Problem: Overfitting During Training\n\nAs neural networks train, they initially learn generalizable patterns from the data. However, with continued training, they may begin to memorize noise and specific details of the training set. This manifests as:\n\n- Training loss continuously decreasing\n- Validation loss initially decreasing, then plateauing or increasing\n- Growing gap between training and validation performance\n- Poor generalization to new, unseen data\n\nEarly stopping addresses this by halting training at the point where validation performance is optimal, before overfitting occurs.\n\n#### Mathematical Framework\n\n**Best Loss Tracking**\n\nAt each epoch $t$, we maintain a record of the best (minimum) validation loss observed:\n\n$$\nL_{best}(t) = \\min_{i=1}^{t} L_{val}(i)\n$$\n\nWhere $L_{val}(i)$ represents the validation loss at epoch $i$.\n\n**Improvement Criterion with Minimum Delta**\n\nTo qualify as a meaningful improvement, the new validation loss must be strictly better than the best loss by at least a threshold $\\delta$ (min_delta):\n\n$$\n\\text{Improvement at epoch } t: \\quad L_{val}(t) < L_{best}(t-1) - \\delta\n$$\n\nThe minimum delta $\\delta$ serves to filter out insignificant fluctuations that don't represent true improvement. Without this threshold, tiny random variations in loss could incorrectly reset the stopping criterion.\n\n**Patience Counter**\n\nWe maintain a counter $c(t)$ that tracks consecutive epochs without significant improvement:\n\n$$\nc(t) = \\begin{cases}\n0 & \\text{if } L_{val}(t) < L_{best}(t-1) - \\delta \\\\\nc(t-1) + 1 & \\text{otherwise}\n\\end{cases}\n$$\n\nThis counter resets to zero whenever we observe a meaningful improvement, but increments when the loss fails to improve by at least $\\delta$.\n\n**Stopping Criterion**\n\nTraining should stop at epoch $t$ when:\n\n$$\nc(t) \\geq P\n$$\n\nWhere $P$ is the patience parameter, representing how many consecutive non-improving epochs we tolerate before stopping.\n\n#### Key Parameters and Their Effects\n\n**Patience ($P$)**\n\nThe patience parameter controls how long we wait for improvement:\n\n- **Low patience** (e.g., 2-3 epochs): Stops training quickly, may terminate too early if validation loss is noisy\n- **High patience** (e.g., 15-20 epochs): Allows more time for improvement, but may permit some overfitting\n- **Typical values**: 5-10 epochs for most applications\n\nMathematically, patience represents the maximum allowable value of consecutive non-improving steps:\n\n$$\nP = \\max \\{ k : L_{val}(t+i) \\geq L_{best}(t) - \\delta, \\forall i \\in [1,k] \\}\n$$\n\n**Minimum Delta ($\\delta$)**\n\nThe minimum delta threshold determines what constitutes \"improvement\":\n\n- **$\\delta = 0$**: Any decrease in loss counts as improvement (sensitive to noise)\n- **$\\delta > 0$**: Loss must decrease by at least $\\delta$ to count (more robust)\n- **Typical values**: 0.001 to 0.01 for normalized losses, scaled appropriately for unnormalized losses\n\nFor relative improvements, $\\delta$ can be defined as a fraction of the current best loss:\n\n$$\n\\delta_{relative} = \\epsilon \\cdot L_{best}\n$$\n\nWhere $\\epsilon$ is a small percentage (e.g., 0.01 for 1% improvement).\n\n#### Conceptual Example\n\nConsider training with patience $P = 3$ and min_delta $\\delta = 0.01$:\n\nSuppose we observe validation losses: $[0.5, 0.4, 0.39, 0.39, 0.39]$\n\n- **Epoch 0**: First epoch, initialize $L_{best} = 0.5$, counter $c = 0$\n- **Epoch 1**: Loss $0.4 < 0.5 - 0.01 = 0.49$ → Improvement! Update $L_{best} = 0.4$, reset $c = 0$\n- **Epoch 2**: Loss $0.39 \\not< 0.4 - 0.01 = 0.39$ → No improvement (exactly at threshold), increment $c = 1$\n- **Epoch 3**: Loss $0.39 \\not< 0.39$ → No improvement, increment $c = 2$\n- **Epoch 4**: Loss $0.39 \\not< 0.39$ → No improvement, increment $c = 3$\n\nSince $c = 3 \\geq P = 3$, we signal to stop training at epoch 4.\n\n#### Why This Works: The Bias-Variance Tradeoff\n\nEarly stopping implicitly regularizes by limiting model complexity through training time:\n\n$$\n\\text{Generalization Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n$$\n\nAs training progresses:\n- **Bias** decreases (model fits training data better)\n- **Variance** increases (model becomes more sensitive to training data specifics)\n\nEarly stopping finds the optimal point where:\n\n$$\nt^* = \\arg\\min_{t} \\mathbb{E}[L_{test}(t)]\n$$\n\nThis typically occurs before the model has fully minimized training loss, preventing the variance component from dominating.\n\n#### Relationship to L2 Regularization\n\nInterestingly, early stopping has a mathematical connection to L2 regularization. For simple linear models with gradient descent, early stopping approximates the effect of L2 regularization:\n\n$$\nw(t) \\approx w_{L2} \\quad \\text{where} \\quad w_{L2} = \\arg\\min_w \\left[ L(w) + \\lambda \\|w\\|^2 \\right]\n$$\n\nThe relationship between training steps and regularization strength is:\n\n$$\n\\lambda \\propto \\frac{1}{t}\n$$\n\nThus, stopping early is equivalent to stronger regularization, while training longer is equivalent to weaker regularization.\n\n#### Practical Considerations\n\n**Validation Set Size**: The reliability of early stopping depends on having a sufficiently large validation set to provide stable loss estimates. With small validation sets, consider:\n- Increasing patience to account for noise\n- Using k-fold cross-validation\n- Averaging over multiple training runs\n\n**Loss Trajectory Patterns**: Different loss trajectories require different patience values:\n- **Smooth decreasing**: Lower patience works well\n- **Noisy/fluctuating**: Higher patience needed\n- **Sudden drops**: May need to track both recent and global best\n\n**Learning Rate Schedules**: Early stopping interacts with learning rate schedules:\n- Sudden learning rate reductions may appear as loss plateaus\n- Consider resetting patience counter after scheduled learning rate changes\n- Or increase patience when using aggressive learning rate decay\n\n#### When to Use Early Stopping\n\nEarly stopping is particularly effective for:\n\n1. **Deep networks**: Where overfitting is likely and optimal epoch count is unknown\n2. **Limited data**: When validation set is small but representative\n3. **Computational constraints**: Automatically finding good stopping point without extensive hyperparameter search\n4. **High-capacity models**: Where model can easily memorize training data\n\nBe cautious with:\n- Very small validation sets (unreliable stopping signals)\n- Highly stochastic training (noisy loss curves)\n- Models that require long training to converge (may stop too early)\n\n#### Extensions and Variations\n\n**Multiple Metrics**: Instead of just validation loss, monitor:\n\n$$\n\\text{Stop when} \\quad c_{loss}(t) \\geq P_{loss} \\quad \\text{OR} \\quad c_{metric}(t) \\geq P_{metric}\n$$\n\n**Restore Best Weights**: Save model parameters at best validation loss and restore them after stopping.\n\n**Delayed Start**: Don't begin monitoring until after a warm-up period to allow initial training dynamics to stabilize.\n\nEarly stopping is a simple yet powerful technique that provides automatic regularization, making it one of the most widely used methods in deep learning practice."
}