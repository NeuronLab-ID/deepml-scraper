{
  "description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHRoZSBGMSBzY29yZSBnaXZlbiBwcmVkaWN0ZWQgYW5kIHRydWUgbGFiZWxzLiBUaGUgRjEgc2NvcmUgaXMgYSB3aWRlbHkgdXNlZCBtZXRyaWMgaW4gbWFjaGluZSBsZWFybmluZywgY29tYmluaW5nIHByZWNpc2lvbiBhbmQgcmVjYWxsIGludG8gYSBzaW5nbGUgbWVhc3VyZS4gcm91bmQgeW91ciBzb2x1dGlvbiB0byB0aGUgM3JkIGRlY2ltYWwgcGxhY2U=",
  "id": "91",
  "test_cases": [
    {
      "test": "print(calculate_f1_score([1, 0, 1, 1, 0], [1, 0, 0, 1, 1]))",
      "expected_output": "0.667"
    },
    {
      "test": "print(calculate_f1_score([1, 1, 0, 0], [1, 0, 0, 1]))",
      "expected_output": "0.5"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "y_true = [1, 0, 1, 1, 0], y_pred = [1, 0, 0, 1, 1]",
    "output": "0.667",
    "reasoning": "The true positives, false positives, and false negatives are calculated from the given labels. Precision and recall are derived, and the F1 score is computed as their harmonic mean."
  },
  "category": "Machine Learning",
  "starter_code": "def calculate_f1_score(y_true, y_pred):\n\t\"\"\"\n\tCalculate the F1 score based on true and predicted labels.\n\n\tArgs:\n\t\ty_true (list): True labels (ground truth).\n\t\ty_pred (list): Predicted labels.\n\n\tReturns:\n\t\tfloat: The F1 score rounded to three decimal places.\n\t\"\"\"\n\t# Your code here\n\tpass\n\treturn round(f1,3)",
  "title": "Calculate F1 Score from Predicted and True Labels",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "description_decoded": "Implement a function to calculate the F1 score given predicted and true labels. The F1 score is a widely used metric in machine learning, combining precision and recall into a single measure. round your solution to the 3rd decimal place",
  "learn_section_decoded": "## **F1 Score**\n\nThe F1 score is a widely used metric in machine learning and statistics, particularly for evaluating classification models. It is the harmonic mean of **precision** and **recall**, providing a single measure that balances the trade-off between these two metrics.\n\n### **Key Concepts**\n\n1. **Precision**: Precision is the fraction of true positive predictions out of all positive predictions made by the model. It measures how many of the predicted positive instances are actually correct.\n\n    $$\n    \\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n    $$\n\n2. **Recall**: Recall is the fraction of true positive predictions out of all actual positive instances in the dataset. It measures how many of the actual positive instances were correctly predicted.\n\n    $$\n    \\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n    $$\n\n3. **F1 Score**: The F1 score is the harmonic mean of precision and recall, providing a balanced measure that takes both metrics into account:\n\n    $$\n    \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n    $$\n\n### **Why Use the F1 Score?**\n\nThe F1 score is particularly useful when the dataset is imbalanced, meaning the classes are not equally represented. It provides a single metric that balances the trade-off between precision and recall, especially in scenarios where maximizing one metric might lead to a significant drop in the other.\n\n### **Example Calculation**\n\nGiven:\ny_true = [1, 0, 1, 1, 0] \n\n\ny_pred = [1, 0, 0, 1, 1] \n\n1. **Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)**:\n\n    $$\n    \\text{TP} = 2, \\quad \\text{FP} = 1, \\quad \\text{FN} = 1\n    $$\n\n2. **Calculate Precision**:\n\n    $$\n    \\text{Precision} = \\frac{2}{2 + 1} = \\frac{2}{3} \\approx 0.667\n    $$\n\n3. **Calculate Recall**:\n\n    $$\n    \\text{Recall} = \\frac{2}{2 + 1} = \\frac{2}{3} \\approx 0.667\n    $$\n\n4. **Calculate F1 Score**:\n\n    $$\n    \\text{F1 Score} = 2 \\times \\frac{0.667 \\times 0.667}{0.667 + 0.667} = 0.667\n    $$\n\n### **Applications**\n\nThe F1 score is widely used in:\n- Binary classification problems (e.g., spam detection, fraud detection).\n- Multi-class classification problems (evaluated per class and averaged).\n- Information retrieval tasks (e.g., search engines, recommendation systems).\n\nMastering the F1 score is essential for evaluating and comparing the performance of classification models.\n"
}