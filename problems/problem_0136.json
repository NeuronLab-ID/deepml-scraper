{
  "description": "S0wgZGl2ZXJnZW5jZSBtZWFzdXJlcyB0aGUgZGlzc2ltaWxhcml0eSBiZXR3ZWVuIHR3byBwcm9iYWJpbGl0eSBkaXN0cmlidXRpb25zLiBJbiB0aGlzIHByb2JsZW0sIHlvdSdsbCBpbXBsZW1lbnQgYSBmdW5jdGlvbiB0byBjb21wdXRlIHRoZSBLTCBkaXZlcmdlbmNlIGJldHdlZW4gdHdvIG11bHRpdmFyaWF0ZSBHYXVzc2lhbiBkaXN0cmlidXRpb25zIGdpdmVuIHRoZWlyIG1lYW5zIGFuZCBjb3ZhcmlhbmNlIG1hdHJpY2VzLiBVc2UgdGhlIHByb3ZpZGVkIG1hdGhlbWF0aWNhbCBmb3JtdWxhcyBhbmQgbnVtZXJpY2FsIGNvbnNpZGVyYXRpb25zIHRvIGVuc3VyZSBhY2N1cmFjeS4=",
  "id": "136",
  "test_cases": [
    {
      "test": "import numpy as np\nnp.random.seed(42)\nPx = np.random.randn(4, 10)\nQx = np.random.randn(4, 10)\nmu1, cov1, mu2, cov2 = np.mean(Px, axis=1), np.cov(Px), np.mean(Qx, axis=1), np.cov(Qx)\nprint(round(multivariate_kl_divergence(mu1, cov1, mu2, cov2),4))",
      "expected_output": "2.193"
    },
    {
      "test": "import numpy as np\nnp.random.seed(42)\nPx = np.random.randn(3, 8)\nQx = np.random.randn(3, 8)\nmu1, cov1, mu2, cov2 = np.mean(Px, axis=1), np.cov(Px), np.mean(Qx, axis=1), np.cov(Qx)\nprint(round(multivariate_kl_divergence(mu1, cov1, mu2, cov2),4))",
      "expected_output": "1.7741"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "mu_p, Cov_p, mu_q, Cov_q for two random multivariate Gaussians",
    "output": "A float representing the KL divergence",
    "reasoning": "The KL divergence is calculated using the formula: 0.5 * (log det term, minus dimension p, Mahalanobis distance between means, and trace term). It measures how dissimilar the second Gaussian is from the first."
  },
  "category": "Probability",
  "starter_code": "import numpy as np\n\ndef multivariate_kl_divergence(mu_p: np.ndarray, Cov_p: np.ndarray, mu_q: np.ndarray, Cov_q: np.ndarray) -> float:\n    \"\"\"\n    Computes the KL divergence between two multivariate Gaussian distributions.\n    \n    Parameters:\n    mu_p: mean vector of the first distribution\n    Cov_p: covariance matrix of the first distribution\n    mu_q: mean vector of the second distribution\n    Cov_q: covariance matrix of the second distribution\n\n    Returns:\n    KL divergence as a float\n    \"\"\"\n    # Your code here\n    pass",
  "title": "Calculate KL Divergence Between Two Multivariate Gaussian Distributions",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/turkunov",
      "name": "turkunov"
    }
  ],
  "description_decoded": "KL divergence measures the dissimilarity between two probability distributions. In this problem, you'll implement a function to compute the KL divergence between two multivariate Gaussian distributions given their means and covariance matrices. Use the provided mathematical formulas and numerical considerations to ensure accuracy.",
  "learn_section_decoded": "## KL divergence and its properties\nKL divergence is used as a measure of dissimilarity between two distributions. It is defined by the following formula:\n$$\nD_{KL}(P || Q) = \\mathbb{E}_{x\\sim P(X)}log\\frac{P(X)}{Q(X)},\n$$\nwhere $P(X)$ observed distribution we compare everything else with and $Q(X)$ is usually the varying one; $P(X)$ and $Q(X)$ are PMF (but could also be denoted as PDFs $f(x)$ and $q(x)$ in continuos case). The function has following properties:\n* $D_{KL}\\geq0$\n* assymetry: $D_{KL}(P || Q) \\neq D_{KL}(Q || P)$\n\n## Finding $D_{KL}$ between two multivariate Gaussians\nConsider two multivariate Normal distributions:\n$$\np(x)\\sim \\mathbb{N}(\\mu_1,\\Sigma_1), \\\\\nq(x)\\sim \\mathbb{N}(\\mu_2,\\Sigma_2)\n$$\n\nPDF of a multivariate Normal distribution is defined as:\n$$\nf(x)=\\frac{1}{(2\\pi)^\\frac{p}{2}|\\Sigma|^\\frac{1}{2}}exp(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)),\n$$\n\nwhere $\\Sigma$ - covariance matrix, $|\\cdot|$ - determinant, $p$ - size of the random vector, i.e. number of different normally distributed features inside $P$ and $Q$ and $x$ usually denotes $x^T$, which is a random vector of size $p\\times1$.\n\nNow we can move onto calculating KL divergence for these two distributions, skipping the division part of two PDFs:\n$$\n\\frac{1}{2}[\\mathbb{E_p}log\\frac{|\\Sigma_q|}{|\\Sigma_p|} ^ \\textbf{[1]} - \\mathbb{E_p}(x-\\mu_p)^T\\Sigma_p^{-1}(x-\\mu_p) ^ \\textbf{[2]} + \\\\\n+ \\mathbb{E_p}(x-\\mu_q)^T\\Sigma_q^{-1}(x-\\mu_q) ^ \\textbf{[3]}]= \\\\\n= \\frac{1}{2}[log\\frac{|\\Sigma_q|}{|\\Sigma_p|}-p+(\\mu_p-\\mu_q)^T\\Sigma^{-1}_q(\\mu_p-\\mu_q) + \\\\\n+ tr(\\Sigma^{-1}_q\\Sigma_p)],\n$$\nwhere in order to achieve an equality we proceed to do $\\textbf{[1]}:$\n$$\nlog\\frac{|\\Sigma_q|}{|\\Sigma_p|}=const\\implies \\text{EV equals to the value itself;}\n$$\n\nthen $\\textbf{[2]}:$\n$$\n\\underset{N \\times p}{(x-\\mu_p)^T} * \\sum_{p \\times p} * \\underset{N \\times p}{(x-\\mu_p)^T} = \\underset{N\\times N}{A}\\text {, where } N=1 \\implies \\\\\n\\implies A=\\operatorname{tr}(A)\n$$\n\nRecall that:\n$$\n\\operatorname{tr}(A B C)=\\operatorname{tr}(B C A)=\\operatorname{tr}(C B A)\n$$\n\nThen:\n$$\n\\operatorname{tr}(A)=\\operatorname{tr}\\left(\\left(x-\\mu_p\\right)^{\\top}\\left(x-\\mu_p\\right) \\Sigma_p^{-1}\\right)\\\\ =\\operatorname{tr}\\left(\\Sigma_p \\Sigma_p^{-1}\\right)=\\operatorname{tr}(I)=p  \n$$\n\nand finally $\\textbf{[3]}$, where we should recall, that for multivariate Normal distributions this is true ($x\\sim\\mathbb{N}(\\mu_2, \\Sigma_2)$):\n$$\n\\mathbb{E}(x-\\mu_1)^TA(x-\\mu_1)= \\\\\n= (\\mu_2-\\mu_1)^TA(\\mu_2-\\mu_1)+tr(A\\Sigma_2)\n$$"
}