{
  "description": "SW4gdGhpcyBwcm9ibGVtLCB5b3UgbmVlZCB0byBpbXBsZW1lbnQgYSAyRCBjb252b2x1dGlvbmFsIGxheWVyIGluIFB5dGhvbi4gVGhpcyBmdW5jdGlvbiB3aWxsIHByb2Nlc3MgYW4gaW5wdXQgbWF0cml4IHVzaW5nIGEgc3BlY2lmaWVkIGNvbnZvbHV0aW9uYWwga2VybmVsLCBwYWRkaW5nLCBhbmQgc3RyaWRlLgo=",
  "mdx_file": "78c5d6df-d000-4da1-a86d-633e13d2c4fa.mdx",
  "id": "41",
  "test_cases": [
    {
      "test": "input_matrix = np.array([\n    [1., 2., 3., 4., 5.],\n    [6., 7., 8., 9., 10.],\n    [11., 12., 13., 14., 15.],\n    [16., 17., 18., 19., 20.],\n    [21., 22., 23., 24., 25.],\n])\nkernel = np.array([\n    [1., 2.],\n    [3., -1.],\n])\npadding, stride = 0, 1\nexpected = np.array([\n    [ 16., 21., 26., 31.],\n    [ 41., 46., 51., 56.],\n    [ 66., 71., 76., 81.],\n    [ 91., 96., 101., 106.],\n])\noutput = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(output)",
      "expected_output": "[[ 16.,  21.,  26.,  31.],\n [ 41.,  46.,  51.,  56.],\n [ 66.,  71.,  76.,  81.],\n [ 91.,  96., 101., 106.]]"
    },
    {
      "test": "input_matrix = np.array([\n    [1., 2., 3., 4., 5.],\n    [6., 7., 8., 9., 10.],\n    [11., 12., 13., 14., 15.],\n    [16., 17., 18., 19., 20.],\n    [21., 22., 23., 24., 25.],\n])\nkernel = np.array([\n    [.5, 3.2],\n    [1., -1.],\n])\npadding, stride = 2, 2\nexpected = np.array([\n        [ -1., 1., 3., 5., 7., 15.],\n        [ -4., 16., 21., 26., 31., 35.],\n        [  1., 41., 46., 51., 56., 55.],\n        [  6., 66., 71., 76., 81., 75.],\n        [ 11., 91., 96., 101., 106., 95.],\n        [ 42., 65., 68., 71., 74.,  25.],\n    ])\noutput = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(output)",
      "expected_output": "[[ 0.,   0.,   0.,   0. ],\n [ 0.,   5.9, 13.3, 12.5],\n [ 0.,  42.9, 50.3, 27.5],\n [ 0.,  80.9, 88.3, 12.5],]"
    }
  ],
  "difficulty": "medium",
  "pytorch_difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "import numpy as np\n\ninput_matrix = np.array([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12],\n    [13, 14, 15, 16]\n])\n\nkernel = np.array([\n    [1, 0],\n    [-1, 1]\n])\n\npadding = 1\nstride = 2\n\noutput = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(output)",
    "reasoning": "The function performs a 2D convolution operation on the input matrix using the specified kernel, padding, and stride. The output matrix contains the results of the convolution operation.",
    "output": "[[ 1.  1. -4.],[ 9.  7. -4.],[ 0. 14. 16.]]"
  },
  "category": "Deep Learning",
  "starter_code": "import numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n\tinput_height, input_width = input_matrix.shape\n\tkernel_height, kernel_width = kernel.shape\n\n\t# Your code here\n    \n\treturn output_matrix\n",
  "title": "Simple Convolutional 2D Layer",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/drogovozDP",
      "name": "Drogovoz Dima"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "import torch\nfrom __main__ import simple_conv2d\ninput_matrix = torch.tensor([\n    [1., 2., 3., 4., 5.],\n    [6., 7., 8., 9., 10.],\n    [11., 12., 13., 14., 15.],\n    [16., 17., 18., 19., 20.],\n    [21., 22., 23., 24., 25.],\n])\nkernel = torch.tensor([\n    [1., 2.],\n    [3., -1.],\n])\noutput = simple_conv2d(input_matrix, kernel, padding=0, stride=1)\nprint(output)",
      "expected_output": "tensor([[ 16.,  21.,  26.,  31.],\n        [ 41.,  46.,  51.,  56.],\n        [ 66.,  71.,  76.,  81.],\n        [ 91.,  96., 101., 106.]])"
    },
    {
      "test": "import torch\nfrom __main__ import simple_conv2d\ninput_matrix = torch.tensor([\n    [1., 2., 3., 4., 5.],\n    [6., 7., 8., 9., 10.],\n    [11., 12., 13., 14., 15.],\n    [16., 17., 18., 19., 20.],\n    [21., 22., 23., 24., 25.],\n])\nkernel = torch.tensor([\n    [0.5, 3.2],\n    [1.0, -1.0],\n])\noutput = simple_conv2d(input_matrix, kernel, padding=2, stride=2)\nprint(output)",
      "expected_output": "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  5.9000, 13.3000, 12.5000],\n        [ 0.0000, 42.9000, 50.3000, 27.5000],\n        [ 0.0000, 80.9000, 88.3000, 12.5000]])"
    }
  ],
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubi5mdW5jdGlvbmFsIGFzIEYKCmRlZiBzaW1wbGVfY29udjJkKGlucHV0X21hdHJpeDogdG9yY2guVGVuc29yLCBrZXJuZWw6IHRvcmNoLlRlbnNvciwgcGFkZGluZzogaW50LCBzdHJpZGU6IGludCkgLT4gdG9yY2guVGVuc29yOgogICAgIiIiCiAgICBQZXJmb3JtIGEgMkQgY29udm9sdXRpb24gb24gYSBzaW5nbGUtY2hhbm5lbCBpbnB1dCB1c2luZyBQeVRvcmNoJ3MgYnVpbHQtaW4gY29udjJkLgogICAgaW5wdXRfbWF0cml4OiAyRCB0ZW5zb3IgKEgsIFcpCiAgICBrZXJuZWw6IDJEIHRlbnNvciAoa0gsIGtXKQogICAgcGFkZGluZzogaW50LCB6ZXJvLXBhZGRpbmcgb24gYWxsIHNpZGVzCiAgICBzdHJpZGU6IGludCwgc3RyaWRlIG9mIHRoZSBjb252b2x1dGlvbgogICAgIiIiCiAgICAjIEhpbnQ6IGNvbnYyZCBleHBlY3RzIGlucHV0IG9mIHNoYXBlIChOLCBDLCBILCBXKSBhbmQgd2VpZ2h0IG9mIHNoYXBlIChvdXRfY2hhbm5lbHMsIGluX2NoYW5uZWxzLCBrSCwga1cpCiAgICBwYXNzCg==",
  "description_decoded": "In this problem, you need to implement a 2D convolutional layer in Python. This function will process an input matrix using a specified convolutional kernel, padding, and stride.\n",
  "learn_section_decoded": "## Simple Convolutional 2D Layer\n\nThe Convolutional layer is a fundamental component used extensively in Computer Vision tasks. Here are the crucial parameters:\n\n### Parameters\n1. **input_matrix**:  \n   A 2D NumPy array representing the input data, such as an image. Each element in this array corresponds to a pixel or a feature value in the input space. The dimensions of the input matrix are typically represented as $ \\text{height} \\times \\text{width} $.\n\n2. **kernel**:  \n   Another 2D NumPy array representing the convolutional filter. The kernel is smaller than the input matrix and slides over it to perform the convolution operation. Each element in the kernel serves as a weight that modifies the input during convolution. The kernel size is denoted as $ \\text{kernel\\_height} \\times \\text{kernel\\_width} $.\n\n3. **padding**:  \n   An integer specifying the number of rows and columns of zeros added around the input matrix. Padding controls the spatial dimensions of the output, allowing the kernel to process edge elements effectively or to maintain the original input size.\n\n4. **stride**:  \n   An integer that represents the number of steps the kernel moves across the input matrix for each convolution. A stride greater than one reduces the output size, as the kernel skips over elements.\n\n### Implementation\n1. **Padding the Input**:  \n   The input matrix is padded with zeros based on the specified `padding` value. This increases the input size and enables the kernel to cover elements at the borders and corners.\n\n2. **Calculating Output Dimensions**:  \n   The height and width of the output matrix are calculated using the following formulas:\n   $$\n   \\text{output\\_height} = \\left( \\frac{\\text{input\\_height, padded} - \\text{kernel\\_height}}{\\text{stride}} \\right) + 1\n   $$\n   $$\n   \\text{output\\_width} = \\left( \\frac{\\text{input\\_width, padded} - \\text{kernel\\_width}}{\\text{stride}} \\right) + 1\n   $$\n\n3. **Performing Convolution**:\n   - A nested loop iterates over each position where the kernel can be applied to the padded input matrix.\n   - At each position, a region of the input matrix, matching the size of the kernel, is selected.\n   - Element-wise multiplication between the kernel and the input region is performed, followed by summing the results to produce a single value. This value is then stored in the corresponding position of the output matrix.\n\n4. **Output**:  \n   The function returns the output matrix, which contains the results of the convolution operation performed across the entire input.\n\n"
}