{
  "description": "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",
  "id": "251",
  "test_cases": [
    {
      "test": "print(analyze_canary_deployment([{'latency_ms': 45, 'prediction': 1, 'ground_truth': 1}, {'latency_ms': 50, 'prediction': 0, 'ground_truth': 0}, {'latency_ms': 48, 'prediction': 1, 'ground_truth': 1}, {'latency_ms': 52, 'prediction': 1, 'ground_truth': 0}, {'latency_ms': 47, 'prediction': 0, 'ground_truth': 0}], [{'latency_ms': 50, 'prediction': 1, 'ground_truth': 1}, {'latency_ms': 55, 'prediction': 0, 'ground_truth': 0}, {'latency_ms': 52, 'prediction': 1, 'ground_truth': 0}, {'latency_ms': 58, 'prediction': 0, 'ground_truth': 0}, {'latency_ms': 53, 'prediction': 1, 'ground_truth': 1}]))",
      "expected_output": "{'canary_accuracy': 0.8, 'baseline_accuracy': 0.8, 'accuracy_change_pct': 0.0, 'canary_avg_latency': 48.4, 'baseline_avg_latency': 53.6, 'latency_change_pct': -9.7, 'promote_recommended': True}"
    },
    {
      "test": "print(analyze_canary_deployment([{'latency_ms': 100, 'prediction': 1, 'ground_truth': 0}, {'latency_ms': 110, 'prediction': 0, 'ground_truth': 0}, {'latency_ms': 95, 'prediction': 1, 'ground_truth': 0}, {'latency_ms': 105, 'prediction': 1, 'ground_truth': 1}], [{'latency_ms': 50, 'prediction': 1, 'ground_truth': 1}, {'latency_ms': 48, 'prediction': 0, 'ground_truth': 0}, {'latency_ms': 52, 'prediction': 1, 'ground_truth': 1}, {'latency_ms': 50, 'prediction': 0, 'ground_truth': 0}]))",
      "expected_output": "{'canary_accuracy': 0.5, 'baseline_accuracy': 1.0, 'accuracy_change_pct': -50.0, 'canary_avg_latency': 102.5, 'baseline_avg_latency': 50.0, 'latency_change_pct': 105.0, 'promote_recommended': False}"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "canary_results = [{'latency_ms': 45, 'prediction': 1, 'ground_truth': 1}, {'latency_ms': 50, 'prediction': 0, 'ground_truth': 0}, {'latency_ms': 48, 'prediction': 1, 'ground_truth': 1}, {'latency_ms': 52, 'prediction': 1, 'ground_truth': 0}, {'latency_ms': 47, 'prediction': 0, 'ground_truth': 0}], baseline_results = [{'latency_ms': 50, 'prediction': 1, 'ground_truth': 1}, {'latency_ms': 55, 'prediction': 0, 'ground_truth': 0}, {'latency_ms': 52, 'prediction': 1, 'ground_truth': 0}, {'latency_ms': 58, 'prediction': 0, 'ground_truth': 0}, {'latency_ms': 53, 'prediction': 1, 'ground_truth': 1}]",
    "output": "{'canary_accuracy': 0.8, 'baseline_accuracy': 0.8, 'accuracy_change_pct': 0.0, 'canary_avg_latency': 48.4, 'baseline_avg_latency': 53.6, 'latency_change_pct': -9.7, 'promote_recommended': True}",
    "reasoning": "Canary has 4/5 correct predictions (accuracy 0.8), baseline also has 4/5 (accuracy 0.8), so accuracy change is 0%. Canary average latency is (45+50+48+52+47)/5 = 48.4ms, baseline is (50+55+52+58+53)/5 = 53.6ms. Latency change is (48.4-53.6)/53.6 * 100 = -9.7% (improved). Since accuracy did not degrade and latency improved, promote_recommended is True."
  },
  "category": "MLOps",
  "starter_code": "def analyze_canary_deployment(canary_results: list, baseline_results: list, accuracy_tolerance: float = 0.05, latency_tolerance: float = 0.10) -> dict:\n    \"\"\"\n    Analyze canary deployment health metrics for model rollout decision.\n    \n    Args:\n        canary_results: list of prediction results from canary (new) model\n                       Each dict has 'latency_ms', 'prediction', 'ground_truth'\n        baseline_results: list of prediction results from baseline (existing) model\n                         Each dict has 'latency_ms', 'prediction', 'ground_truth'\n        accuracy_tolerance: max acceptable relative accuracy degradation (0.05 = 5%)\n        latency_tolerance: max acceptable relative latency increase (0.10 = 10%)\n    \n    Returns:\n        dict with canary/baseline metrics and promotion recommendation\n    \"\"\"\n    pass",
  "title": "Analyze Canary Deployment Health for Model Rollout",
  "createdAt": "December 14, 2025 at 11:57:17â€¯AM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "learn_section": "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",
  "description_decoded": "In production ML systems, canary deployments are a critical strategy for safely rolling out new model versions. A small percentage of traffic is routed to the new (canary) model while the majority continues to use the existing (baseline) model. By comparing their performance, you can decide whether to promote the canary to full production or roll back.\n\nGiven prediction results from both canary and baseline models, compute key comparison metrics to determine if the canary deployment is healthy.\n\nEach result in both lists is a dictionary with:\n- 'latency_ms': Response latency in milliseconds (float)\n- 'prediction': The model's predicted value\n- 'ground_truth': The actual correct value\n\nWrite a function `analyze_canary_deployment(canary_results, baseline_results, accuracy_tolerance, latency_tolerance)` that computes:\n\n1. **canary_accuracy**: Fraction of correct predictions for canary model (0-1)\n2. **baseline_accuracy**: Fraction of correct predictions for baseline model (0-1)\n3. **accuracy_change_pct**: Relative change in accuracy as percentage\n4. **canary_avg_latency**: Average latency of canary model (ms)\n5. **baseline_avg_latency**: Average latency of baseline model (ms)\n6. **latency_change_pct**: Relative change in latency as percentage\n7. **promote_recommended**: Boolean - True if canary accuracy did not degrade beyond accuracy_tolerance AND latency did not increase beyond latency_tolerance\n\nIf either input list is empty, return an empty dictionary.\n\nAll numeric values should be rounded to 2 decimal places except accuracy values which should be rounded to 4 decimal places.",
  "learn_section_decoded": "## Canary Deployments in MLOps\n\nCanary deployment is a risk-mitigation strategy used when releasing new model versions to production. Instead of switching all traffic to the new model at once, you gradually route a small percentage (typically 1-10%) to the \"canary\" while monitoring its performance against the existing \"baseline\" model.\n\n### Key Metrics for Canary Analysis\n\n#### 1. Accuracy Comparison\n\nAccuracy measures the fraction of correct predictions:\n\n$$\\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}$$\n\nThe relative accuracy change quantifies how the canary compares to baseline:\n\n$$\\text{Accuracy Change \\%} = \\frac{A_{canary} - A_{baseline}}{A_{baseline}} \\times 100$$\n\nwhere $A_{canary}$ and $A_{baseline}$ are the respective accuracies.\n\n#### 2. Latency Comparison\n\nAverage latency affects user experience:\n\n$$\\bar{L} = \\frac{1}{n}\\sum_{i=1}^{n} L_i$$\n\nThe relative latency change:\n\n$$\\text{Latency Change \\%} = \\frac{\\bar{L}_{canary} - \\bar{L}_{baseline}}{\\bar{L}_{baseline}} \\times 100$$\n\nPositive values indicate the canary is slower; negative values indicate improvement.\n\n### Promotion Decision Logic\n\nThe promotion decision uses tolerance thresholds:\n\n$$\\text{Promote} = (\\Delta_{acc} \\geq -\\tau_{acc}) \\land (\\Delta_{lat} \\leq \\tau_{lat})$$\n\nwhere:\n- $\\Delta_{acc}$ is the accuracy change percentage\n- $\\Delta_{lat}$ is the latency change percentage  \n- $\\tau_{acc}$ is the accuracy tolerance (e.g., 5%)\n- $\\tau_{lat}$ is the latency tolerance (e.g., 10%)\n\n### Interpreting Results\n\n| Scenario | Accuracy Change | Latency Change | Recommendation |\n|----------|----------------|----------------|----------------|\n| Ideal | Positive or zero | Negative or within tolerance | Promote |\n| Acceptable | Within tolerance | Within tolerance | Promote |\n| Risky | Beyond tolerance | Any | Do not promote |\n| Slow | Any | Beyond tolerance | Do not promote |\n\n### Best Practices\n\n1. **Statistical Significance**: Ensure sufficient sample sizes before making decisions\n2. **Multiple Metrics**: Consider additional metrics like error rates, p99 latency\n3. **Gradual Rollout**: Increase canary traffic incrementally (1% -> 5% -> 25% -> 100%)\n4. **Automatic Rollback**: Implement automated rollback if metrics degrade significantly\n5. **Segmented Analysis**: Analyze performance across different user segments or request types"
}