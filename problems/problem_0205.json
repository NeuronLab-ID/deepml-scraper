{
  "description": "SW1wbGVtZW50IGZ1bmN0aW9ucyB0byBjb21wdXRlIGJvdGggZW50cm9weSBhbmQgY3Jvc3MtZW50cm9weSBmb3IgcHJvYmFiaWxpdHkgZGlzdHJpYnV0aW9ucy4gRW50cm9weSBtZWFzdXJlcyB0aGUgdW5jZXJ0YWludHkgb3IgaW5mb3JtYXRpb24gY29udGVudCBpbiBhIHNpbmdsZSBkaXN0cmlidXRpb24sIHdoaWxlIGNyb3NzLWVudHJvcHkgbWVhc3VyZXMgdGhlIGF2ZXJhZ2UgbnVtYmVyIG9mIGJpdHMgbmVlZGVkIHRvIGlkZW50aWZ5IGV2ZW50cyBmcm9tIGRpc3RyaWJ1dGlvbiBQIHdoZW4gdXNpbmcgYSBjb2Rpbmcgc2NoZW1lIG9wdGltaXplZCBmb3IgZGlzdHJpYnV0aW9uIFEuIFJldHVybiBib3RoIHZhbHVlcyBhcyBhIHR1cGxlLg==",
  "id": "205",
  "test_cases": [
    {
      "test": "import numpy as np; h, ce = entropy_and_cross_entropy([0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]); print(round(h, 6), round(ce, 6))",
      "expected_output": "1.386294 1.386294"
    },
    {
      "test": "import numpy as np; h, ce = entropy_and_cross_entropy([1.0, 0.0, 0.0], [0.9, 0.05, 0.05]); print(round(h, 6), round(ce, 6))",
      "expected_output": "0.0 0.105361"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "P = [0.7, 0.3], Q = [0.6, 0.4]",
    "output": "(0.610864, 0.632465)",
    "reasoning": "Entropy: $H(P) = -(0.7\\log(0.7) + 0.3\\log(0.3)) \\approx 0.611$. Cross-entropy: $H(P,Q) = -(0.7\\log(0.6) + 0.3\\log(0.4)) \\approx 0.632$. Cross-entropy is higher because Q doesn't perfectly match P."
  },
  "category": "Information Theory",
  "starter_code": "import numpy as np\n\ndef entropy_and_cross_entropy(P: list[float], Q: list[float]) -> tuple[float, float]:\n\t\"\"\"\n\tCompute entropy of P and cross-entropy between P and Q.\n\t\n\tArgs:\n\t\tP: True probability distribution\n\t\tQ: Predicted probability distribution\n\t\n\tReturns:\n\t\tTuple of (entropy H(P), cross-entropy H(P,Q))\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Entropy & Cross-Entropy",
  "createdAt": "November 16, 2025 at 8:36:50â€¯AM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement functions to compute both entropy and cross-entropy for probability distributions. Entropy measures the uncertainty or information content in a single distribution, while cross-entropy measures the average number of bits needed to identify events from distribution P when using a coding scheme optimized for distribution Q. Return both values as a tuple.",
  "learn_section_decoded": "### Understanding Entropy and Cross-Entropy\n\nEntropy and cross-entropy are fundamental concepts in information theory that quantify uncertainty and the cost of encoding information. These measures are essential in machine learning, particularly in classification tasks.\n\n#### Entropy: Measuring Uncertainty\n\nEntropy measures the average amount of information (or uncertainty) in a probability distribution. For a discrete distribution $P$:\n\n$$\nH(P) = -\\sum_{i} P(i) \\log P(i)\n$$\n\nBy convention, $0 \\log 0 = 0$ (since $\\lim_{x \\to 0} x \\log x = 0$).\n\n**Intuition**: Entropy answers \"How surprised would we be, on average, by events drawn from this distribution?\" Higher entropy means more uncertainty.\n\n#### Key Properties of Entropy\n\n**Minimum entropy (0)**: Occurs for deterministic distributions where one outcome has probability 1:\n\n$$\nP = [1, 0, 0, \\ldots] \\implies H(P) = 0\n$$\n\nNo surprise - we always know what will happen.\n\n**Maximum entropy**: For $n$ outcomes, entropy is maximized by the uniform distribution:\n\n$$\nP = \\left[\\frac{1}{n}, \\frac{1}{n}, \\ldots, \\frac{1}{n}\\right] \\implies H(P) = \\log(n)\n$$\n\nMaximum surprise - all outcomes equally likely.\n\n**Non-negativity**: $H(P) \\geq 0$ always.\n\n#### Cross-Entropy: Measuring Distribution Difference\n\nCross-entropy measures the average number of bits needed to encode events from distribution $P$ using a coding scheme optimized for distribution $Q$:\n\n$$\nH(P, Q) = -\\sum_{i} P(i) \\log Q(i)\n$$\n\n**Intuition**: If $Q$ is a model's predicted distribution and $P$ is the true distribution, cross-entropy measures how \"surprised\" we are on average when using $Q$ to encode events that actually follow $P$.\n\n#### Relationship Between Entropy and Cross-Entropy\n\nCross-entropy is always at least as large as entropy:\n\n$$\nH(P, Q) \\geq H(P)\n$$\n\nWith equality if and only if $P = Q$. This follows from Gibbs' inequality.\n\nThe difference is the KL divergence:\n\n$$\nH(P, Q) = H(P) + D_{KL}(P || Q)\n$$\n\nWhere $D_{KL}(P || Q) \\geq 0$ measures how different $Q$ is from $P$.\n\n#### Example Calculation\n\nConsider $P = [0.7, 0.3]$ and $Q = [0.6, 0.4]$.\n\n**Entropy of P**:\n\n$$\nH(P) = -(0.7 \\log 0.7 + 0.3 \\log 0.3)\n$$\n$$\n= -(0.7 \\times (-0.357) + 0.3 \\times (-1.204))\n$$\n$$\n= 0.250 + 0.361 = 0.611 \\text{ nats}\n$$\n\n**Cross-Entropy**:\n\n$$\nH(P, Q) = -(0.7 \\log 0.6 + 0.3 \\log 0.4)\n$$\n$$\n= -(0.7 \\times (-0.511) + 0.3 \\times (-0.916))\n$$\n$$\n= 0.358 + 0.275 = 0.632 \\text{ nats}\n$$\n\nSince $Q \\neq P$, we have $H(P,Q) > H(P)$. The difference (0.021 nats) is the KL divergence.\n\n#### Why Cross-Entropy Matters in Machine Learning\n\n**Loss Function**: In classification, cross-entropy loss measures how well predicted probabilities match true labels:\n\n$$\n\\text{Loss} = -\\sum_{i} y_i \\log(\\hat{y}_i)\n$$\n\nWhere $y$ is the true distribution (often one-hot) and $\\hat{y}$ is the model's prediction.\n\n**Training Objective**: Minimizing cross-entropy is equivalent to:\n- Maximizing log-likelihood\n- Minimizing KL divergence from true distribution\n- Maximum likelihood estimation\n\n**Binary Classification**: For binary outcomes, cross-entropy becomes:\n\n$$\nH(P, Q) = -[p \\log q + (1-p) \\log(1-q)]\n$$\n\nThis is the binary cross-entropy loss commonly used in neural networks.\n\n#### Information-Theoretic Interpretation\n\n**Entropy as optimal code length**: $H(P)$ represents the minimum average bits needed to encode messages from $P$ using an optimal code.\n\n**Cross-entropy as suboptimal code length**: $H(P,Q)$ represents the average bits needed when using a code optimized for $Q$ to encode messages from $P$.\n\n**Penalty for mismatch**: The extra cost $H(P,Q) - H(P) = D_{KL}(P||Q)$ quantifies the inefficiency of using the wrong distribution.\n\n#### Practical Considerations\n\n**Logarithm base**: \n- Natural log (base $e$): Results in \"nats\"\n- Base 2: Results in \"bits\"\n- Both are valid; just scale factors\n\n**Numerical stability**: Add small epsilon when computing $\\log(Q(i))$ to avoid $\\log(0)$:\n\n$$\n\\log(Q(i) + \\epsilon) \\quad \\text{where } \\epsilon \\approx 10^{-10}\n$$\n\n**Infinite cross-entropy**: If $P(i) > 0$ but $Q(i) = 0$, cross-entropy is infinite - the coding scheme fails for possible events.\n\n#### Applications Beyond ML\n\n**Data Compression**: Huffman coding uses entropy to determine optimal compression.\n\n**Model Selection**: Lower cross-entropy indicates better predictive models.\n\n**Natural Language Processing**: Perplexity ($2^{H(P,Q)}$) measures language model quality.\n\n**Physics**: Entropy appears in statistical mechanics and thermodynamics.\n\n**Neuroscience**: Information-theoretic measures quantify neural coding efficiency."
}