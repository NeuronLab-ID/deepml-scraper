{
  "description": "SW1wbGVtZW50IHRoZSBIYXJkdGFuaCBhY3RpdmF0aW9uIGZ1bmN0aW9uLCBhIHBpZWNld2lzZSBsaW5lYXIgZnVuY3Rpb24gY29tbW9ubHkgdXNlZCBpbiBuZXVyYWwgbmV0d29ya3MuIEhhcmR0YW5oIGNsaXBzIGlucHV0IHZhbHVlcyB0byBsaWUgd2l0aGluIGEgc3BlY2lmaWVkIHJhbmdlIGRlZmluZWQgYnkgbWluaW11bSBhbmQgbWF4aW11bSBib3VuZHMuIFZhbHVlcyBiZWxvdyB0aGUgbWluaW11bSBhcmUgc2V0IHRvIHRoZSBtaW5pbXVtLCB2YWx1ZXMgYWJvdmUgdGhlIG1heGltdW0gYXJlIHNldCB0byB0aGUgbWF4aW11bSwgYW5kIHZhbHVlcyB3aXRoaW4gdGhlIHJhbmdlIHJlbWFpbiB1bmNoYW5nZWQuIFRoaXMgYm91bmRlZCBhY3RpdmF0aW9uIGZ1bmN0aW9uIGlzIHVzZWZ1bCB3aGVuIHlvdSB3YW50IHRvIGxpbWl0IHRoZSByYW5nZSBvZiBhY3RpdmF0aW9ucyBpbiBhIG5ldXJhbCBuZXR3b3JrIHdoaWxlIG1haW50YWluaW5nIGNvbXB1dGF0aW9uYWwgZWZmaWNpZW5jeS4=",
  "id": "266",
  "test_cases": [
    {
      "test": "print(hardtanh(0.5))",
      "expected_output": "0.5"
    },
    {
      "test": "print(hardtanh(-2.0))",
      "expected_output": "-1.0"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "hardtanh(0.5)",
    "output": "0.5",
    "reasoning": "For x = 0.5 with default min_val = -1.0 and max_val = 1.0, the input value 0.5 lies within the range [-1.0, 1.0]. Since the value is within bounds, it passes through unchanged and the output is 0.5."
  },
  "category": "Deep Learning",
  "starter_code": "def hardtanh(x: float, min_val: float = -1.0, max_val: float = 1.0) -> float:\n\t\"\"\"\n\tCompute the Hardtanh activation function.\n\n\tArgs:\n\t\tx: Input value\n\t\tmin_val: Minimum value for the output range (default: -1.0)\n\t\tmax_val: Maximum value for the output range (default: 1.0)\n\n\tReturns:\n\t\tThe Hardtanh value clipped to [min_val, max_val]\n\t\"\"\"\n\tpass",
  "title": "Implement the Hardtanh Activation Function",
  "createdAt": "December 15, 2025 at 7:02:03â€¯AM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement the Hardtanh activation function, a piecewise linear function commonly used in neural networks. Hardtanh clips input values to lie within a specified range defined by minimum and maximum bounds. Values below the minimum are set to the minimum, values above the maximum are set to the maximum, and values within the range remain unchanged. This bounded activation function is useful when you want to limit the range of activations in a neural network while maintaining computational efficiency.",
  "learn_section_decoded": "## Understanding the Hardtanh Activation Function\n\nThe Hardtanh activation function is a piecewise linear activation that bounds the output to a specified range. It's essentially a \"hard\" version of the hyperbolic tangent (tanh) function, with linear behavior between the bounds and saturation at the extremes.\n\n### Mathematical Definition\n\nThe Hardtanh function is defined as:\n\n$$\n\\text{Hardtanh}(x) = \\begin{cases} \n\\text{min\\_val} & \\text{if } x < \\text{min\\_val} \\\\\nx & \\text{if } \\text{min\\_val} \\leq x \\leq \\text{max\\_val} \\\\\n\\text{max\\_val} & \\text{if } x > \\text{max\\_val}\n\\end{cases}\n$$\n\nBy default, $\\text{min\\_val} = -1$ and $\\text{max\\_val} = 1$, making it clip values to the range $[-1, 1]$.\n\n### Characteristics\n\n1. **Output Range**: \n   - The output is bounded: $[\\text{min\\_val}, \\text{max\\_val}]$\n   - With default parameters, outputs lie in $[-1, 1]$\n\n2. **Linearity**:\n   - Within the range, the function is the identity function: $f(x) = x$\n   - Outside the range, the function saturates to constant values\n\n3. **Derivative**:\n   - The derivative of Hardtanh is:\n   $$\n   \\frac{d}{dx}\\text{Hardtanh}(x) = \\begin{cases} \n   0 & \\text{if } x < \\text{min\\_val} \\\\\n   1 & \\text{if } \\text{min\\_val} \\leq x \\leq \\text{max\\_val} \\\\\n   0 & \\text{if } x > \\text{max\\_val}\n   \\end{cases}\n   $$\n\n### Comparison with Other Activations\n\n| Property | Hardtanh | Tanh | ReLU |\n|----------|----------|------|------|\n| Lower Bound | $\\text{min\\_val}$ | -1 (asymptotic) | 0 |\n| Upper Bound | $\\text{max\\_val}$ | 1 (asymptotic) | None |\n| Smoothness | Piecewise linear | Smooth | Piecewise linear |\n| Computational Cost | Low | Higher | Low |\n\n### Use Cases\n\n- **Bounded activations**: When outputs must stay within a specific range\n- **Recurrent networks**: To prevent exploding activations during long sequences\n- **Quantized networks**: Where hard boundaries align well with fixed-point representations\n- **Regularization**: As a simple mechanism to limit activation magnitudes\n\n### Example Visualization\n\nFor default parameters ($\\text{min\\_val} = -1$, $\\text{max\\_val} = 1$):\n- Input $x = -5$: Output = $-1$ (clipped to minimum)\n- Input $x = 0.3$: Output = $0.3$ (unchanged)\n- Input $x = 3$: Output = $1$ (clipped to maximum)"
}