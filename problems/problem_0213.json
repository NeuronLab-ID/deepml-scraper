{
  "description": "SW1wbGVtZW50IEJheWVzaWFuIGluZmVyZW5jZSBmb3IgYSBiaW5vbWlhbCBsaWtlbGlob29kIHdpdGggYSBCZXRhIHByaW9yIChCZXRhLUJpbm9taWFsIGNvbmp1Z2F0ZSBtb2RlbCkuIEdpdmVuIHByaW9yIHBhcmFtZXRlcnMgKGFscGhhLCBiZXRhKSBhbmQgb2JzZXJ2ZWQgZGF0YSAoc3VjY2Vzc2VzLCB0cmlhbHMpLCBjb21wdXRlIHRoZSBwb3N0ZXJpb3IgZGlzdHJpYnV0aW9uIHBhcmFtZXRlcnMgYW5kIHBvc3RlcmlvciBtZWFuLiBUaGlzIGRlbW9uc3RyYXRlcyBob3cgcHJpb3IgYmVsaWVmcyBhcmUgdXBkYXRlZCB3aXRoIG9ic2VydmVkIGRhdGEgdXNpbmcgQmF5ZXMnIHRoZW9yZW0u",
  "id": "213",
  "test_cases": [
    {
      "test": "post_alpha, post_beta, post_mean = bayesian_inference_beta_binomial(1, 1, 7, 10); print(f\"{post_alpha:.1f},{post_beta:.1f},{post_mean:.4f}\")",
      "expected_output": "8.0,4.0,0.6667"
    },
    {
      "test": "post_alpha, post_beta, post_mean = bayesian_inference_beta_binomial(10, 2, 3, 10); print(f\"{post_mean:.4f}\")",
      "expected_output": "0.5909"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "prior_alpha=1, prior_beta=1, successes=7, trials=10",
    "output": "(8.0, 4.0, 0.6667)",
    "reasoning": "Prior: Beta(1,1) uniform. Data: 7 successes, 3 failures. Posterior: Beta(1+7, 1+3) = Beta(8, 4). Posterior mean = 8/(8+4) = 0.6667. The uniform prior is updated by the data to give posterior centered at observed proportion."
  },
  "category": "Statistics",
  "starter_code": "import numpy as np\n\ndef bayesian_inference_beta_binomial(prior_alpha: float, prior_beta: float, \n                                     successes: int, trials: int) -> tuple[float, float, float]:\n\t\"\"\"\n\tPerform Bayesian inference for Beta-Binomial model.\n\t\n\tArgs:\n\t\tprior_alpha: Alpha parameter of Beta prior\n\t\tprior_beta: Beta parameter of Beta prior\n\t\tsuccesses: Number of successes observed\n\t\ttrials: Total number of trials\n\t\n\tReturns:\n\t\tTuple of (posterior_alpha, posterior_beta, posterior_mean) where:\n\t\t- posterior_alpha: Updated alpha parameter\n\t\t- posterior_beta: Updated beta parameter\n\t\t- posterior_mean: Mean of posterior distribution\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Bayesian Inference for Beta-Binomial Model",
  "createdAt": "November 22, 2025 at 9:19:34â€¯PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement Bayesian inference for a binomial likelihood with a Beta prior (Beta-Binomial conjugate model). Given prior parameters (alpha, beta) and observed data (successes, trials), compute the posterior distribution parameters and posterior mean. This demonstrates how prior beliefs are updated with observed data using Bayes' theorem.",
  "learn_section_decoded": "### Bayesian Inference\n\nBayesian inference updates prior beliefs with observed data to obtain posterior beliefs using Bayes' theorem.\n\n#### Bayes' Theorem\n\n$$\nP(\\theta | \\text{data}) = \\frac{P(\\text{data} | \\theta) \\cdot P(\\theta)}{P(\\text{data})}\n$$\n\nIn words:\n$$\n\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}\n$$\n\n**Components**:\n- **Prior** $P(\\theta)$: Belief about $\\theta$ before seeing data\n- **Likelihood** $P(\\text{data} | \\theta)$: Probability of data given $\\theta$\n- **Posterior** $P(\\theta | \\text{data})$: Updated belief after seeing data\n- **Evidence** $P(\\text{data})$: Normalizing constant\n\n#### Beta-Binomial Model\n\nThis is a **conjugate model**: prior and posterior have the same family (Beta distribution).\n\n**Setup**:\n- Parameter: $\\theta$ = success probability (unknown)\n- Prior: $\\theta \\sim \\text{Beta}(\\alpha, \\beta)$\n- Data: $k$ successes in $n$ trials\n- Likelihood: Binomial$(n, \\theta)$\n- Posterior: $\\theta | \\text{data} \\sim \\text{Beta}(\\alpha', \\beta')$\n\n**Beta Distribution**:\n$$\n\\text{Beta}(\\alpha, \\beta) \\propto \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\n$$\n\n**Mean**: $E[\\theta] = \\frac{\\alpha}{\\alpha + \\beta}$\n\n#### Conjugate Update Formula\n\n**Prior**: $\\theta \\sim \\text{Beta}(\\alpha, \\beta)$\n\n**Data**: $k$ successes, $n-k$ failures\n\n**Posterior**: $\\theta | \\text{data} \\sim \\text{Beta}(\\alpha', \\beta')$\n\nWhere:\n$$\n\\begin{align}\n\\alpha' &= \\alpha + k \\\\\n\\beta' &= \\beta + (n-k)\n\\end{align}\n$$\n\n**Posterior mean**:\n$$\nE[\\theta | \\text{data}] = \\frac{\\alpha'}{\\alpha' + \\beta'} = \\frac{\\alpha + k}{\\alpha + \\beta + n}\n$$\n\n#### Intuition\n\n**Prior parameters as \"pseudo-counts\"**:\n- $\\alpha$ = prior successes\n- $\\beta$ = prior failures\n- Total prior \"observations\" = $\\alpha + \\beta$\n\n**Update rule**: Simply add observed counts to prior counts\n- New successes = $\\alpha + k$\n- New failures = $\\beta + (n-k)$\n\n**Posterior mean** is weighted average:\n$$\n\\text{Posterior mean} = \\frac{\\text{prior successes + data successes}}{\\text{prior total + data total}}\n$$\n\n#### Algorithm\n\n**Input**: Prior Beta$(\\alpha, \\beta)$, data $(k, n)$\n\n**Step 1**: Calculate failures\n$$\n\\text{failures} = n - k\n$$\n\n**Step 2**: Update posterior parameters\n$$\n\\begin{align}\n\\alpha' &= \\alpha + k \\\\\n\\beta' &= \\beta + \\text{failures}\n\\end{align}\n$$\n\n**Step 3**: Calculate posterior mean\n$$\n\\bar{\\theta} = \\frac{\\alpha'}{\\alpha' + \\beta'}\n$$\n\n**Output**: $(\\alpha', \\beta', \\bar{\\theta})$\n\n#### Example Calculation\n\n**Prior**: Beta(1, 1) - uniform prior (no preference)\n\n**Data**: 7 successes in 10 trials\n\n**Step 1**: Failures\n$$\n\\text{failures} = 10 - 7 = 3\n$$\n\n**Step 2**: Posterior parameters\n$$\n\\begin{align}\n\\alpha' &= 1 + 7 = 8 \\\\\n\\beta' &= 1 + 3 = 4\n\\end{align}\n$$\n\n**Step 3**: Posterior mean\n$$\n\\bar{\\theta} = \\frac{8}{8 + 4} = \\frac{8}{12} = 0.6667\n$$\n\n**Result**: Posterior is Beta(8, 4) with mean 0.6667\n\n**Interpretation**: After observing 7/10 successes, our best estimate is $\\theta \\approx 0.67$\n\n#### Special Priors\n\n**1. Uniform Prior: Beta(1, 1)**\n- No prior preference, all values of $\\theta$ equally likely\n- Posterior mean = MLE = $k/n$\n- \"Let the data speak\"\n\n**2. Jeffreys Prior: Beta(0.5, 0.5)**\n- Non-informative prior\n- Treats success and failure symmetrically\n\n**3. Symmetric Prior: Beta($\\alpha$, $\\alpha$)**\n- Prior belief centered at 0.5\n- Larger $\\alpha$ = stronger prior\n\n**4. Informative Prior: Beta(10, 2)**\n- Strong prior belief $\\theta \\approx 0.83$\n- Regularizes estimates toward prior\n\n#### Prior Strength vs Data\n\n**Weak prior** (e.g., Beta(1,1)):\n- Prior total = $\\alpha + \\beta = 2$\n- Data dominates with moderate sample size\n- Posterior $\\approx$ MLE\n\n**Strong prior** (e.g., Beta(20,20)):\n- Prior total = 40\n- Requires lots of data to override\n- Posterior regularized toward prior mean\n\n**Trade-off**:\n$$\n\\text{Posterior mean} = \\frac{\\alpha + k}{\\alpha + \\beta + n} = \\frac{\\text{prior}}{\\text{prior + data}} \\cdot \\frac{\\alpha}{\\alpha+\\beta} + \\frac{\\text{data}}{\\text{prior + data}} \\cdot \\frac{k}{n}\n$$\n\n#### Bayesian vs Frequentist\n\n**Maximum Likelihood (Frequentist)**:\n$$\n\\hat{\\theta}_{\\text{MLE}} = \\frac{k}{n}\n$$\n\n**Bayesian (with uniform prior)**:\n$$\n\\hat{\\theta}_{\\text{Bayes}} = \\frac{1 + k}{2 + n}\n$$\n\n**Differences**:\n- MLE can give $\\hat{\\theta} = 0$ or $1$ with extreme data\n- Bayesian estimate avoids extremes (regularization)\n- With large $n$, both converge\n\n**Example**: 0 successes in 10 trials\n- MLE: $\\hat{\\theta} = 0/10 = 0.0$\n- Bayes (uniform): $\\hat{\\theta} = 1/(1+1+10) = 0.083$\n\nBayesian estimate more reasonable with limited data.\n\n#### Properties\n\n**1. Sequential updating**: Can update posterior with new data iteratively\n$$\n\\text{New posterior} = \\text{Update}(\\text{old posterior, new data})\n$$\n\n**2. Conjugacy**: Posterior has same form as prior (convenient)\n\n**3. Regularization**: Prior acts as regularizer, prevents overfitting\n\n**4. Uncertainty quantification**: Full posterior distribution, not just point estimate\n\n#### Applications\n\n- **A/B testing**: Update conversion rate beliefs with test data\n- **Clinical trials**: Combine prior knowledge with trial results\n- **Quality control**: Update defect rate estimates\n- **Machine learning**: Bayesian optimization, Naive Bayes classifier"
}