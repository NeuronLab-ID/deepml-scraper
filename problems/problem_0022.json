{
  "description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBjb21wdXRlcyB0aGUgb3V0cHV0IG9mIHRoZSBzaWdtb2lkIGFjdGl2YXRpb24gZnVuY3Rpb24gZ2l2ZW4gYW4gaW5wdXQgdmFsdWUgei4gVGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gdGhlIG91dHB1dCByb3VuZGVkIHRvIGZvdXIgZGVjaW1hbCBwbGFjZXMu",
  "mdx_file": "43e88c36-680f-4fd4-a992-b170a4306dff.mdx",
  "tinygrad_difficulty": "easy",
  "tinygrad_starter_code": "ZnJvbSB0aW55Z3JhZC50ZW5zb3IgaW1wb3J0IFRlbnNvcgoKZGVmIHNpZ21vaWRfdGcoejogZmxvYXQpIC0+IGZsb2F0OgogICAgIiIiCiAgICBDb21wdXRlIHRoZSBzaWdtb2lkIGFjdGl2YXRpb24gZnVuY3Rpb24gdXNpbmcgdGlueWdyYWQuCiAgICBJbnB1dDoKICAgICAgLSB6OiBmbG9hdCBvciB0aW55Z3JhZCBUZW5zb3Igc2NhbGFyCiAgICBSZXR1cm5zOgogICAgICAtIHNpZ21vaWQoeikgYXMgUHl0aG9uIGZsb2F0IHJvdW5kZWQgdG8gNCBkZWNpbWFscy4KICAgICIiIgogICAgIyBZb3VyIGltcGxlbWVudGF0aW9uIGhlcmUKICAgIHBhc3MK",
  "test_cases": [
    {
      "test": "print(sigmoid(0))",
      "expected_output": "0.5"
    },
    {
      "test": "print(sigmoid(1))",
      "expected_output": "0.7311"
    }
  ],
  "code": "import math\n\ndef sigmoid(z: float) -> float:\n\t#Your code here\n\treturn result",
  "pytorch_difficulty": "easy",
  "likes": "0",
  "video": "https://youtu.be/DL_PVRD-NOg",
  "marimo_link": "https://open-deep-ml.github.io/deepml-notebooks/22/",
  "difficulty": "easy",
  "example": {
    "input": "z = 0",
    "output": "0.5",
    "reasoning": "The sigmoid function is defined as Ïƒ(z) = 1 / (1 + exp(-z)). For z = 0, exp(-0) = 1, hence the output is 1 / (1 + 1) = 0.5."
  },
  "dislikes": "0",
  "category": "Deep Learning",
  "starter_code": "import math\n\ndef sigmoid(z: float) -> float:\n\t#Your code here\n\treturn result",
  "learn_section": "CiMjIFVuZGVyc3RhbmRpbmcgdGhlIFNpZ21vaWQgQWN0aXZhdGlvbiBGdW5jdGlvbgoKVGhlIHNpZ21vaWQgYWN0aXZhdGlvbiBmdW5jdGlvbiBpcyBjcnVjaWFsIGluIG5ldXJhbCBuZXR3b3JrcywgZXNwZWNpYWxseSBmb3IgYmluYXJ5IGNsYXNzaWZpY2F0aW9uIHRhc2tzLiBJdCBtYXBzIGFueSByZWFsLXZhbHVlZCBudW1iZXIgaW50byB0aGUgaW50ZXJ2YWwgXCggKDAsIDEpIFwpLCBtYWtpbmcgaXQgdXNlZnVsIGZvciBtb2RlbGluZyBwcm9iYWJpbGl0eSBhcyBhbiBvdXRwdXQuCgojIyMgTWF0aGVtYXRpY2FsIERlZmluaXRpb24KVGhlIHNpZ21vaWQgZnVuY3Rpb24gaXMgbWF0aGVtYXRpY2FsbHkgZGVmaW5lZCBhczoKJCQKXHNpZ21hKHopID0gXGZyYWN7MX17MSArIGVeey16fX0KJCQKd2hlcmUgXCggeiBcKSBpcyB0aGUgaW5wdXQgdG8gdGhlIGZ1bmN0aW9uLgoKIyMjIENoYXJhY3RlcmlzdGljcwotICoqT3V0cHV0IFJhbmdlKio6IFRoZSBvdXRwdXQgaXMgYWx3YXlzIGJldHdlZW4gMCBhbmQgMS4KLSAqKlNoYXBlKio6IFRoZSBmdW5jdGlvbiBoYXMgYW4gIlMiIHNoYXBlZCBjdXJ2ZS4KLSAqKkdyYWRpZW50Kio6IFRoZSBncmFkaWVudCBpcyBoaWdoZXN0IG5lYXIgXCggeiA9IDAgXCkgYW5kIGRlY3JlYXNlcyBhcyBcKCB6IFwpIG1vdmVzIGF3YXkgZnJvbSAwIGluIGVpdGhlciBkaXJlY3Rpb24uCgpUaGUgc2lnbW9pZCBmdW5jdGlvbiBpcyBwYXJ0aWN1bGFybHkgdXNlZnVsIGZvciB0dXJuaW5nIGxvZ2l0cyAocmF3IHByZWRpY3Rpb24gdmFsdWVzKSBpbnRvIHByb2JhYmlsaXRpZXMgaW4gYmluYXJ5IGNsYXNzaWZpY2F0aW9uIG1vZGVscy4KCg==",
  "title": "Sigmoid Activation Function Understanding",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    },
    {
      "profile_link": "https://github.com/Selbl",
      "name": "Selbl"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "print(sigmoid(0))",
      "expected_output": "0.5"
    },
    {
      "test": "print(sigmoid(1))",
      "expected_output": "0.7311"
    }
  ],
  "tinygrad_test_cases": [
    {
      "test": "print(sigmoid_tg(0))",
      "expected_output": "0.5"
    },
    {
      "test": "print(sigmoid_tg(1))",
      "expected_output": "0.7311"
    }
  ],
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpkZWYgc2lnbW9pZCh6OiBmbG9hdCkgLT4gZmxvYXQ6CiAgICAiIiIKICAgIENvbXB1dGUgdGhlIHNpZ21vaWQgYWN0aXZhdGlvbiBmdW5jdGlvbi4KICAgIElucHV0OgogICAgICAtIHo6IGZsb2F0IG9yIHRvcmNoIHNjYWxhciB0ZW5zb3IKICAgIFJldHVybnM6CiAgICAgIC0gc2lnbW9pZCh6KSBhcyBQeXRob24gZmxvYXQgcm91bmRlZCB0byA0IGRlY2ltYWxzLgogICAgIiIiCiAgICAjIFlvdXIgaW1wbGVtZW50YXRpb24gaGVyZQogICAgcGFzcwo=",
  "description_decoded": "Write a Python function that computes the output of the sigmoid activation function given an input value z. The function should return the output rounded to four decimal places.",
  "learn_section_decoded": "\n## Understanding the Sigmoid Activation Function\n\nThe sigmoid activation function is crucial in neural networks, especially for binary classification tasks. It maps any real-valued number into the interval \\( (0, 1) \\), making it useful for modeling probability as an output.\n\n### Mathematical Definition\nThe sigmoid function is mathematically defined as:\n$$\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n$$\nwhere \\( z \\) is the input to the function.\n\n### Characteristics\n- **Output Range**: The output is always between 0 and 1.\n- **Shape**: The function has an \"S\" shaped curve.\n- **Gradient**: The gradient is highest near \\( z = 0 \\) and decreases as \\( z \\) moves away from 0 in either direction.\n\nThe sigmoid function is particularly useful for turning logits (raw prediction values) into probabilities in binary classification models.\n\n",
  "tinygrad_starter_code_decoded": "from tinygrad.tensor import Tensor\n\ndef sigmoid_tg(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid activation function using tinygrad.\n    Input:\n      - z: float or tinygrad Tensor scalar\n    Returns:\n      - sigmoid(z) as Python float rounded to 4 decimals.\n    \"\"\"\n    # Your implementation here\n    pass\n"
}