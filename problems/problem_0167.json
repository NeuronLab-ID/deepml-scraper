{
  "description": "V3JpdGUgYSBmdW5jdGlvbiB0aGF0IGNvbXB1dGVzIHRoZSBkaXNjb3VudGVkIHJldHVybiAkR190ID0gXHN1bV97az0wfV5caW5mdHkgXGdhbW1hXmsgUl97dCtrKzF9JCBmb3IgYSBnaXZlbiBzZXF1ZW5jZSBvZiByZXdhcmRzIGFuZCBkaXNjb3VudCBmYWN0b3IgZ2FtbWEuIFRoaXMgcXVhbnRpdHkgY29ycmVzcG9uZHMgdG8gdGhlIGV4cGVjdGVkIHJldHVybiAkdl9ccGkocykkIGluIHJlaW5mb3JjZW1lbnQgbGVhcm5pbmcsIGFzIGRlZmluZWQgYnkgdGhlIGVxdWF0aW9uIGluIHRoZSBpbWFnZS4gT25seSB1c2UgTnVtUHku",
  "id": "167",
  "test_cases": [
    {
      "test": "import numpy as np\nrewards = [1, 2, 3, 4]\ngamma = 0.9\nprint(round(discounted_return(rewards, gamma), 4))",
      "expected_output": "8.146"
    },
    {
      "test": "import numpy as np\nrewards = [10, 0, 0, 0, 0]\ngamma = 0.5\nprint(round(discounted_return(rewards, gamma), 4))",
      "expected_output": "10.0"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "rewards = [1, 2, 3, 4]\ngamma = 0.9\nprint(discounted_return(rewards, gamma))",
    "output": "8.146",
    "reasoning": "G = 1 + 0.9*2 + 0.9^2*3 + 0.9^3*4 = 1 + 1.8 + 2.43 + 2.916 = 8.146"
  },
  "category": "Reinforcement Learning",
  "starter_code": "import numpy as np\n\ndef discounted_return(rewards, gamma):\n    \"\"\"\n    Compute the discounted return for a given list of rewards.\n    Args:\n      rewards (list of float): sequence of rewards R_{t+1}, R_{t+2}, ...\n      gamma (float): discount factor (0 <= gamma <= 1)\n    Returns:\n      float: discounted return G_t\n    \"\"\"\n    # Your code here\n    pass",
  "title": "Calculate the Discounted Return for a Given Trajectory",
  "learn_section": "IyAqKkRpc2NvdW50ZWQgUmV0dXJuIGluIFJlaW5mb3JjZW1lbnQgTGVhcm5pbmcqKgoKVGhlICoqZGlzY291bnRlZCByZXR1cm4qKiAkR190JCBpcyB0aGUgdG90YWwgZXhwZWN0ZWQgc3VtIG9mIHJld2FyZHMgdGhhdCBhbiBhZ2VudCB3aWxsIHJlY2VpdmUgaW4gdGhlIGZ1dHVyZSwgd2hlcmUgZnV0dXJlIHJld2FyZHMgYXJlIG11bHRpcGxpZWQgYnkgYSBkaXNjb3VudCBmYWN0b3IgJFxnYW1tYSQgKCQwIFxsZXEgXGdhbW1hIFxsZXEgMSQpLiBUaGlzIGNvbmNlcHQgaXMgY2VudHJhbCBpbiByZWluZm9yY2VtZW50IGxlYXJuaW5nIGFuZCBpcyBjYXB0dXJlZCBieToKCiQkCkdfdCA9IFxzdW1fe2s9MH1eXGluZnR5IFxnYW1tYV5rIFJfe3QraysxfQokJAoKLSAkR190JDogRGlzY291bnRlZCByZXR1cm4gYXQgdGltZSAkdCQKLSAkUl97dCtrKzF9JDogUmV3YXJkIHJlY2VpdmVkICRrJCBzdGVwcyBpbnRvIHRoZSBmdXR1cmUKLSAkXGdhbW1hJDogRGlzY291bnQgZmFjdG9yIChjb250cm9scyBob3cgbXVjaCB0aGUgYWdlbnQgY2FyZXMgYWJvdXQgZnV0dXJlIHJld2FyZHMpCgojIyAqKldoeSBEaXNjb3VudD8qKgotIEVuc3VyZXMgdGhlIHN1bSBjb252ZXJnZXMgKGZvciBpbmZpbml0ZSBob3Jpem9ucykKLSBFbmNvdXJhZ2VzIHRoZSBhZ2VudCB0byBwcmVmZXIgaW1tZWRpYXRlIHJld2FyZHMgb3ZlciBkaXN0YW50IHJld2FyZHMgKGlmICRcZ2FtbWEgPCAxJCkKLSBNb2RlbHMgdW5jZXJ0YWludHkgb3IgdGltZSBwcmVmZXJlbmNlCgojIyAqKkNvbm5lY3Rpb24gdG8gVmFsdWUgRnVuY3Rpb24qKgpUaGUgc3RhdGUtdmFsdWUgZnVuY3Rpb24gJHZfXHBpKHMpJCB1bmRlciBwb2xpY3kgJFxwaSQgaXMgdGhlIGV4cGVjdGVkIGRpc2NvdW50ZWQgcmV0dXJuIHdoZW4gc3RhcnRpbmcgaW4gc3RhdGUgJHMkIGFuZCBmb2xsb3dpbmcgJFxwaSQ6CgokJAp2X1xwaShzKSA9IFxtYXRoYmJ7RX1fXHBpW0dfdCB8IFNfdCA9IHNdCiQkCgpUaGlzIG1lYW5zIHRoZSB2YWx1ZSBvZiBhIHN0YXRlIGlzIHRoZSBleHBlY3RlZCBzdW0gb2YgZGlzY291bnRlZCBmdXR1cmUgcmV3YXJkcyBzdGFydGluZyBmcm9tIHRoYXQgc3RhdGUuCgojIyAqKlN1bW1hcnkqKgotIERpc2NvdW50ZWQgcmV0dXJuIHF1YW50aWZpZXMgdGhlIGZ1dHVyZSByZXdhcmQgYW4gYWdlbnQgZXhwZWN0cywgYWNjb3VudGluZyBmb3IgdGltZSBhbmQgdW5jZXJ0YWludHkKLSBJdCBpcyBmb3VuZGF0aW9uYWwgZm9yIGRlZmluaW5nIG9wdGltYWxpdHkgYW5kIGxlYXJuaW5nIGluIHJlaW5mb3JjZW1lbnQgbGVhcm5pbmcKLSBJbXBsZW1lbnRpbmcgdGhlIHN1bSBpcyBvZnRlbiB0aGUgZmlyc3Qgc3RlcCB0byB1bmRlcnN0YW5kaW5nIHBvbGljeSBldmFsdWF0aW9u",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "description_decoded": "Write a function that computes the discounted return $G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$ for a given sequence of rewards and discount factor gamma. This quantity corresponds to the expected return $v_\\pi(s)$ in reinforcement learning, as defined by the equation in the image. Only use NumPy.",
  "learn_section_decoded": "# **Discounted Return in Reinforcement Learning**\n\nThe **discounted return** $G_t$ is the total expected sum of rewards that an agent will receive in the future, where future rewards are multiplied by a discount factor $\\gamma$ ($0 \\leq \\gamma \\leq 1$). This concept is central in reinforcement learning and is captured by:\n\n$$\nG_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\n$$\n\n- $G_t$: Discounted return at time $t$\n- $R_{t+k+1}$: Reward received $k$ steps into the future\n- $\\gamma$: Discount factor (controls how much the agent cares about future rewards)\n\n## **Why Discount?**\n- Ensures the sum converges (for infinite horizons)\n- Encourages the agent to prefer immediate rewards over distant rewards (if $\\gamma < 1$)\n- Models uncertainty or time preference\n\n## **Connection to Value Function**\nThe state-value function $v_\\pi(s)$ under policy $\\pi$ is the expected discounted return when starting in state $s$ and following $\\pi$:\n\n$$\nv_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]\n$$\n\nThis means the value of a state is the expected sum of discounted future rewards starting from that state.\n\n## **Summary**\n- Discounted return quantifies the future reward an agent expects, accounting for time and uncertainty\n- It is foundational for defining optimality and learning in reinforcement learning\n- Implementing the sum is often the first step to understanding policy evaluation"
}