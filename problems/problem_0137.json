{
  "description": "Q3JlYXRlIGEgZnVuY3Rpb24gYGRlbnNlX25ldF9ibG9ja2AgdGhhdCBwZXJmb3JtcyB0aGUgZm9yd2FyZCBwYXNzIG9mIGEgKipEZW5zZU5ldCBkZW5zZSBibG9jayoqIG9uIGEgYmF0Y2ggb2YgaW1hZ2VzIHN0b3JlZCBpbiBhbiAqKk5IV0MqKiBOdW1QeSB0ZW5zb3IgYGlucHV0X2RhdGFgIChzaGFwZSBgKE4sIEgsIFcsIEMwKWApLiBUaGUgYmxvY2sgbXVzdCBydW4gYG51bV9sYXllcnNgIGl0ZXJhdGlvbnM7IGF0IGVhY2ggaXRlcmF0aW9uIGl0IHNob3VsZCAoaSkgYXBwbHkgKipSZUxVKiogdG8gdGhlIHJ1bm5pbmcgZmVhdHVyZSB0ZW5zb3IsIChpaSkgY29udm9sdmUgaXQgd2l0aCB0aGUgY29ycmVzcG9uZGluZyBrZXJuZWwgZnJvbSBga2VybmVsc2AgKHVzaW5nIHN0cmlkZSAxLCBubyBiaWFzLCBhbmQgc3ltbWV0cmljIHplcm8tcGFkZGluZyBzbyB0aGF0IGBIYCBhbmQgYFdgIGFyZSBwcmVzZXJ2ZWQpLCBhbmQgKGlpaSkgY29uY2F0ZW5hdGUgdGhlIGNvbnZvbHV0aW9uIG91dHB1dCAod2hvc2UgY2hhbm5lbCBjb3VudCBlcXVhbHMgYGdyb3d0aF9yYXRlYCkgdG8gdGhlIHJ1bm5pbmcgdGVuc29yIGFsb25nIHRoZSBjaGFubmVsIGF4aXMuIEV2ZXJ5IGtlcm5lbCBga2VybmVsc1tsXWAgdGhlcmVmb3JlIGhhcyBzaGFwZSBgKGtoLCBrdywgQzAgKyBsIHggZ3Jvd3RoX3JhdGUsIGdyb3d0aF9yYXRlKWAsIHdoZXJlIGAoa2gsIGt3KWAgZXF1YWxzIGBrZXJuZWxfc2l6ZWAgKGRlZmF1bHQgYCgzLCAzKWApLiBBZnRlciB0aGUgZmluYWwgbGF5ZXIgdGhlIGZ1bmN0aW9uIG11c3QgcmV0dXJuIGEgdGVuc29yIG9mIHNoYXBlIGAoTiwgSCwgVywgQzAgKyBudW1fbGF5ZXJzIHggZ3Jvd3RoX3JhdGUpYC4gSWYgYW55IGtlcm5lbCdzIGlucHV0LWNoYW5uZWwgZGltZW5zaW9uIGRvZXMgbm90IG1hdGNoIHRoZSBjdXJyZW50IGZlYXR1cmUtbWFwIGNoYW5uZWxzLCB0aGUgZnVuY3Rpb24gc2hvdWxkIHJhaXNlIGEgYFZhbHVlRXJyb3JgLg==",
  "id": "137",
  "test_cases": [
    {
      "test": "import numpy as np\nnp.random.seed(42)\nX = np.random.randn(1, 1, 1, 2)\nkernels = [np.random.randn(3, 3, 2 + i*1, 1) * 0.01 for i in range(2)]\nprint(dense_net_block(X, 2, 1, kernels))",
      "expected_output": "[[[[ 4.96714153e-01, -1.38264301e-01, -2.30186127e-03, -6.70426255e-05]]]]"
    },
    {
      "test": "import numpy as np\n\nnp.random.seed(42)\nX = np.random.randn(1, 2, 3, 2)\nkernels = [np.random.randn(3, 3, 2 + i*1, 1) * 0.01 for i in range(2)]\nprint(dense_net_block(X, 2, 1, kernels))",
      "expected_output": "[[[[ 0.49671415, -0.1382643 , -0.0308579 , -0.01845547], [ 0.64768854, 1.52302986, -0.0041634 , -0.0161227 ], [-0.23415337, -0.23413696, -0.02678915, 0.00295656]], [[ 1.57921282, 0.76743473, 0.00334109, -0.04043312], [-0.46947439, 0.54256004, -0.04493715, 0.00983633], [-0.46341769, -0.46572975, -0.03523526, 0.02832019]]]]"
    }
  ],
  "difficulty": "hard",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "X = np.random.randn(1, 2, 2, 1); kernels = [np.random.randn(3, 3, 2 + i*1, 1) * 0.01 for i in range(2)]; print(dense_net_block(X, 2, 1, kernels))",
    "reasoning": "Each dense block layer concatenates its output to the existing feature maps, expanding the number of output channels by 1 per layer (the growth rate). After 2 layers, the original 2 channels become 4.",
    "output": "[[[[ 4.96714153e-01, -1.38264301e-01, -2.30186127e-03, -6.70426255e-05]]]]"
  },
  "category": "Deep Learning",
  "starter_code": "import numpy as np\n\ndef dense_net_block(input_data, num_layers, growth_rate, kernels, kernel_size=(3, 3)):\n    # Your code here\n    pass",
  "title": "Implement a Dense Block with 2D Convolutions",
  "learn_section": "IyMgVW5kZXJzdGFuZGluZyBEZW5zZSBCbG9ja3MgYW5kIDJEIENvbnZvbHV0aW9ucwoKRGVuc2UgYmxvY2tzIGFyZSBhIGtleSBpbm5vdmF0aW9uIGluIHRoZSBEZW5zZU5ldCBhcmNoaXRlY3R1cmUuIEVhY2ggbGF5ZXIgcmVjZWl2ZXMgaW5wdXQgZnJvbSAqKmFsbCoqIHByZXZpb3VzIGxheWVycywgbGVhZGluZyB0byByaWNoIGZlYXR1cmUgcmV1c2UgYW5kIGVmZmljaWVudCBncmFkaWVudCBmbG93LgoKIyMjIERlbnNlIEJsb2NrIENvbmNlcHQKRm9yIGEgZGVuc2UgYmxvY2s6Ci0gKipFYWNoIGxheWVyKio6IEFwcGxpZXMgUmVMVSwgdGhlbiAyRCBjb252b2x1dGlvbiwgYW5kIHRoZW4gY29uY2F0ZW5hdGVzIHRoZSBvdXRwdXQgdG8gcHJldmlvdXMgZmVhdHVyZXMuCi0gTWF0aGVtYXRpY2FsbHk6CiQkCnhfbCA9IEhfbChbeF8wLCB4XzEsIFxsZG90cywgeF97bC0xfV0pCiQkCndoZXJlICRIX2woXGNkb3QpJCBpcyB0aGUgY29udm9sdXRpb24gYW5kIGFjdGl2YXRpb24gb3BlcmF0aW9ucy4KCiMjIyAyRCBDb252b2x1dGlvbiBCYXNpY3MKQSAyRCBjb252b2x1dGlvbiBhdCBhIHBvc2l0aW9uICQoaSwgaikkIGZvciBpbnB1dCAkWCQgYW5kIGtlcm5lbCAkSyQgaXM6CiQkCllbaSwgal0gPSBcc3VtX3ttPTB9XntrX2ggLSAxfSBcc3VtX3tuPTB9XntrX3cgLSAxfSBYW2kgKyBtLCBqICsgbl0gXGNkb3QgS1ttLCBuXQokJAoKIyMjIFBhZGRpbmcgdG8gUHJlc2VydmUgU3BhdGlhbCBEaW1lbnNpb25zClRvIHByZXNlcnZlIGhlaWdodCBhbmQgd2lkdGg6CiQkClx0ZXh0e3BhZGRpbmd9ID0gXGZyYWN7ayAtIDF9ezJ9CiQkCgojIyMgRGVuc2UgQmxvY2sgR3Jvd3RoCi0gRWFjaCBsYXllciBhZGRzICRcdGV4dHtncm93dGggcmF0ZX0kIGNoYW5uZWxzLgotIEFmdGVyICRMJCBsYXllcnMsIHRvdGFsIGNoYW5uZWxzID0gaW5wdXQgY2hhbm5lbHMgKyAkTCBcdGltZXMgXHRleHR7Z3Jvd3RoIHJhdGV9JC4KCiMjIyBQdXR0aW5nIEl0IEFsbCBUb2dldGhlcgox77iP4oOjIFN0YXJ0IHdpdGggYW4gaW5wdXQgdGVuc29yLiAgCjLvuI/ig6MgUmVwZWF0IGZvciAkXHRleHR7bnVtIGxheWVyc30kOgotIEFwcGx5IFJlTFUgYWN0aXZhdGlvbi4KLSBBcHBseSAyRCBjb252b2x1dGlvbiAod2l0aCBwYWRkaW5nKS4KLSBDb25jYXRlbmF0ZSB0aGUgb3V0cHV0IGFsb25nIHRoZSBjaGFubmVsIGRpbWVuc2lvbi4KCkJ5IHVuZGVyc3RhbmRpbmcgdGhlc2UgY29yZSBwcmluY2lwbGVzLCB5b3XigJlyZSByZWFkeSB0byBidWlsZCB0aGUgZGVuc2UgYmxvY2sgZnVuY3Rpb24h",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe"
    }
  ],
  "description_decoded": "Create a function `dense_net_block` that performs the forward pass of a **DenseNet dense block** on a batch of images stored in an **NHWC** NumPy tensor `input_data` (shape `(N, H, W, C0)`). The block must run `num_layers` iterations; at each iteration it should (i) apply **ReLU** to the running feature tensor, (ii) convolve it with the corresponding kernel from `kernels` (using stride 1, no bias, and symmetric zero-padding so that `H` and `W` are preserved), and (iii) concatenate the convolution output (whose channel count equals `growth_rate`) to the running tensor along the channel axis. Every kernel `kernels[l]` therefore has shape `(kh, kw, C0 + l x growth_rate, growth_rate)`, where `(kh, kw)` equals `kernel_size` (default `(3, 3)`). After the final layer the function must return a tensor of shape `(N, H, W, C0 + num_layers x growth_rate)`. If any kernel's input-channel dimension does not match the current feature-map channels, the function should raise a `ValueError`.",
  "learn_section_decoded": "## Understanding Dense Blocks and 2D Convolutions\n\nDense blocks are a key innovation in the DenseNet architecture. Each layer receives input from **all** previous layers, leading to rich feature reuse and efficient gradient flow.\n\n### Dense Block Concept\nFor a dense block:\n- **Each layer**: Applies ReLU, then 2D convolution, and then concatenates the output to previous features.\n- Mathematically:\n$$\nx_l = H_l([x_0, x_1, \\ldots, x_{l-1}])\n$$\nwhere $H_l(\\cdot)$ is the convolution and activation operations.\n\n### 2D Convolution Basics\nA 2D convolution at a position $(i, j)$ for input $X$ and kernel $K$ is:\n$$\nY[i, j] = \\sum_{m=0}^{k_h - 1} \\sum_{n=0}^{k_w - 1} X[i + m, j + n] \\cdot K[m, n]\n$$\n\n### Padding to Preserve Spatial Dimensions\nTo preserve height and width:\n$$\n\\text{padding} = \\frac{k - 1}{2}\n$$\n\n### Dense Block Growth\n- Each layer adds $\\text{growth rate}$ channels.\n- After $L$ layers, total channels = input channels + $L \\times \\text{growth rate}$.\n\n### Putting It All Together\n1️⃣ Start with an input tensor.  \n2️⃣ Repeat for $\\text{num layers}$:\n- Apply ReLU activation.\n- Apply 2D convolution (with padding).\n- Concatenate the output along the channel dimension.\n\nBy understanding these core principles, you’re ready to build the dense block function!"
}