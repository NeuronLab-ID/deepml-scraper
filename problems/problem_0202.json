{
  "description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gY29tcHV0ZSB0aGUgSmFjb2JpYW4gbWF0cml4IG9mIGEgdmVjdG9yLXZhbHVlZCBmdW5jdGlvbiB1c2luZyBudW1lcmljYWwgZGlmZmVyZW50aWF0aW9uLiBUaGUgSmFjb2JpYW4gbWF0cml4IGNvbnRhaW5zIGFsbCBmaXJzdC1vcmRlciBwYXJ0aWFsIGRlcml2YXRpdmVzIG9mIGEgZnVuY3Rpb24gZjogUl5uIC0+IFJebS4gR2l2ZW4gYSBmdW5jdGlvbiBmIGFuZCBhIHBvaW50IHgsIGFwcHJveGltYXRlIGVhY2ggcGFydGlhbCBkZXJpdmF0aXZlIHVzaW5nIGZpbml0ZSBkaWZmZXJlbmNlcyBhbmQgcmV0dXJuIHRoZSBtIHggbiBKYWNvYmlhbiBtYXRyaXgu",
  "id": "202",
  "test_cases": [
    {
      "test": "import numpy as np; J = jacobian_matrix(lambda x: [2*x[0] + 3*x[1], x[0] - x[1]], [1, 2]); print([[round(val, 4) for val in row] for row in J])",
      "expected_output": "[[2.0, 3.0], [1.0, -1.0]]"
    },
    {
      "test": "import numpy as np; J = jacobian_matrix(lambda x: [x[0]**2, x[0]*x[1], x[1]**2], [2, 3]); print([[round(val, 4) for val in row] for row in J])",
      "expected_output": "[[4.0, 0.0], [3.0, 2.0], [0.0, 6.0]]"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "f(x, y) = [x^2, xy, y^2], x = [2, 3]",
    "output": "[[4.0, 0.0], [3.0, 2.0], [0.0, 6.0]]",
    "reasoning": "The Jacobian is a 3x2 matrix. First row: $\\frac{\\partial(x^2)}{\\partial x} = 2x = 4$, $\\frac{\\partial(x^2)}{\\partial y} = 0$. Second row: $\\frac{\\partial(xy)}{\\partial x} = y = 3$, $\\frac{\\partial(xy)}{\\partial y} = x = 2$. Third row: $\\frac{\\partial(y^2)}{\\partial x} = 0$, $\\frac{\\partial(y^2)}{\\partial y} = 2y = 6$."
  },
  "category": "Calculus",
  "starter_code": "import numpy as np\n\ndef jacobian_matrix(f, x: list[float], h: float = 1e-5) -> list[list[float]]:\n\t\"\"\"\n\tCompute the Jacobian matrix using numerical differentiation.\n\t\n\tArgs:\n\t\tf: Function that takes a list and returns a list\n\t\tx: Point at which to evaluate the Jacobian\n\t\th: Step size for finite differences\n\t\n\tReturns:\n\t\tJacobian matrix as list of lists\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Jacobian Matrix Calculation",
  "createdAt": "November 15, 2025 at 9:52:59â€¯PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement a function to compute the Jacobian matrix of a vector-valued function using numerical differentiation. The Jacobian matrix contains all first-order partial derivatives of a function f: R^n -> R^m. Given a function f and a point x, approximate each partial derivative using finite differences and return the m x n Jacobian matrix.",
  "learn_section_decoded": "### Understanding the Jacobian Matrix\n\nThe Jacobian matrix is a fundamental concept in multivariable calculus that generalizes the derivative to vector-valued functions. It captures how a function transforms space locally and is essential in optimization, robotics, and numerical methods.\n\n#### Definition\n\nFor a function $\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^m$ that maps an n-dimensional input to an m-dimensional output:\n\n$$\n\\mathbf{f}(\\mathbf{x}) = \\begin{pmatrix}\nf_1(x_1, \\ldots, x_n) \\\\\nf_2(x_1, \\ldots, x_n) \\\\\n\\vdots \\\\\nf_m(x_1, \\ldots, x_n)\n\\end{pmatrix}\n$$\n\nThe Jacobian matrix $J$ is the $m \\times n$ matrix of all first-order partial derivatives:\n\n$$\nJ = \\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n\\end{pmatrix}\n$$\n\nEach entry $J_{ij}$ represents:\n\n$$\nJ_{ij} = \\frac{\\partial f_i}{\\partial x_j}\n$$\n\nThis tells us how the $i$-th output component changes with respect to the $j$-th input component.\n\n#### Geometric Interpretation\n\nThe Jacobian matrix represents the best linear approximation of a function near a point. For small displacements $\\Delta\\mathbf{x}$ from point $\\mathbf{x}$:\n\n$$\n\\mathbf{f}(\\mathbf{x} + \\Delta\\mathbf{x}) \\approx \\mathbf{f}(\\mathbf{x}) + J(\\mathbf{x}) \\Delta\\mathbf{x}\n$$\n\nThis is the multivariate generalization of the linear approximation $f(x + \\Delta x) \\approx f(x) + f'(x)\\Delta x$.\n\n#### Example: Quadratic Functions\n\nConsider $\\mathbf{f}(x, y) = [x^2, xy, y^2]$ evaluated at point $(2, 3)$.\n\n**Computing partial derivatives**:\n\n$$\n\\frac{\\partial f_1}{\\partial x} = \\frac{\\partial(x^2)}{\\partial x} = 2x \\bigg|_{(2,3)} = 4\n$$\n\n$$\n\\frac{\\partial f_1}{\\partial y} = \\frac{\\partial(x^2)}{\\partial y} = 0\n$$\n\n$$\n\\frac{\\partial f_2}{\\partial x} = \\frac{\\partial(xy)}{\\partial x} = y \\bigg|_{(2,3)} = 3\n$$\n\n$$\n\\frac{\\partial f_2}{\\partial y} = \\frac{\\partial(xy)}{\\partial y} = x \\bigg|_{(2,3)} = 2\n$$\n\n$$\n\\frac{\\partial f_3}{\\partial x} = \\frac{\\partial(y^2)}{\\partial x} = 0\n$$\n\n$$\n\\frac{\\partial f_3}{\\partial y} = \\frac{\\partial(y^2)}{\\partial y} = 2y \\bigg|_{(2,3)} = 6\n$$\n\n**Jacobian matrix**:\n\n$$\nJ = \\begin{pmatrix}\n4 & 0 \\\\\n3 & 2 \\\\\n0 & 6\n\\end{pmatrix}\n$$\n\n#### Numerical Approximation\n\nWhen analytical derivatives are unavailable or impractical, we use finite differences to approximate partial derivatives:\n\n$$\n\\frac{\\partial f_i}{\\partial x_j} \\approx \\frac{f_i(x_1, \\ldots, x_j + h, \\ldots, x_n) - f_i(x_1, \\ldots, x_j, \\ldots, x_n)}{h}\n$$\n\nThis is called the forward difference approximation. For small $h$ (typically $10^{-5}$ to $10^{-8}$), this approximates the true derivative with error $O(h)$.\n\n**Central difference** (more accurate but requires more function evaluations):\n\n$$\n\\frac{\\partial f_i}{\\partial x_j} \\approx \\frac{f_i(\\ldots, x_j + h, \\ldots) - f_i(\\ldots, x_j - h, \\ldots)}{2h}\n$$\n\nThis has error $O(h^2)$, providing better accuracy for the same step size.\n\n#### Special Cases\n\n**Gradient (n inputs, 1 output)**:\n\nWhen $m = 1$, the Jacobian is a $1 \\times n$ row vector, which is the transpose of the gradient:\n\n$$\nJ = \\nabla f^T = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} & \\frac{\\partial f}{\\partial x_2} & \\cdots & \\frac{\\partial f}{\\partial x_n} \\end{pmatrix}\n$$\n\n**Derivative (1 input, m outputs)**:\n\nWhen $n = 1$, the Jacobian is an $m \\times 1$ column vector:\n\n$$\nJ = \\begin{pmatrix} \\frac{df_1}{dx} \\\\ \\frac{df_2}{dx} \\\\ \\vdots \\\\ \\frac{df_m}{dx} \\end{pmatrix}\n$$\n\n**Scalar derivative (1 input, 1 output)**:\n\nWhen both $m = 1$ and $n = 1$, the Jacobian reduces to the ordinary derivative: $J = f'(x)$.\n\n#### The Jacobian Determinant\n\nWhen the Jacobian is square ($m = n$), its determinant $\\det(J)$ has important meaning:\n\n- **Volume scaling**: $|\\det(J)|$ represents the local volume scaling factor of the transformation\n- **Invertibility**: $\\det(J) \\neq 0$ means the function is locally invertible near that point (Inverse Function Theorem)\n- **Change of variables**: In integration, $\\det(J)$ appears in change-of-variable formulas:\n\n$$\n\\int_{\\mathbf{f}(D)} g(\\mathbf{y}) d\\mathbf{y} = \\int_D g(\\mathbf{f}(\\mathbf{x})) |\\det(J(\\mathbf{x}))| d\\mathbf{x}\n$$\n\n#### Applications\n\n**Newton's Method for Systems**: To solve $\\mathbf{f}(\\mathbf{x}) = \\mathbf{0}$, iterate:\n\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - J(\\mathbf{x}_k)^{-1} \\mathbf{f}(\\mathbf{x}_k)\n$$\n\n**Backpropagation in Neural Networks**: The Jacobian represents how network outputs change with respect to parameters, enabling gradient-based optimization via the chain rule.\n\n**Robotics**: The Jacobian maps joint velocities to end-effector velocities in manipulators:\n\n$$\n\\mathbf{v}_{end} = J(\\mathbf{q}) \\dot{\\mathbf{q}}\n$$\n\nWhere $\\mathbf{q}$ are joint angles and $\\mathbf{v}_{end}$ is end-effector velocity.\n\n**Sensitivity Analysis**: The Jacobian quantifies how sensitive outputs are to input perturbations, crucial for understanding model behavior and uncertainty propagation.\n\n#### The Chain Rule\n\nFor composite functions $\\mathbf{h}(\\mathbf{x}) = \\mathbf{g}(\\mathbf{f}(\\mathbf{x}))$, the chain rule states:\n\n$$\nJ_{\\mathbf{h}}(\\mathbf{x}) = J_{\\mathbf{g}}(\\mathbf{f}(\\mathbf{x})) \\cdot J_{\\mathbf{f}}(\\mathbf{x})\n$$\n\nThis matrix multiplication is the foundation of automatic differentiation and backpropagation algorithms.\n\n#### Computational Considerations\n\nComputing the Jacobian numerically requires $n$ function evaluations (one per column). For functions $\\mathbb{R}^n \\to \\mathbb{R}^m$, this gives complexity:\n\n- **Time**: $O(n \\cdot T_f)$ where $T_f$ is the time to evaluate $\\mathbf{f}$\n- **Space**: $O(mn)$ to store the Jacobian\n\nFor large $n$, automatic differentiation techniques can compute Jacobian-vector products more efficiently than constructing the full matrix."
}