{
  "description": "UHJvYmxlbSBTdGF0ZW1lbnQ6IFRhc2sgaXMgdG8gaW1wbGVtZW50IEdhdXNzaWFuUHJvY2Vzc1JlZ3Jlc3Npb24gY2xhc3Mgd2hpY2ggaXMgYSBndWFzc2lhbiBwcm9jZXNzIG1vZGVsIGZvciBwcmVkaWN0aW9uIHJlZ3Jlc3Npb24gcHJvYmxlbXMu",
  "id": "186",
  "test_cases": [
    {
      "test": "import numpy as np\ngp = GaussianProcessRegression(kernel='rbf', kernel_params={'sigma': 1.0, 'length_scale': 1.0}, noise=1e-8)\nX_train = np.array([[0], [2.5], [5.0], [7.5], [10.0]])\ny_train = np.sin(X_train).ravel()\ngp.fit(X_train, y_train)\nX_test = np.array([[1.25]])\nmu = gp.predict(X_test)\nprint(f\"{mu[0]:.4f}\")",
      "expected_output": "0.2814"
    },
    {
      "test": "import numpy as np\ngp = GaussianProcessRegression(kernel='rbf', kernel_params={'sigma': 1.0, 'length_scale': 1.0}, noise=1e-8)\nX_train = np.array([[0], [2.5], [5.0], [7.5], [10.0]])\ny_train = np.sin(X_train).ravel()\ngp.fit(X_train, y_train)\nX_test = np.array([[1.25]])\nmu, std = gp.predict(X_test, return_std=True)\nprint(f\"mu={mu[0]:.4f}, std={std[0]:.4f}\")",
      "expected_output": "mu=0.2814, std=0.7734"
    }
  ],
  "difficulty": "hard",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "import numpy as np\ngp = GaussianProcessRegression(kernel='linear', kernel_params={'sigma_b': 0.0, 'sigma_v': 1.0}, noise=1e-8)\nX_train = np.array([[1], [2], [4]])\ny_train = np.array([3, 5, 9])\ngp.fit(X_train, y_train)\nX_test = np.array([[3.0]])\nmu = gp.predict(X_test)\nprint(f\"{mu[0]:.4f}\")",
    "output": "7.0000",
    "reasoning": "A Gaussian Process with a linear kernel is trained on perfectly linear data that follows the function y = 2x + 1. When asked to predict the value at x=3, the model perfectly interpolates the linear function it has learned, resulting in a prediction of 2*3 + 1 = 7. The near-zero noise ensures the prediction is exact."
  },
  "category": "Machine Learning",
  "starter_code": "import math  # ---------------------------------------- utf-8 encoding ---------------------------------\n\n# This file contains Gaussian Process implementation.\nimport numpy as np\nimport math\n\n\ndef matern_kernel(x: np.ndarray, x_prime: np.ndarray, length_scale=1.0, nu=1.5):\n    pass\n\n\ndef rbf_kernel(x: np.ndarray, x_prime, sigma=1.0, length_scale=1.0):\n    pass\n\n\ndef periodic_kernel(\n    x: np.ndarray, x_prime: np.ndarray, sigma=1.0, length_scale=1.0, period=1.0\n):\n    pass\n\n\ndef linear_kernel(x: np.ndarray, x_prime: np.ndarray, sigma_b=1.0, sigma_v=1.0):\n    pass\n\n\ndef rational_quadratic_kernel(\n    x: np.ndarray, x_prime: np.ndarray, sigma=1.0, length_scale=1.0, alpha=1.0\n):\n    pass\n\n\n# --- BASE CLASS -------------------------------------------------------------\n\n\nclass _GaussianProcessBase:\n    def __init__(self, kernel=\"rbf\", noise=1e-5, kernel_params=None):\n        pass\n\n    def _select_kernel(self, x1, x2):\n        '''Selects and computes the kernel value for two single data points.'''\n        pass\n\n    def _compute_covariance(self, X1, X2):\n        '''\n        Computes the covariance matrix between two sets of points.\n        This method fixes the vectorization bug from the original code.\n        '''\n        pass\n\n\n# --- REGRESSION MODEL -------------------------------------------------------\nclass GaussianProcessRegression(_GaussianProcessBase):\n    def fit(self, X, y):\n        pass\n\n    def predict(self, X_test, return_std=False):\n        pass\n\n    def log_marginal_likelihood(self):\n        pass\n\n    def optimize_hyperparameters(self):\n        pass",
  "title": "Gaussian Process for Regression",
  "createdAt": "October 10, 2025 at 12:59:51 PM UTUTC-4",
  "contributor": [
    {
      "profile_link": "https://github.com/Coder1010ayush",
      "name": "Ayush"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Problem Statement: Task is to implement GaussianProcessRegression class which is a guassian process model for prediction regression problems.",
  "learn_section_decoded": "# **Gaussian Processes (GP): From-Scratch Regression Example**\n\n## **1. What's a Gaussian Process?**\n\nA **Gaussian Process** defines a distribution over functions $f(\\cdot)$.\nFor any finite set of inputs $( X = {x_i}_{i=1}^n )$, the function values $f(X)$ follow a multivariate normal:\n\n$$\nf(X) \\sim \\mathcal{N}\\big(0,; K(X,X)\\big)\n$$\n\nwhere ( K ) is a **kernel** (covariance) function encoding similarity between inputs.\nWith noisy targets $( y = f(X) + \\varepsilon, \\varepsilon \\sim \\mathcal{N}(0,\\sigma_n^2 I) )$,\nGP regression yields a closed-form posterior predictive mean and variance at new points $( X_* )$.\n\n---\n\n## **2. The Implementation at a Glance**\n\nThe provided code builds a minimal yet complete GP regression stack:\n\n* **Kernels implemented**\n\n  * Radial Basis Function (RBF / Squared Exponential)\n  * Matérn $(( \\nu = 0.5, 1.5, 2.5 ), or general ( \\nu ))$\n  * Periodic\n  * Linear\n  * Rational Quadratic\n\n* **Core GP classes**\n\n  * `_GaussianProcessBase`: kernel selection & covariance matrix computation\n  * `GaussianProcessRegression`:\n\n    * `fit`: $builds ( K )$, does **Cholesky decomposition**, $solves ( \\alpha )$\n    * `predict`: returns posterior mean & variance\n    * `log_marginal_likelihood`: computes GP evidence\n    * `optimize_hyperparameters`: basic optimizer (for RBF hyperparams)\n\n---\n\n## **3. Kernel Cheat-Sheet**\n\nLet $( x, x' \\in \\mathbb{R}^d ), ( r = \\lVert x - x' \\rVert )$.\n\n* **RBF (SE):**\n  $$\n  k_{\\text{RBF}}(x,x') = \\sigma^2 \\exp!\\left(-\\tfrac{1}{2}\\tfrac{r^2}{\\ell^2}\\right)\n  $$\n\n* **Matérn (( \\nu = 1.5 )):**\n  $$\n  k(x,x') = \\Big(1 + \\tfrac{\\sqrt{3},r}{\\ell}\\Big)\\exp!\\Big(-\\tfrac{\\sqrt{3},r}{\\ell}\\Big)\n  $$\n\n* **Periodic:**\n  $$\n  k(x,x') = \\sigma^2 \\exp!\\left(-\\tfrac{2}{\\ell^2}\\sin^2!\\Big(\\tfrac{\\pi r}{p}\\Big)\\right)\n  $$\n\n* **Linear:**\n  $$\n  k(x,x') = \\sigma_b^2 + \\sigma_v^2,x^\\top x'\n  $$\n\n* **Rational Quadratic:**\n  $$\n  k(x,x') = \\sigma^2\\Big(1 + \\tfrac{r^2}{2\\alpha \\ell^2}\\Big)^{-\\alpha}\n  $$\n\n---\n\n## **4. GP Regression Mechanics**\n\n### **Training**\n\n1. Build covariance:\n   $$\n   K = K(X,X) + \\sigma_n^2 I\n   $$\n\n2. Cholesky factorization:\n   $$\n   K = L L^\\top\n   $$\n\n3. Solve ( \\alpha ):\n   $$\n   L L^\\top \\alpha = y\n   $$\n\n### **Prediction**\n\nAt new inputs ( X_* ):\n\n* $( K_* = K(X, X_*) ), ( K_{**} = K(X_*, X_*) )$\n\n* **Mean:**\n  $$\n  \\mu_* = K_*^\\top \\alpha\n  $$\n\n* **Covariance:**\n  $$\n  \\Sigma_* = K_{**} - V^\\top V, \\quad V = L^{-1} K_*\n  $$\n\n### **Model Selection**\n\n* **Log Marginal Likelihood (LML):**\n  $$\n  \\log p(y \\mid X) = -\\tfrac{1}{2} y^\\top \\alpha - \\sum\\nolimits_i \\log L_{ii} - \\tfrac{n}{2}\\log(2\\pi)\n  $$\n\n---\n\n## **5. Worked Example (Linear Kernel)**\n\n```python\nimport numpy as np\ngp = GaussianProcessRegression(kernel='linear',\n                               kernel_params={'sigma_b': 0.0, 'sigma_v': 1.0},\n                               noise=1e-8)\n\nX_train = np.array([[1], [2], [4]])\ny_train = np.array([3, 5, 9])   # y = 2x + 1\ngp.fit(X_train, y_train)\n\nX_test = np.array([[3.0]])\nmu = gp.predict(X_test)\nprint(f\"{mu[0]:.4f}\")   # -> 7.0000\n```\n\n---\n\n## **6. When to Use GP Regression**\n\n* **Small-to-medium datasets** where uncertainty estimates are valuable\n* Cases requiring **predictive intervals** (not just point predictions)\n* **Nonparametric modeling** with kernel priors\n* Automatic hyperparameter tuning via **marginal likelihood**\n\n---\n\n## **7. Practical Tips**\n\n* Always add **jitter** $10^{-6}$ to the diagonal for numerical stability\n* **Standardize inputs/outputs** before training\n* Be aware: Exact GP has complexity **$\\mathcal{O}(n^3)$** in time and **$\\mathcal{O}(n^2)$** in memory\n* Choose kernels to match problem structure:\n\n  * **RBF:** smooth functions\n  * **Matérn:** rougher functions\n  * **Periodic:** seasonal/cyclical data\n  * **Linear:** global linear trends"
}