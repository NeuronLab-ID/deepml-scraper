{
  "title": "Linear Regression Using Normal Equation",
  "description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBwZXJmb3JtcyBsaW5lYXIgcmVncmVzc2lvbiB1c2luZyB0aGUgbm9ybWFsIGVxdWF0aW9uLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSBtYXRyaXggWCAoZmVhdHVyZXMpIGFuZCBhIHZlY3RvciB5ICh0YXJnZXQpIGFzIGlucHV0LCBhbmQgcmV0dXJuIHRoZSBjb2VmZmljaWVudHMgb2YgdGhlIGxpbmVhciByZWdyZXNzaW9uIG1vZGVsLiBSb3VuZCB5b3VyIGFuc3dlciB0byBmb3VyIGRlY2ltYWwgcGxhY2VzLCAtMC4wIGlzIGEgdmFsaWQgcmVzdWx0IGZvciByb3VuZGluZyBhIHZlcnkgc21hbGwgbnVtYmVyLg==",
  "mdx_file": "e1356255-ae67-4503-9e23-378f3cca5f38.mdx",
  "tinygrad_difficulty": "medium",
  "tinygrad_starter_code": "ZnJvbSB0aW55Z3JhZC50ZW5zb3IgaW1wb3J0IFRlbnNvcgoKZGVmIGxpbmVhcl9yZWdyZXNzaW9uX25vcm1hbF9lcXVhdGlvbl90ZyhYLCB5KSAtPiBUZW5zb3I6CiAgICAiIiIKICAgIFNvbHZlIGxpbmVhciByZWdyZXNzaW9uIHZpYSB0aGUgbm9ybWFsIGVxdWF0aW9uIHVzaW5nIHRpbnlncmFkLgogICAgWDogbGlzdCwgTnVtUHkgYXJyYXksIG9yIFRlbnNvciBvZiBzaGFwZSAobSxuKTsgeTogc2hhcGUgKG0sKSBvciAobSwxKS4KICAgIFJldHVybnMgYSAxLUQgVGVuc29yIG9mIGxlbmd0aCBuLCByb3VuZGVkIHRvIDQgZGVjaW1hbHMuCiAgICAiIiIKICAgICMgWW91ciBpbXBsZW1lbnRhdGlvbiBoZXJlCiAgICBwYXNzCg==",
  "test_cases": [
    {
      "test": "print(linear_regression_normal_equation([[1, 1], [1, 2], [1, 3]], [1, 2, 3]))",
      "expected_output": "[-0.0, 1.0]"
    },
    {
      "test": "print(linear_regression_normal_equation([[1, 3, 4], [1, 2, 5], [1, 3, 2]], [1, 2, 1]))",
      "expected_output": "[4.0, -1.0, -0.0]"
    }
  ],
  "pytorch_difficulty": "medium",
  "likes": "0",
  "difficulty": "easy",
  "video": "https://youtu.be/9hkQJxICZj8",
  "cuda_test_cases": [
    {
      "test": "#include <iostream>\n#include <vector>\nstd::vector<float> linear_regression_normal(const std::vector<std::vector<float>>& X, const std::vector<float>& y);\nint main() { auto r=linear_regression_normal({{1,1},{1,2},{1,3}},{1,2,3}); std::cout<<\"[\"<<r[0]<<\", \"<<r[1]<<\"]\"<<std::endl; return 0; }",
      "expected_output": "[0, 1]"
    },
    {
      "test": "#include <iostream>\n#include <vector>\nstd::vector<float> linear_regression_normal(const std::vector<std::vector<float>>& X, const std::vector<float>& y);\nint main() { auto r=linear_regression_normal({{1,3,4},{1,2,5},{1,3,2}},{1,2,1}); std::cout<<\"[\"<<r[0]<<\", \"<<r[1]<<\", \"<<r[2]<<\"]\"<<std::endl; return 0; }",
      "expected_output": "[4, -1, 0]"
    }
  ],
  "cuda_difficulty": "medium",
  "example": {
    "input": "X = [[1, 1], [1, 2], [1, 3]], y = [1, 2, 3]",
    "output": "[0.0, 1.0]",
    "reasoning": "The linear model is y = 0.0 + 1.0*x, perfectly fitting the input data."
  },
  "dislikes": "0",
  "category": "Machine Learning",
  "starter_code": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n\t# Your code here, make sure to round\n\treturn theta",
  "learn_section": "CiMjIExpbmVhciBSZWdyZXNzaW9uIFVzaW5nIHRoZSBOb3JtYWwgRXF1YXRpb24KCkxpbmVhciByZWdyZXNzaW9uIGFpbXMgdG8gbW9kZWwgdGhlIHJlbGF0aW9uc2hpcCBiZXR3ZWVuIGEgc2NhbGFyIGRlcGVuZGVudCB2YXJpYWJsZSBcKCB5IFwpIGFuZCBvbmUgb3IgbW9yZSBleHBsYW5hdG9yeSB2YXJpYWJsZXMgKG9yIGluZGVwZW5kZW50IHZhcmlhYmxlcykgXCggWCBcKS4gVGhlIG5vcm1hbCBlcXVhdGlvbiBwcm92aWRlcyBhbiBhbmFseXRpY2FsIHNvbHV0aW9uIHRvIGZpbmQgdGhlIGNvZWZmaWNpZW50cyBcKCBcdGhldGEgXCkgdGhhdCBtaW5pbWl6ZSB0aGUgY29zdCBmdW5jdGlvbiBmb3IgbGluZWFyIHJlZ3Jlc3Npb24uCgpHaXZlbiBhIG1hdHJpeCBcKCBYIFwpICh3aXRoIGVhY2ggcm93IHJlcHJlc2VudGluZyBhIHRyYWluaW5nIGV4YW1wbGUgYW5kIGVhY2ggY29sdW1uIGEgZmVhdHVyZSkgYW5kIGEgdmVjdG9yIFwoIHkgXCkgKHJlcHJlc2VudGluZyB0aGUgdGFyZ2V0IHZhbHVlcyksIHRoZSBub3JtYWwgZXF1YXRpb24gaXM6CiQkClx0aGV0YSA9IChYXlRYKV57LTF9WF5UeQokJAoKIyMjIEV4cGxhbmF0aW9uIG9mIFRlcm1zCjEuIFwoIFheVCBcKSBpcyB0aGUgdHJhbnNwb3NlIG9mIFwoIFggXCkuCjIuIFwoIChYXlRYKV57LTF9IFwpIGlzIHRoZSBpbnZlcnNlIG9mIHRoZSBtYXRyaXggXCggWF5UWCBcKS4KMy4gXCggeSBcKSBpcyB0aGUgdmVjdG9yIG9mIHRhcmdldCB2YWx1ZXMuCgojIyMgS2V5IFBvaW50cwotICoqRmVhdHVyZSBTY2FsaW5nKio6IFRoaXMgbWV0aG9kIGRvZXMgbm90IHJlcXVpcmUgZmVhdHVyZSBzY2FsaW5nLgotICoqTGVhcm5pbmcgUmF0ZSoqOiBUaGVyZSBpcyBubyBuZWVkIHRvIGNob29zZSBhIGxlYXJuaW5nIHJhdGUuCi0gKipDb21wdXRhdGlvbmFsIENvc3QqKjogQ29tcHV0aW5nIHRoZSBpbnZlcnNlIG9mIFwoIFheVFggXCkgY2FuIGJlIGNvbXB1dGF0aW9uYWxseSBleHBlbnNpdmUgaWYgdGhlIG51bWJlciBvZiBmZWF0dXJlcyBpcyB2ZXJ5IGxhcmdlLgoKIyMjIFByYWN0aWNhbCBJbXBsZW1lbnRhdGlvbgpBIHByYWN0aWNhbCBpbXBsZW1lbnRhdGlvbiBpbnZvbHZlcyBhdWdtZW50aW5nIFwoIFggXCkgd2l0aCBhIGNvbHVtbiBvZiBvbmVzIHRvIGFjY291bnQgZm9yIHRoZSBpbnRlcmNlcHQgdGVybSBhbmQgdGhlbiBhcHBseWluZyB0aGUgbm9ybWFsIGVxdWF0aW9uIGRpcmVjdGx5IHRvIGNvbXB1dGUgXCggXHRoZXRhIFwpLgoK",
  "cuda_starter_code": "I2luY2x1ZGUgPGN1ZGFfcnVudGltZS5oPgojaW5jbHVkZSA8aW9zdHJlYW0+CiNpbmNsdWRlIDx2ZWN0b3I+CiNpbmNsdWRlIDxjbWF0aD4KCnN0ZDo6dmVjdG9yPGZsb2F0PiBsaW5lYXJfcmVncmVzc2lvbl9ub3JtYWwoY29uc3Qgc3RkOjp2ZWN0b3I8c3RkOjp2ZWN0b3I8ZmxvYXQ+PiYgWCwgY29uc3Qgc3RkOjp2ZWN0b3I8ZmxvYXQ+JiB5KTs=",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    },
    {
      "profile_link": "https://www.youtube.com/@StoatScript/videos",
      "name": "Stoatscript"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "import torch\nX = torch.eye(2)\ny = torch.tensor([5.0, 3.0])\nres = linear_regression_normal_equation(X, y)\nprint(res.detach().numpy().tolist())",
      "expected_output": "[5.0, 3.0]"
    },
    {
      "test": "import torch\nX = torch.tensor([[1.0,1.0],[1.0,2.0],[1.0,3.0]])\ny = torch.tensor([1.0,2.0,3.0])\nres = linear_regression_normal_equation(X, y)\nprint(res.detach().numpy().tolist())",
      "expected_output": "[0.0, 1.0]"
    }
  ],
  "tinygrad_test_cases": [
    {
      "test": "from tinygrad.tensor import Tensor\nres = linear_regression_normal_equation_tg(\n    [[1.0,0.0],[0.0,1.0]],\n    [5.0,3.0]\n)\nprint(res.numpy().tolist())",
      "expected_output": "[5.0, 3.0]"
    },
    {
      "test": "from tinygrad.tensor import Tensor\nres = linear_regression_normal_equation_tg(\n    [[1.0,1.0],[1.0,2.0],[1.0,3.0]],\n    [1.0,2.0,3.0]\n)\nprint(res.numpy().tolist())",
      "expected_output": "[0.0, 1.0]"
    }
  ],
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpkZWYgbGluZWFyX3JlZ3Jlc3Npb25fbm9ybWFsX2VxdWF0aW9uKFgsIHkpIC0+IHRvcmNoLlRlbnNvcjoKICAgICIiIgogICAgU29sdmUgbGluZWFyIHJlZ3Jlc3Npb24gdmlhIHRoZSBub3JtYWwgZXF1YXRpb24gdXNpbmcgUHlUb3JjaC4KICAgIFg6IFRlbnNvciBvciBjb252ZXJ0aWJsZSBvZiBzaGFwZSAobSxuKTsgeTogc2hhcGUgKG0sKSBvciAobSwxKS4KICAgIFJldHVybnMgYSAxLUQgdGVuc29yIG9mIGxlbmd0aCBuLCByb3VuZGVkIHRvIDQgZGVjaW1hbHMuCiAgICAiIiIKICAgIFhfdCA9IHRvcmNoLmFzX3RlbnNvcihYLCBkdHlwZT10b3JjaC5mbG9hdCkKICAgIHlfdCA9IHRvcmNoLmFzX3RlbnNvcih5LCBkdHlwZT10b3JjaC5mbG9hdCkucmVzaGFwZSgtMSwxKQogICAgIyBZb3VyIGltcGxlbWVudGF0aW9uIGhlcmUKICAgIHBhc3MK",
  "description_decoded": "Write a Python function that performs linear regression using the normal equation. The function should take a matrix X (features) and a vector y (target) as input, and return the coefficients of the linear regression model. Round your answer to four decimal places, -0.0 is a valid result for rounding a very small number.",
  "learn_section_decoded": "\n## Linear Regression Using the Normal Equation\n\nLinear regression aims to model the relationship between a scalar dependent variable \\( y \\) and one or more explanatory variables (or independent variables) \\( X \\). The normal equation provides an analytical solution to find the coefficients \\( \\theta \\) that minimize the cost function for linear regression.\n\nGiven a matrix \\( X \\) (with each row representing a training example and each column a feature) and a vector \\( y \\) (representing the target values), the normal equation is:\n$$\n\\theta = (X^TX)^{-1}X^Ty\n$$\n\n### Explanation of Terms\n1. \\( X^T \\) is the transpose of \\( X \\).\n2. \\( (X^TX)^{-1} \\) is the inverse of the matrix \\( X^TX \\).\n3. \\( y \\) is the vector of target values.\n\n### Key Points\n- **Feature Scaling**: This method does not require feature scaling.\n- **Learning Rate**: There is no need to choose a learning rate.\n- **Computational Cost**: Computing the inverse of \\( X^TX \\) can be computationally expensive if the number of features is very large.\n\n### Practical Implementation\nA practical implementation involves augmenting \\( X \\) with a column of ones to account for the intercept term and then applying the normal equation directly to compute \\( \\theta \\).\n\n",
  "tinygrad_starter_code_decoded": "from tinygrad.tensor import Tensor\n\ndef linear_regression_normal_equation_tg(X, y) -> Tensor:\n    \"\"\"\n    Solve linear regression via the normal equation using tinygrad.\n    X: list, NumPy array, or Tensor of shape (m,n); y: shape (m,) or (m,1).\n    Returns a 1-D Tensor of length n, rounded to 4 decimals.\n    \"\"\"\n    # Your implementation here\n    pass\n"
}