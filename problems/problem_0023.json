{
  "description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBjb21wdXRlcyB0aGUgc29mdG1heCBhY3RpdmF0aW9uIGZvciBhIGdpdmVuIGxpc3Qgb2Ygc2NvcmVzLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHJldHVybiB0aGUgc29mdG1heCB2YWx1ZXMgYXMgYSBsaXN0LCBlYWNoIHJvdW5kZWQgdG8gZm91ciBkZWNpbWFsIHBsYWNlcy4=",
  "mdx_file": "49a6d7c0-5d07-4ed9-a51a-a836fdc7b96b.mdx",
  "tinygrad_difficulty": "easy",
  "tinygrad_starter_code": "ZnJvbSB0aW55Z3JhZC50ZW5zb3IgaW1wb3J0IFRlbnNvcgoKZGVmIHNvZnRtYXhfdGcoc2NvcmVzOiBsaXN0W2Zsb2F0XSkgLT4gbGlzdFtmbG9hdF06CiAgICAiIiIKICAgIENvbXB1dGUgdGhlIHNvZnRtYXggYWN0aXZhdGlvbiBmdW5jdGlvbiB1c2luZyB0aW55Z3JhZC4KICAgIElucHV0OgogICAgICAtIHNjb3JlczogbGlzdCBvZiBmbG9hdHMgKGxvZ2l0cykKICAgIFJldHVybnM6CiAgICAgIC0gbGlzdCBvZiBmbG9hdHMgcmVwcmVzZW50aW5nIHRoZSBzb2Z0bWF4IHByb2JhYmlsaXRpZXMsCiAgICAgICAgZWFjaCByb3VuZGVkIHRvIDQgZGVjaW1hbHMuCiAgICAiIiIKICAgICMgWW91ciBpbXBsZW1lbnRhdGlvbiBoZXJlCiAgICBwYXNzCg==",
  "test_cases": [
    {
      "test": "print(softmax([1, 2, 3]))",
      "expected_output": "[0.09, 0.2447, 0.6652]"
    },
    {
      "test": "print(softmax([1, 1, 1]))",
      "expected_output": "[0.3333, 0.3333, 0.3333]"
    }
  ],
  "pytorch_difficulty": "easy",
  "likes": "0",
  "video": "https://youtu.be/WIUTXGFAuXY",
  "marimo_link": "https://adityakhalkar.github.io/Deep-ML-x-Marimo/23",
  "difficulty": "easy",
  "example": {
    "input": "scores = [1, 2, 3]",
    "output": "[0.0900, 0.2447, 0.6652]",
    "reasoning": "The softmax function converts a list of values into a probability distribution. The probabilities are proportional to the exponential of each element divided by the sum of the exponentials of all elements in the list."
  },
  "dislikes": "0",
  "category": "Deep Learning",
  "starter_code": "import math\n\ndef softmax(scores: list[float]) -> list[float]:\n\t# Your code here\n\treturn probabilities",
  "title": "Softmax Activation Function Implementation ",
  "learn_section": "CiMjIFVuZGVyc3RhbmRpbmcgdGhlIFNvZnRtYXggQWN0aXZhdGlvbiBGdW5jdGlvbgoKVGhlIHNvZnRtYXggZnVuY3Rpb24gaXMgYSBnZW5lcmFsaXphdGlvbiBvZiB0aGUgc2lnbW9pZCBmdW5jdGlvbiBhbmQgaXMgdXNlZCBpbiB0aGUgb3V0cHV0IGxheWVyIG9mIGEgbmV1cmFsIG5ldHdvcmsgbW9kZWwgdGhhdCBoYW5kbGVzIG11bHRpLWNsYXNzIGNsYXNzaWZpY2F0aW9uIHRhc2tzLgoKIyMjIE1hdGhlbWF0aWNhbCBEZWZpbml0aW9uClRoZSBzb2Z0bWF4IGZ1bmN0aW9uIGlzIG1hdGhlbWF0aWNhbGx5IHJlcHJlc2VudGVkIGFzOgokJApcdGV4dHtzb2Z0bWF4fSh6X2kpID0gXGZyYWN7ZV57el9pfX17XHN1bV97an0gZV57el9qfX0KJCQKCiMjIyBDaGFyYWN0ZXJpc3RpY3MKLSAqKk91dHB1dCBSYW5nZSoqOiBFYWNoIG91dHB1dCB2YWx1ZSBpcyBiZXR3ZWVuIDAgYW5kIDEsIGFuZCB0aGUgc3VtIG9mIGFsbCBvdXRwdXRzIGlzIDEuCi0gKipQcm9iYWJpbGl0eSBEaXN0cmlidXRpb24qKjogSXQgdHJhbnNmb3JtcyBzY29yZXMgaW50byBwcm9iYWJpbGl0aWVzLCBtYWtpbmcgdGhlbSBlYXNpZXIgdG8gaW50ZXJwcmV0IGFuZCB1c2VmdWwgZm9yIGNsYXNzaWZpY2F0aW9uIHRhc2tzLgoKVGhlIHNvZnRtYXggZnVuY3Rpb24gaXMgZXNzZW50aWFsIGZvciBtb2RlbHMgd2hlcmUgdGhlIG91dHB1dCBuZWVkcyB0byByZXByZXNlbnQgYSBwcm9iYWJpbGl0eSBkaXN0cmlidXRpb24gYWNyb3NzIG11bHRpcGxlIGNsYXNzZXMuCgo=",
  "contributor": null,
  "pytorch_test_cases": [
    {
      "test": "print(softmax([1, 2, 3]))",
      "expected_output": "[0.09, 0.2447, 0.6652]"
    },
    {
      "test": "print(softmax([1, 1, 1]))",
      "expected_output": "[0.3333, 0.3333, 0.3333]"
    }
  ],
  "tinygrad_test_cases": [
    {
      "test": "print(softmax_tg([1, 2, 3]))",
      "expected_output": "[0.09, 0.2447, 0.6652]"
    },
    {
      "test": "print(softmax_tg([1, 1, 1]))",
      "expected_output": "[0.3333, 0.3333, 0.3333]"
    }
  ],
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubi5mdW5jdGlvbmFsIGFzIEYKCmRlZiBzb2Z0bWF4KHNjb3JlczogbGlzdFtmbG9hdF0pIC0+IGxpc3RbZmxvYXRdOgogICAgIiIiCiAgICBDb21wdXRlIHRoZSBzb2Z0bWF4IGFjdGl2YXRpb24gZnVuY3Rpb24gdXNpbmcgUHlUb3JjaCdzIGJ1aWx0LWluIEFQSS4KICAgIElucHV0OgogICAgICAtIHNjb3JlczogbGlzdCBvZiBmbG9hdHMgKGxvZ2l0cykKICAgIFJldHVybnM6CiAgICAgIC0gbGlzdCBvZiBmbG9hdHMgcmVwcmVzZW50aW5nIHRoZSBzb2Z0bWF4IHByb2JhYmlsaXRpZXMsCiAgICAgICAgZWFjaCByb3VuZGVkIHRvIDQgZGVjaW1hbHMuCiAgICAiIiIKICAgICMgWW91ciBpbXBsZW1lbnRhdGlvbiBoZXJlCiAgICBwYXNzCg==",
  "description_decoded": "Write a Python function that computes the softmax activation for a given list of scores. The function should return the softmax values as a list, each rounded to four decimal places.",
  "learn_section_decoded": "\n## Understanding the Softmax Activation Function\n\nThe softmax function is a generalization of the sigmoid function and is used in the output layer of a neural network model that handles multi-class classification tasks.\n\n### Mathematical Definition\nThe softmax function is mathematically represented as:\n$$\n\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n$$\n\n### Characteristics\n- **Output Range**: Each output value is between 0 and 1, and the sum of all outputs is 1.\n- **Probability Distribution**: It transforms scores into probabilities, making them easier to interpret and useful for classification tasks.\n\nThe softmax function is essential for models where the output needs to represent a probability distribution across multiple classes.\n\n",
  "tinygrad_starter_code_decoded": "from tinygrad.tensor import Tensor\n\ndef softmax_tg(scores: list[float]) -> list[float]:\n    \"\"\"\n    Compute the softmax activation function using tinygrad.\n    Input:\n      - scores: list of floats (logits)\n    Returns:\n      - list of floats representing the softmax probabilities,\n        each rounded to 4 decimals.\n    \"\"\"\n    # Your implementation here\n    pass\n"
}