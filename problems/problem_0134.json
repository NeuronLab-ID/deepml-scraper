{
  "description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBjb21wdXRlcyB0aGUgYXZlcmFnZSBjcm9zcy1lbnRyb3B5IGxvc3MgZm9yIGEgYmF0Y2ggb2YgcHJlZGljdGlvbnMgaW4gYSBtdWx0aS1jbGFzcyBjbGFzc2lmaWNhdGlvbiB0YXNrLiBZb3VyIGZ1bmN0aW9uIHNob3VsZCB0YWtlIGluIGEgYmF0Y2ggb2YgcHJlZGljdGVkIHByb2JhYmlsaXRpZXMgYW5kIG9uZS1ob3QgZW5jb2RlZCB0cnVlIGxhYmVscywgdGhlbiByZXR1cm4gdGhlIGF2ZXJhZ2UgY3Jvc3MtZW50cm9weSBsb3NzLiBFbnN1cmUgdGhhdCB5b3UgaGFuZGxlIG51bWVyaWNhbCBzdGFiaWxpdHkgYnkgY2xpcHBpbmcgcHJvYmFiaWxpdGllcyBieSBlcHNpbG9uLg==",
  "id": "134",
  "test_cases": [
    {
      "test": "import numpy as np\npred = np.array([[1, 0, 0], [0, 1, 0]])\ntrue = np.array([[1, 0, 0], [0, 1, 0]])\nprint(round(compute_cross_entropy_loss(pred, true), 4))",
      "expected_output": "0.0"
    },
    {
      "test": "import numpy as np\npred = np.array([[0.1, 0.8, 0.1], [0.8, 0.1, 0.1]])\ntrue = np.array([[0, 0, 1], [0, 1, 0]])\nprint(round(compute_cross_entropy_loss(pred, true), 4))",
      "expected_output": "2.3026"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "predicted_probs = [[0.7, 0.2, 0.1], [0.3, 0.6, 0.1]]\ntrue_labels = [[1, 0, 0], [0, 1, 0]]",
    "output": "0.4338",
    "reasoning": "The predicted probabilities for the correct classes are 0.7 and 0.6. The cross-entropy is computed as -mean(log(0.7), log(0.6)), resulting in approximately 0.4463."
  },
  "category": "Deep Learning",
  "starter_code": "import numpy as np\n\ndef compute_cross_entropy_loss(predicted_probs: np.ndarray, true_labels: np.ndarray, epsilon = 1e-15) -> float:\n    # Your code here\n    pass",
  "title": "Compute Multi-class Cross-Entropy Loss",
  "learn_section": "IyMgTXVsdGktY2xhc3MgQ3Jvc3MtRW50cm9weSBMb3NzIEltcGxlbWVudGF0aW9uCgpDcm9zcy1lbnRyb3B5IGxvc3MsIGFsc28ga25vd24gYXMgbG9nIGxvc3MsIG1lYXN1cmVzIHRoZSBwZXJmb3JtYW5jZSBvZiBhIGNsYXNzaWZpY2F0aW9uIG1vZGVsIHdob3NlIG91dHB1dCBpcyBhIHByb2JhYmlsaXR5IHZhbHVlIGJldHdlZW4gMCBhbmQgMS4gRm9yIG11bHRpLWNsYXNzIGNsYXNzaWZpY2F0aW9uIHRhc2tzLCB3ZSB1c2UgdGhlIGNhdGVnb3JpY2FsIGNyb3NzLWVudHJvcHkgbG9zcy4KCiMjIyBNYXRoZW1hdGljYWwgQmFja2dyb3VuZAoKRm9yIGEgc2luZ2xlIHNhbXBsZSB3aXRoIEMgY2xhc3NlcywgdGhlIGNhdGVnb3JpY2FsIGNyb3NzLWVudHJvcHkgbG9zcyBpcyBkZWZpbmVkIGFzOgoKJEwgPSAtXHN1bV97Yz0xfV57Q30geV9jIFxsb2cocF9jKSQKCndoZXJlOgoKLSAkeV9jJCBpcyBhIGJpbmFyeSBpbmRpY2F0b3IgKDAgb3IgMSkgaWYgY2xhc3MgbGFiZWwgYyBpcyB0aGUgY29ycmVjdCBjbGFzc2lmaWNhdGlvbiBmb3IgdGhlIHNhbXBsZQotICRwX2MkIGlzIHRoZSBwcmVkaWN0ZWQgcHJvYmFiaWxpdHkgdGhhdCB0aGUgc2FtcGxlIGJlbG9uZ3MgdG8gY2xhc3MgYwotICRDJCBpcyB0aGUgbnVtYmVyIG9mIGNsYXNzZXMKCiMjIyBJbXBsZW1lbnRhdGlvbiBSZXF1aXJlbWVudHMKCllvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgYSBmdW5jdGlvbiB0aGF0IGNvbXB1dGVzIHRoZSBhdmVyYWdlIGNyb3NzLWVudHJvcHkgbG9zcyBhY3Jvc3MgbXVsdGlwbGUgc2FtcGxlczoKCiRMX3tiYXRjaH0gPSAtXGZyYWN7MX17Tn1cc3VtX3tuPTF9XntOfVxzdW1fe2M9MX1ee0N9IHlfe24sY30gXGxvZyhwX3tuLGN9KSQKCndoZXJlIE4gaXMgdGhlIG51bWJlciBvZiBzYW1wbGVzIGluIHRoZSBiYXRjaC4KCiMjIyBJbXBvcnRhbnQgQ29uc2lkZXJhdGlvbnMKCi0gSGFuZGxlIG51bWVyaWNhbCBzdGFiaWxpdHkgYnkgYWRkaW5nIGEgc21hbGwgZXBzaWxvbiB0byBhdm9pZCBsb2coMCkKLSBFbnN1cmUgcHJlZGljdGVkIHByb2JhYmlsaXRpZXMgc3VtIHRvIDEgZm9yIGVhY2ggc2FtcGxlCi0gUmV0dXJuIGF2ZXJhZ2UgbG9zcyBhY3Jvc3MgYWxsIHNhbXBsZXMKLSBIYW5kbGUgaW52YWxpZCBpbnB1dHMgYXBwcm9wcmlhdGVseQoKVGhlIGZ1bmN0aW9uIHNob3VsZCB0YWtlIHByZWRpY3RlZCBwcm9iYWJpbGl0aWVzIGFuZCB0cnVlIGxhYmVscyBhcyBpbnB1dCBhbmQgcmV0dXJuIHRoZSBhdmVyYWdlIGNyb3NzLWVudHJvcHkgbG9zcy4=",
  "contributor": [
    {
      "profile_link": "https://github.com/emharsha1812",
      "name": "Harshwardhan Fartale"
    }
  ],
  "description_decoded": "Implement a function that computes the average cross-entropy loss for a batch of predictions in a multi-class classification task. Your function should take in a batch of predicted probabilities and one-hot encoded true labels, then return the average cross-entropy loss. Ensure that you handle numerical stability by clipping probabilities by epsilon.",
  "learn_section_decoded": "## Multi-class Cross-Entropy Loss Implementation\n\nCross-entropy loss, also known as log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. For multi-class classification tasks, we use the categorical cross-entropy loss.\n\n### Mathematical Background\n\nFor a single sample with C classes, the categorical cross-entropy loss is defined as:\n\n$L = -\\sum_{c=1}^{C} y_c \\log(p_c)$\n\nwhere:\n\n- $y_c$ is a binary indicator (0 or 1) if class label c is the correct classification for the sample\n- $p_c$ is the predicted probability that the sample belongs to class c\n- $C$ is the number of classes\n\n### Implementation Requirements\n\nYour task is to implement a function that computes the average cross-entropy loss across multiple samples:\n\n$L_{batch} = -\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{c=1}^{C} y_{n,c} \\log(p_{n,c})$\n\nwhere N is the number of samples in the batch.\n\n### Important Considerations\n\n- Handle numerical stability by adding a small epsilon to avoid log(0)\n- Ensure predicted probabilities sum to 1 for each sample\n- Return average loss across all samples\n- Handle invalid inputs appropriately\n\nThe function should take predicted probabilities and true labels as input and return the average cross-entropy loss."
}