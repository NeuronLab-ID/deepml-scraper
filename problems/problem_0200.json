{
  "description": "SW1wbGVtZW50IGEgc2luZ2xlIHVwZGF0ZSBzdGVwIG9mIHRoZSBSTVNQcm9wIG9wdGltaXplci4gR2l2ZW4gY3VycmVudCBwYXJhbWV0ZXIgdmFsdWVzLCB0aGVpciBncmFkaWVudHMsIGFuZCBhIGNhY2hlICh3aGljaCBzdG9yZXMgYSBtb3ZpbmcgYXZlcmFnZSBvZiBwYXN0IHNxdWFyZWQgZ3JhZGllbnRzKSwgY29tcHV0ZSB0aGUgdXBkYXRlZCBwYXJhbWV0ZXIgdmFsdWVzIGFuZCB1cGRhdGVkIGNhY2hlLiBSTVNQcm9wIGFkYXB0cyB0aGUgbGVhcm5pbmcgcmF0ZSBmb3IgZWFjaCBwYXJhbWV0ZXIgaW5kaXZpZHVhbGx5IGJ5IGRpdmlkaW5nIHRoZSBncmFkaWVudCBieSB0aGUgc3F1YXJlIHJvb3Qgb2YgdGhlIGNhY2hlIHZhbHVlLCB3aGljaCBoZWxwcyBwYXJhbWV0ZXJzIHdpdGggbGFyZ2UgZ3JhZGllbnRzIHRha2Ugc21hbGxlciBzdGVwcyBhbmQgcGFyYW1ldGVycyB3aXRoIHNtYWxsIGdyYWRpZW50cyB0YWtlIGxhcmdlciBzdGVwcy4=",
  "id": "200",
  "test_cases": [
    {
      "test": "params, cache = rmsprop_update([1.0], [0.1], [0.0], lr=0.1, beta=0.9); print([round(p, 6) for p in params], [round(c, 6) for c in cache])",
      "expected_output": "[0.683772] [0.001]"
    },
    {
      "test": "params, cache = rmsprop_update([1.0, 2.0, 3.0], [0.1, 0.2, 0.3], [0.0, 0.0, 0.0], lr=0.01, beta=0.9); print([round(p, 6) for p in params], [round(c, 6) for c in cache])",
      "expected_output": "[0.968377, 1.968377, 2.968377] [0.001, 0.004, 0.009]"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "params=[1.0], grads=[0.1], cache=[0.0], lr=0.1, beta=0.9",
    "output": "([0.683772], [0.001])",
    "reasoning": "First, update cache: $v_{new} = 0.9 \\times 0.0 + 0.1 \\times 0.1^2 = 0.001$. Then update parameter: $p_{new} = 1.0 - 0.1 \\times \\frac{0.1}{\\sqrt{0.001} + 10^{-8}} = 1.0 - \\frac{0.01}{0.0316228} \\approx 0.683772$."
  },
  "category": "Optimization",
  "starter_code": "def rmsprop_update(params: list[float], grads: list[float], cache: list[float], \n                   lr: float = 0.01, beta: float = 0.9, epsilon: float = 1e-8) -> tuple[list[float], list[float]]:\n\t\"\"\"\n\tPerform RMSProp optimization update.\n\t\n\tArgs:\n\t\tparams: List of parameter values\n\t\tgrads: List of gradients for each parameter\n\t\tcache: List of cache values (moving average of squared gradients)\n\t\tlr: Learning rate\n\t\tbeta: Decay rate for moving average\n\t\tepsilon: Small constant for numerical stability\n\t\n\tReturns:\n\t\tTuple of (updated_params, updated_cache)\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Implement RMSProp Optimizer",
  "createdAt": "November 10, 2025 at 8:15:34â€¯AM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement a single update step of the RMSProp optimizer. Given current parameter values, their gradients, and a cache (which stores a moving average of past squared gradients), compute the updated parameter values and updated cache. RMSProp adapts the learning rate for each parameter individually by dividing the gradient by the square root of the cache value, which helps parameters with large gradients take smaller steps and parameters with small gradients take larger steps.",
  "learn_section_decoded": "### Understanding RMSProp (Root Mean Square Propagation)\n\nRMSProp is an adaptive learning rate optimization algorithm that adjusts the learning rate for each parameter based on the history of its gradients. It was developed by Geoffrey Hinton to address limitations in earlier optimizers.\n\n#### The Core Problem\n\nStandard gradient descent uses a fixed learning rate for all parameters:\n\n$$\n\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)\n$$\n\nThis causes issues when:\n- Different parameters have vastly different gradient scales\n- Some dimensions have steep slopes (need small steps) while others are flat (need large steps)\n- Gradients are noisy or sparse\n\n#### The RMSProp Algorithm\n\nRMSProp maintains a moving average of squared gradients (the \"cache\") and uses it to normalize each parameter's update.\n\n**Step 1: Update the Cache**\n\n$$\nv_t = \\beta v_{t-1} + (1 - \\beta) g_t^2\n$$\n\nWhere:\n- $v_t$ is the cache (moving average of squared gradients)\n- $\\beta$ is the decay rate (typically 0.9)\n- $g_t$ is the current gradient\n- The squaring is element-wise\n\n**Step 2: Update the Parameters**\n\n$$\n\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} g_t\n$$\n\nWhere:\n- $\\eta$ is the base learning rate\n- $\\epsilon$ is a small constant (typically $10^{-8}$) for numerical stability\n- Division and square root are element-wise\n\n#### Why This Works\n\n**Adaptive Learning Rates**\n\nEach parameter gets an effective learning rate of:\n\n$$\n\\eta_{effective} = \\frac{\\eta}{\\sqrt{v_t} + \\epsilon}\n$$\n\nThis creates automatic adaptation:\n- **Large historical gradients** -> Large $v_t$ -> Small effective learning rate -> Prevents overshooting\n- **Small historical gradients** -> Small $v_t$ -> Large effective learning rate -> Faster progress\n\n**Moving Average Smoothing**\n\nThe exponential moving average gives more weight to recent gradients while still considering history:\n\n$$\nv_t = (1-\\beta) \\sum_{i=1}^{t} \\beta^{t-i} g_i^2\n$$\n\nFor $\\beta = 0.9$, the algorithm effectively \"remembers\" about the last 10 gradients.\n\n#### Key Insight: Gradient Normalization\n\nRMSProp approximately normalizes gradients by their root mean square (RMS):\n\n$$\n\\frac{g_t}{\\sqrt{v_t}} \\approx \\frac{g_t}{\\text{RMS}(g_{recent})}\n$$\n\nThis makes the algorithm scale-invariant: it works similarly regardless of the absolute magnitude of gradients.\n\n#### Comparison with Other Optimizers\n\n**Versus Gradient Descent**: Adds per-parameter adaptive scaling instead of using a single global learning rate.\n\n**Versus AdaGrad**: AdaGrad accumulates all squared gradients ($v_t = \\sum_{i=1}^{t} g_i^2$), causing learning rates to decrease monotonically and eventually become infinitesimally small. RMSProp's moving average prevents this by \"forgetting\" old gradients.\n\n**Versus Momentum**: Momentum accumulates gradients themselves to build velocity. RMSProp accumulates squared gradients for normalization. These are complementary ideas (both are used in Adam).\n\n#### Parameters\n\n**Learning Rate ($\\eta$)**: Typical values 0.001 to 0.01. Can often be larger than vanilla SGD due to adaptive scaling.\n\n**Decay Rate ($\\beta$)**: Typical values 0.9 or 0.99. Higher values give longer memory of past gradients.\n\n**Epsilon ($\\epsilon$)**: Typical value $10^{-8}$. Prevents division by zero when gradients are very small.\n\n#### Mathematical Connection to Second-Order Methods\n\nRMSProp approximates a diagonal preconditioner for gradient descent:\n\n$$\n\\frac{1}{\\sqrt{v_t}} \\approx \\frac{1}{\\sqrt{\\text{diag}(H)}}\n$$\n\nWhere $H$ is the Hessian matrix. This means RMSProp adapts to the curvature of the loss surface without computing expensive second derivatives.\n\n#### When to Use RMSProp\n\nRMSProp works particularly well for:\n- Recurrent neural networks (RNNs, LSTMs, GRUs)\n- Problems with noisy or sparse gradients\n- Non-stationary objectives where gradient statistics change over time\n\nModern alternatives like Adam often perform better for CNNs and transformers, but RMSProp remains a solid, reliable choice for many deep learning applications."
}