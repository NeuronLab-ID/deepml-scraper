{
  "description": "IyMgVGFzazogSW1wbGVtZW50IHRoZSBTZWxmLUF0dGVudGlvbiBNZWNoYW5pc20KCllvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgdGhlICoqc2VsZi1hdHRlbnRpb24qKiBtZWNoYW5pc20sIHdoaWNoIGlzIGEgZnVuZGFtZW50YWwgY29tcG9uZW50IG9mIHRyYW5zZm9ybWVyIG1vZGVscywgd2lkZWx5IHVzZWQgaW4gbmF0dXJhbCBsYW5ndWFnZSBwcm9jZXNzaW5nIGFuZCBjb21wdXRlciB2aXNpb24gdGFza3MuIFRoZSBzZWxmLWF0dGVudGlvbiBtZWNoYW5pc20gYWxsb3dzIGEgbW9kZWwgdG8gZHluYW1pY2FsbHkgZm9jdXMgb24gZGlmZmVyZW50IHBhcnRzIG9mIHRoZSBpbnB1dCBzZXF1ZW5jZSB3aGVuIGdlbmVyYXRpbmcgYSBjb250ZXh0dWFsaXplZCByZXByZXNlbnRhdGlvbi4KCllvdXIgZnVuY3Rpb24gc2hvdWxkIHJldHVybiB0aGUgc2VsZi1hdHRlbnRpb24gb3V0cHV0IGFzIGEgbnVtcHkgYXJyYXkuCgogICAg",
  "mdx_file": "dafb6e03-b5e6-4710-9608-81b0f5bc1249.mdx",
  "id": "53",
  "test_cases": [
    {
      "test": "import numpy as np\n\nX = np.array([[1, 0], [0, 1]])\nW_q = np.array([[1, 0], [0, 1]])\nW_k = np.array([[1, 0], [0, 1]])\nW_v = np.array([[1, 2], [3, 4]])\n\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\noutput = self_attention(Q, K, V)\nprint(output)",
      "expected_output": "[[1.660477, 2.660477], [2.339523, 3.339523]]"
    },
    {
      "test": "import numpy as np\n\nX = np.array([[1, 1], [1, 0]])\nW_q = np.array([[1, 0], [0, 1]])\nW_k = np.array([[1, 0], [0, 1]])\nW_v = np.array([[1, 2], [3, 4]])\n\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\noutput = self_attention(Q, K, V)\nprint(output)",
      "expected_output": "[[3.00928465, 4.6790462], [2.5, 4.0]]"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "example": {
    "input": "import numpy as np\n\nX = np.array([[1, 0], [0, 1]])\nW_q = np.array([[1, 0], [0, 1]])\nW_k = np.array([[1, 0], [0, 1]])\nW_v = np.array([[1, 2], [3, 4]])\n\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\noutput = self_attention(Q, K, V)\n\nprint(output)",
    "output": "# [[1.660477 2.660477]\n#  [2.339523 3.339523]]",
    "reasoning": "The self-attention mechanism calculates the attention scores for each input, determining how much focus to put on other inputs when generating a contextualized representation. The output is the weighted sum of the values based on the attention scores."
  },
  "dislikes": "0",
  "category": "Deep Learning",
  "starter_code": "import numpy as np\n\ndef self_attention(Q, K, V):\n    \n\treturn attention_output\n",
  "title": "Implement Self-Attention Mechanism",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/Jayanth-vardhan",
      "name": "Jayanth-vardhan"
    }
  ],
  "description_decoded": "## Task: Implement the Self-Attention Mechanism\n\nYour task is to implement the **self-attention** mechanism, which is a fundamental component of transformer models, widely used in natural language processing and computer vision tasks. The self-attention mechanism allows a model to dynamically focus on different parts of the input sequence when generating a contextualized representation.\n\nYour function should return the self-attention output as a numpy array.\n\n    ",
  "learn_section_decoded": "## Self-Attention Mechanism\n\nThe **self-attention mechanism** is a fundamental concept in **transformer models** and is widely used in **natural language processing (NLP)** and **computer vision (CV)**. It allows models to dynamically weigh different parts of the input sequence, enabling them to capture **long-range dependencies** effectively.\n\n---\n\n### **Understanding Self-Attention**\n\nSelf-attention helps a model determine **which parts of an input sequence are relevant to each other**. Instead of treating every word or token equally, self-attention assigns different weights to different parts of the sequence, allowing the model to capture contextual relationships.\n\nFor example, in machine translation, self-attention allows the model to **focus on relevant words** from the input sentence when generating each word in the output.\n\n---\n\n### **Mathematical Formulation of Self-Attention**\n\nGiven an input sequence $X$, self-attention computes three key components:\n\n1. **Query ($Q$)**: Represents the current token we are processing.\n2. **Key ($K$)**: Represents each token in the sequence.\n3. **Value ($V$)**: Contains the actual token embeddings.\n\nThe Query, Key, and Value matrices are computed as:\n\n$$\nQ = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n$$\n\nwhere $W_Q$, $W_K$, and $W_V$ are learned weight matrices.\n\nThe attention scores are computed using the **scaled dot-product attention**:\n\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n$$\n\nwhere $d_k$ is the dimensionality of the key vectors.\n\n---\n\n### **Why Self-Attention is Powerful?**\n\n- **Captures long-range dependencies**: Unlike RNNs, which process input sequentially, self-attention can relate any word in the sequence to any other word, regardless of distance.\n- **Parallelization**: Since self-attention is computed **simultaneously** across the entire sequence, it is much faster than sequential models like LSTMs.\n- **Contextual Understanding**: Each token is **contextually enriched** by attending to relevant tokens in the sequence.\n\n---\n\n### **Example Calculation**\n\nConsider an input sequence of three tokens:\n\n$$\nX = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\n$$\n\nWe compute $Q$, $K$, and $V$ as:\n\n$$\nQ = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n$$\n\nNext, we compute the attention scores:\n\n$$\nS = \\frac{Q K^T}{\\sqrt{d_k}}\n$$\n\nApplying the softmax function:\n\n$$\nA = \\text{softmax}(S)\n$$\n\nFinally, the weighted sum of values:\n\n$$\n\\text{Output} = A V\n$$\n\n---\n\n### **Applications of Self-Attention**\n\nSelf-attention is widely used in:\n- **Transformer models (e.g., BERT, GPT-3)** for language modeling.\n- **Speech processing models** for transcribing audio.\n- **Vision Transformers (ViTs)** for computer vision tasks.\n- **Recommender systems** for learning item-user relationships.\n\nMastering self-attention is essential for understanding modern deep learning architectures, especially in NLP and computer vision.\n"
}