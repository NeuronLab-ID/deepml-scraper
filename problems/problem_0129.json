{
  "description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBjYWxjdWxhdGVzIHRoZSB1bmlncmFtIHByb2JhYmlsaXR5IG9mIGEgZ2l2ZW4gd29yZCBpbiBhIGNvcnB1cyBvZiBzZW50ZW5jZXMuIEluY2x1ZGUgc3RhcnQgYDxzPmAgYW5kIGVuZCBgPC9zPmAgdG9rZW5zIGluIHRoZSBjYWxjdWxhdGlvbi4gVGhlIHByb2JhYmlsaXR5IHNob3VsZCBiZSByb3VuZGVkIHRvIDQgZGVjaW1hbCBwbGFjZXMu",
  "id": "129",
  "test_cases": [
    {
      "test": "corpus = \"\"\"<s> I am Jack </s> <s> Jack I am </s> <s> Jack I like </s> <s> Jack I do like </s> <s> do I like Jack </s>\"\"\"\nprint(round(unigram_probability(corpus, \"Jack\"),4))",
      "expected_output": "0.1852"
    },
    {
      "test": "corpus = \"\"\"<s> I am Jack </s> <s> Jack I am </s> <s> Jack I like </s> <s> Jack I do like </s> <s> do I like Jack </s>\"\"\"\nprint(round(unigram_probability(corpus, \"like\"),4))",
      "expected_output": "0.1111"
    }
  ],
  "difficulty": "easy",
  "likes": "0",
  "video": "",
  "dislikes": "0",
  "example": {
    "input": "corpus = \"<s> Jack I like </s> <s> Jack I do like </s>\", word = \"Jack\"",
    "output": "0.1818",
    "reasoning": "The corpus has 11 total tokens. 'Jack' appears twice. So, probability = 2 / 11"
  },
  "category": "NLP",
  "starter_code": "def unigram_probability(corpus: str, word: str) -> float:\n    # Your code here\n    pass",
  "title": "Calculate Unigram Probability from Corpus",
  "learn_section": "IyBVbmlncmFtIFByb2JhYmlsaXR5IENhbGN1bGF0aW9uCgotIEluIE5hdHVyYWwgTGFuZ3VhZ2UgUHJvY2Vzc2luZyAoTkxQKSwgYSAgdW5pZ3JhbSBtb2RlbCBpcyB0aGUgc2ltcGxlc3QgZm9ybSBvZiBhIGxhbmd1YWdlIG1vZGVsLiAKLSBJdCBhc3N1bWVzIGVhY2ggd29yZCBpbiBhIHNlbnRlbmNlIGlzIGdlbmVyYXRlZCBpbmRlcGVuZGVudGx5LiAgCgoKLSBUaGUgcHJvYmFiaWxpdHkgb2YgYSB3b3JkIHcgdW5kZXIgdGhlIHVuaWdyYW0gbW9kZWwgaXM6CgokUCh3KSA9IFxmcmFje1x0ZXh0e0NvdW50fSh3KX17XHN1bV97dycgXGluIFZ9IFx0ZXh0e0NvdW50fSh3Jyl9JAoKV2hlcmU6CgotICRcdGV4dHtDb3VudH0odykkID0gTnVtYmVyIG9mIHRpbWVzIHRoZSB3b3JkIHcgYXBwZWFycyBpbiB0aGUgY29ycHVzLgoKLSAkViQgPSBWb2NhYnVsYXJ5IChhbGwgd29yZCB0b2tlbnMgaW4gdGhlIGNvcnB1cykuCgotICRcc3VtX3t3JyBcaW4gVn0gXHRleHR7Q291bnR9KHcnKSQgPSBUb3RhbCBudW1iZXIgb2Ygd29yZCB0b2tlbnMuCi0gUm91bmQgdXB0byB0aGUgNHRoIGRlY2ltYWwgcG9pbnQuCgoKLS0tCgojIyMgU2FtcGxlIENvcnB1cwoKYGBgdGV4dAo8cz4gSSBhbSBKYWNrIDwvcz4KPHM+IEphY2sgSSBhbSA8L3M+CjxzPiBKYWNrIEkgbGlrZSA8L3M+CjxzPiBKYWNrIEkgZG8gbGlrZSA8L3M+CjxzPiBkbyBJIGxpa2UgSmFjayA8L3M+CmBgYAoKTm90ZXMgOiAKLSBcPHM+IDogU3RhcnQgb2YgYSBzZW50ZW5jZQotIFw8L3M+IDogRW5kIG9mIGEgc2VudGVuY2UKLSBOZWVkIHRvIGNvdW50IGJvdGggdGhlIHN0YXJ0IGFuZCBlbm9kIG9mIHNlbnRlbmNlIHRva2VucyB3aGlsZSBjYWxjdWxhdGluZyBwcm9iYWJpbGl0eS4KLSBaZXJvIHByb2JhYmlsaXR5IGlzc3VlcyBhcmUgbm90IGFkZHJlc3NlZCBoZXJlIGFuZCB3aWxsIGJlIGNvdmVyZWQgc2VwYXJhdGVseSB1bmRlciBzbW9vdGhpbmcgdGVjaG5pcXVlcyBpbiAgbGF0ZXIgcHJvYmxlbXMu",
  "contributor": [
    {
      "profile_link": "https://github.com/Noth2006",
      "name": "Noth2006"
    }
  ],
  "description_decoded": "Implement a function that calculates the unigram probability of a given word in a corpus of sentences. Include start `<s>` and end `</s>` tokens in the calculation. The probability should be rounded to 4 decimal places.",
  "learn_section_decoded": "# Unigram Probability Calculation\n\n- In Natural Language Processing (NLP), a  unigram model is the simplest form of a language model. \n- It assumes each word in a sentence is generated independently.  \n\n\n- The probability of a word w under the unigram model is:\n\n$P(w) = \\frac{\\text{Count}(w)}{\\sum_{w' \\in V} \\text{Count}(w')}$\n\nWhere:\n\n- $\\text{Count}(w)$ = Number of times the word w appears in the corpus.\n\n- $V$ = Vocabulary (all word tokens in the corpus).\n\n- $\\sum_{w' \\in V} \\text{Count}(w')$ = Total number of word tokens.\n- Round upto the 4th decimal point.\n\n\n---\n\n### Sample Corpus\n\n```text\n<s> I am Jack </s>\n<s> Jack I am </s>\n<s> Jack I like </s>\n<s> Jack I do like </s>\n<s> do I like Jack </s>\n```\n\nNotes : \n- \\<s> : Start of a sentence\n- \\</s> : End of a sentence\n- Need to count both the start and enod of sentence tokens while calculating probability.\n- Zero probability issues are not addressed here and will be covered separately under smoothing techniques in  later problems."
}