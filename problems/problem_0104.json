{
  "description": "SW1wbGVtZW50IHRoZSBwcmVkaWN0aW9uIGZ1bmN0aW9uIGZvciBiaW5hcnkgY2xhc3NpZmljYXRpb24gdXNpbmcgTG9naXN0aWMgUmVncmVzc2lvbi4gWW91ciB0YXNrIGlzIHRvIGNvbXB1dGUgY2xhc3MgcHJvYmFiaWxpdGllcyB1c2luZyB0aGUgc2lnbW9pZCBmdW5jdGlvbiBhbmQgcmV0dXJuIGJpbmFyeSBwcmVkaWN0aW9ucyBiYXNlZCBvbiBhIHRocmVzaG9sZCBvZiAwLjUu",
  "id": "104",
  "test_cases": [
    {
      "test": "print(predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0))",
      "expected_output": "[1 1 0 0]"
    },
    {
      "test": "print(predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0))",
      "expected_output": "[1 1 0]"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0)",
    "output": "[1 1 0 0]",
    "reasoning": "Each sample's linear combination is computed using $z = Xw + b$. The sigmoid function is applied, and the output is thresholded at 0.5, resulting in binary predictions."
  },
  "category": "Machine Learning",
  "starter_code": "import numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> np.ndarray:\n\t\"\"\"\n\tImplements binary classification prediction using Logistic Regression.\n\n\tArgs:\n\t\tX: Input feature matrix (shape: N x D)\n\t\tweights: Model weights (shape: D)\n\t\tbias: Model bias\n\n\tReturns:\n\t\tBinary predictions (0 or 1)\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Binary Classification with Logistic Regression",
  "learn_section": "IyMgQmluYXJ5IENsYXNzaWZpY2F0aW9uIHdpdGggTG9naXN0aWMgUmVncmVzc2lvbgoKTG9naXN0aWMgUmVncmVzc2lvbiBpcyBhIGZ1bmRhbWVudGFsIGFsZ29yaXRobSBmb3IgYmluYXJ5IGNsYXNzaWZpY2F0aW9uLiBHaXZlbiBpbnB1dCBmZWF0dXJlcyBhbmQgbGVhcm5lZCBtb2RlbCBwYXJhbWV0ZXJzICh3ZWlnaHRzIGFuZCBiaWFzKSwgeW91ciB0YXNrIGlzIHRvIGltcGxlbWVudCB0aGUgcHJlZGljdGlvbiBmdW5jdGlvbiB0aGF0IGNvbXB1dGVzIGNsYXNzIHByb2JhYmlsaXRpZXMuCgojIyMgTWF0aGVtYXRpY2FsIEJhY2tncm91bmQKClRoZSBsb2dpc3RpYyByZWdyZXNzaW9uIG1vZGVsIG1ha2VzIHByZWRpY3Rpb25zIHVzaW5nIHRoZSBzaWdtb2lkIGZ1bmN0aW9uOgoKJCRcc2lnbWEoeikgPSBcZnJhY3sxfXsxICsgZV57LXp9fSQkCgp3aGVyZSB6IGlzIHRoZSBsaW5lYXIgY29tYmluYXRpb24gb2YgZmVhdHVyZXMgYW5kIHdlaWdodHMgcGx1cyBiaWFzOgoKJCR6ID0gXG1hdGhiZnt3fV5UXG1hdGhiZnt4fSArIGIgPSBcc3VtX3tpPTF9XntufSB3X2l4X2kgKyBiJCQKCiMjIyBJbXBsZW1lbnRhdGlvbiBSZXF1aXJlbWVudHMKCllvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgYSBmdW5jdGlvbiB0aGF0OgoKLSBUYWtlcyBhIGJhdGNoIG9mIHNhbXBsZXMgJFxtYXRoYmZ7WH0kIChzaGFwZTogTiB4IEQpLCB3ZWlnaHRzICRcbWF0aGJme3d9JCAoc2hhcGU6IEQpLCBhbmQgYmlhcyBiCi0gQ29tcHV0ZXMgJHogPSBcbWF0aGJme1h9XG1hdGhiZnt3fSArIGIkIGZvciBhbGwgc2FtcGxlcwotIEFwcGxpZXMgdGhlIHNpZ21vaWQgZnVuY3Rpb24gdG8gZ2V0IHByb2JhYmlsaXRpZXMKLSBSZXR1cm5zIGJpbmFyeSBwcmVkaWN0aW9ucyBpLmUuLCAwIG9yIDEgdXNpbmcgYSB0aHJlc2hvbGQgb2YgMC41CgojIyMgSW1wb3J0YW50IENvbnNpZGVyYXRpb25zCgotIEhhbmRsZSBudW1lcmljYWwgc3RhYmlsaXR5IGluIHNpZ21vaWQgY29tcHV0YXRpb24KLSBFbnN1cmUgZWZmaWNpZW50IHZlY3Rvcml6ZWQgb3BlcmF0aW9ucyB1c2luZyBudW1weQotIFJldHVybiBiaW5hcnkgcHJlZGljdGlvbnMgKDAgb3IgMSkKCiMjIyBIaW50CgpUbyBwcmV2ZW50IG92ZXJmbG93IGluIHRoZSBleHBvbmVudGlhbCBjYWxjdWxhdGlvbiBvZiB0aGUgc2lnbW9pZCBmdW5jdGlvbiwgdXNlIGBucC5jbGlwYCB0byBsaW1pdCB6IHZhbHVlczoKCmBgYHB5dGhvbgp6ID0gbnAuY2xpcCh6LCAtNTAwLCA1MDApCmBgYApUaGlzIGVuc3VyZXMgbnVtZXJpY2FsIHN0YWJpbGl0eSB3aGVuIGRlYWxpbmcgd2l0aCBsYXJnZSBpbnB1dCB2YWx1ZXMu",
  "contributor": [
    {
      "profile_link": "https://github.com/emharsha1812",
      "name": "emharsha1812"
    }
  ],
  "description_decoded": "Implement the prediction function for binary classification using Logistic Regression. Your task is to compute class probabilities using the sigmoid function and return binary predictions based on a threshold of 0.5.",
  "learn_section_decoded": "## Binary Classification with Logistic Regression\n\nLogistic Regression is a fundamental algorithm for binary classification. Given input features and learned model parameters (weights and bias), your task is to implement the prediction function that computes class probabilities.\n\n### Mathematical Background\n\nThe logistic regression model makes predictions using the sigmoid function:\n\n$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n\nwhere z is the linear combination of features and weights plus bias:\n\n$$z = \\mathbf{w}^T\\mathbf{x} + b = \\sum_{i=1}^{n} w_ix_i + b$$\n\n### Implementation Requirements\n\nYour task is to implement a function that:\n\n- Takes a batch of samples $\\mathbf{X}$ (shape: N x D), weights $\\mathbf{w}$ (shape: D), and bias b\n- Computes $z = \\mathbf{X}\\mathbf{w} + b$ for all samples\n- Applies the sigmoid function to get probabilities\n- Returns binary predictions i.e., 0 or 1 using a threshold of 0.5\n\n### Important Considerations\n\n- Handle numerical stability in sigmoid computation\n- Ensure efficient vectorized operations using numpy\n- Return binary predictions (0 or 1)\n\n### Hint\n\nTo prevent overflow in the exponential calculation of the sigmoid function, use `np.clip` to limit z values:\n\n```python\nz = np.clip(z, -500, 500)\n```\nThis ensures numerical stability when dealing with large input values."
}