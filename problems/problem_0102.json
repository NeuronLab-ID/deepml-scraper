{
  "description": "SW1wbGVtZW50IHRoZSBTd2lzaCBhY3RpdmF0aW9uIGZ1bmN0aW9uLCBhIHNlbGYtZ2F0ZWQgYWN0aXZhdGlvbiBmdW5jdGlvbiB0aGF0IGhhcyBzaG93biBzdXBlcmlvciBwZXJmb3JtYW5jZSBpbiBkZWVwIG5ldXJhbCBuZXR3b3JrcyBjb21wYXJlZCB0byBSZUxVLiBZb3VyIHRhc2sgaXMgdG8gY29tcHV0ZSB0aGUgU3dpc2ggdmFsdWUgZm9yIGEgZ2l2ZW4gaW5wdXQu",
  "id": "102",
  "test_cases": [
    {
      "test": "print(round(swish(0),4))",
      "expected_output": "0.0"
    },
    {
      "test": "print(round(swish(1),4))",
      "expected_output": "0.7311"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "swish(1)",
    "output": "0.7311",
    "reasoning": "For x = 1, the Swish activation is calculated as $Swish(x) = x \\times \\sigma(x)$, where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$. Substituting the value, $Swish(1) = 1 \\times \\frac{1}{1 + e^{-1}} = 0.7311$."
  },
  "category": "Deep Learning",
  "starter_code": "def swish(x: float) -> float:\n\t\"\"\"\n\tImplements the Swish activation function.\n\n\tArgs:\n\t\tx: Input value\n\n\tReturns:\n\t\tThe Swish activation value\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Implement the Swish Activation Function",
  "learn_section": "IyMgVW5kZXJzdGFuZGluZyB0aGUgU3dpc2ggQWN0aXZhdGlvbiBGdW5jdGlvbgoKVGhlIFN3aXNoIGFjdGl2YXRpb24gZnVuY3Rpb24gaXMgYSBtb2Rlcm4gc2VsZi1nYXRlZCBhY3RpdmF0aW9uIGZ1bmN0aW9uIGludHJvZHVjZWQgYnkgcmVzZWFyY2hlcnMgYXQgR29vZ2xlIEJyYWluLiBJdCBoYXMgYmVlbiBzaG93biB0byBwZXJmb3JtIGJldHRlciB0aGFuIFJlTFUgaW4gbWFueSBkZWVwIG5ldHdvcmtzLCBwYXJ0aWN1bGFybHkgaW4gZGVlcGVyIGFyY2hpdGVjdHVyZXMuCgojIyMgTWF0aGVtYXRpY2FsIERlZmluaXRpb24KClRoZSBTd2lzaCBmdW5jdGlvbiBpcyBkZWZpbmVkIGFzOgoKJCRTd2lzaCh4KSA9IHggXHRpbWVzIFxzaWdtYSh4KSQkCgp3aGVyZSAkXHNpZ21hKHgpJCBpcyB0aGUgc2lnbW9pZCBmdW5jdGlvbiBkZWZpbmVkIGFzOgoKJCRcc2lnbWEoeCkgPSBcZnJhY3sxfXsxICsgZV57LXh9fSQkCgojIyMgQ2hhcmFjdGVyaXN0aWNzCgotICoqT3V0cHV0IFJhbmdlKio6IFVubGlrZSBSZUxVIHdoaWNoIGhhcyBhIHJhbmdlIG9mICRbMCwgXGluZnR5KSQsIFN3aXNoIGhhcyBhIHJhbmdlIG9mICQoLVxpbmZ0eSwgXGluZnR5KSQKLSAqKlNtb290aG5lc3MqKjogU3dpc2ggaXMgc21vb3RoIGFuZCBub24tbW9ub3RvbmljLCBtYWtpbmcgaXQgZGlmZmVyZW50aWFibGUgZXZlcnl3aGVyZQotICoqU2hhcGUqKjogVGhlIGZ1bmN0aW9uIGhhcyBhIHNsaWdodCBkaXAgYmVsb3cgMCBmb3IgbmVnYXRpdmUgdmFsdWVzLCB0aGVuIGN1cnZlcyB1cCBzbW9vdGhseSBmb3IgcG9zaXRpdmUgdmFsdWVzCi0gKipQcm9wZXJ0aWVzKio6CiAgLSBGb3IgbGFyZ2UgcG9zaXRpdmUgeDogU3dpc2goeCkgfiB4IChzaW1pbGFyIHRvIGxpbmVhciBmdW5jdGlvbikKICAtIEZvciBsYXJnZSBuZWdhdGl2ZSB4OiBTd2lzaCh4KSB+IDAgKHNpbWlsYXIgdG8gUmVMVSkKICAtIEhhcyBhIG1pbmltYWwgdmFsdWUgYXJvdW5kIHggfiAtMS4yOAoKIyMjIEFkdmFudGFnZXMKCi0gU21vb3RoIGZ1bmN0aW9uIHdpdGggbm8gaGFyZCB6ZXJvIHRocmVzaG9sZCBsaWtlIFJlTFUKLSBTZWxmLWdhdGVkIG5hdHVyZSBhbGxvd3MgZm9yIG1vcmUgY29tcGxleCByZWxhdGlvbnNoaXBzCi0gT2Z0ZW4gcHJvdmlkZXMgYmV0dGVyIHBlcmZvcm1hbmNlIGluIGRlZXAgbmV1cmFsIG5ldHdvcmtzCi0gUmVkdWNlcyB0aGUgdmFuaXNoaW5nIGdyYWRpZW50IHByb2JsZW0gY29tcGFyZWQgdG8gc2lnbW9pZA==",
  "contributor": [
    {
      "profile_link": "https://github.com/Haleshot",
      "name": "Haleshot"
    }
  ],
  "description_decoded": "Implement the Swish activation function, a self-gated activation function that has shown superior performance in deep neural networks compared to ReLU. Your task is to compute the Swish value for a given input.",
  "learn_section_decoded": "## Understanding the Swish Activation Function\n\nThe Swish activation function is a modern self-gated activation function introduced by researchers at Google Brain. It has been shown to perform better than ReLU in many deep networks, particularly in deeper architectures.\n\n### Mathematical Definition\n\nThe Swish function is defined as:\n\n$$Swish(x) = x \\times \\sigma(x)$$\n\nwhere $\\sigma(x)$ is the sigmoid function defined as:\n\n$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n\n### Characteristics\n\n- **Output Range**: Unlike ReLU which has a range of $[0, \\infty)$, Swish has a range of $(-\\infty, \\infty)$\n- **Smoothness**: Swish is smooth and non-monotonic, making it differentiable everywhere\n- **Shape**: The function has a slight dip below 0 for negative values, then curves up smoothly for positive values\n- **Properties**:\n  - For large positive x: Swish(x) ~ x (similar to linear function)\n  - For large negative x: Swish(x) ~ 0 (similar to ReLU)\n  - Has a minimal value around x ~ -1.28\n\n### Advantages\n\n- Smooth function with no hard zero threshold like ReLU\n- Self-gated nature allows for more complex relationships\n- Often provides better performance in deep neural networks\n- Reduces the vanishing gradient problem compared to sigmoid"
}