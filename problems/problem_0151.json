{
  "description": "SW1wbGVtZW50IGEgZHJvcG91dCBsYXllciB0aGF0IGFwcGxpZXMgcmFuZG9tIG5ldXJvbiBkZWFjdGl2YXRpb24gZHVyaW5nIHRyYWluaW5nIHRvIHByZXZlbnQgb3ZlcmZpdHRpbmcgaW4gbmV1cmFsIG5ldHdvcmtzLiBUaGUgbGF5ZXIgc2hvdWxkIHJhbmRvbWx5IHplcm8gb3V0IGEgcHJvcG9ydGlvbiBvZiBpbnB1dCBlbGVtZW50cyBiYXNlZCBvbiBhIGRyb3BvdXQgcmF0ZSBwLCBzY2FsZSB0aGUgcmVtYWluaW5nIHZhbHVlcyBieSAxLygxLXApIHRvIG1haW50YWluIGV4cGVjdGVkIHZhbHVlcywgYW5kIHBhc3MgaW5wdXRzIHVuY2hhbmdlZCBkdXJpbmcgaW5mZXJlbmNlLiBEdXJpbmcgYmFja3Byb3BhZ2F0aW9uLCBncmFkaWVudHMgbXVzdCBiZSBtYXNrZWQgd2l0aCB0aGUgc2FtZSBkcm9wb3V0IHBhdHRlcm4gYW5kIHNjYWxlZCBieSB0aGUgc2FtZSBmYWN0b3IgdG8gZW5zdXJlIHByb3BlciBncmFkaWVudCBmbG93Lg==",
  "id": "151",
  "test_cases": [
    {
      "test": "import numpy as np\nnp.random.seed(42)\nx = np.array([[1.0, 2.0], [3.0, 4.0]])\ngrad = np.array([[0.5, 0.2], [1.0, 2.0]])\n\ndropout = DropoutLayer(0.2)\n\nprint(dropout.forward(x, training=True), dropout.forward(x, training=False), dropout.backward(grad))",
      "expected_output": "(array([[1.25, 0.], [3.75, 5.]]), array([[1.0, 2.0], [3.0, 4.0]]), array([[0.625, 0.], [1.25, 2.5]]))"
    },
    {
      "test": "import numpy as np\nnp.random.seed(42)\nx = np.ones((1000, 1000))\ndropout = DropoutLayer(0.2)\n\n_ = dropout.forward(x, training=True)\nmask1 = dropout.mask.copy()\n_ = dropout.forward(x, training=True)\nmask2 = dropout.mask.copy()\nprint(np.array_equal(mask1, mask2))",
      "expected_output": "False"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "x = np.array([1.0, 2.0, 3.0, 4.0]), grad = np.array([0.1, 0.2, 0.3, 0.4]), p = 0.5",
    "output": "output = array([[2., 0. , 6. , 0. ]]), grad = array([[0.2, 0. , 0.6, 0. ]])",
    "reasoning": "The Dropout layer randomly zeroes out elements of the input tensor with probability p during training. To maintain the expected value of the activations, the remaining elements are scaled by a factor of 1 / (1 - p). During inference, Dropout is disabled and the input is passed through unchanged. During backpropagation, the same dropout mask and scaling are applied to the gradients, ensuring the expected gradient magnitude is preserved."
  },
  "category": "Deep Learning",
  "starter_code": "import numpy as np\n\nclass DropoutLayer:\n    def __init__(self, p: float):\n        \"\"\"Initialize the dropout layer.\"\"\"\n        # Your code here\n\n    def forward(self, x: np.ndarray, training: bool = True) -> np.ndarray:\n        \"\"\"Forward pass of the dropout layer.\"\"\"\n        # Your code here\n\n    def backward(self, grad: np.ndarray) -> np.ndarray:\n        \"\"\"Backward pass of the dropout layer.\"\"\"\n        # Your code here",
  "title": "Dropout Layer",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/mavleo96",
      "name": "Vijayabharathi Murugan"
    }
  ],
  "description_decoded": "Implement a dropout layer that applies random neuron deactivation during training to prevent overfitting in neural networks. The layer should randomly zero out a proportion of input elements based on a dropout rate p, scale the remaining values by 1/(1-p) to maintain expected values, and pass inputs unchanged during inference. During backpropagation, gradients must be masked with the same dropout pattern and scaled by the same factor to ensure proper gradient flow.",
  "learn_section_decoded": "# Implementing Dropout Layer\n\n## Introduction\nDropout is a regularization technique that randomly deactivates neurons during training to prevent overfitting. It forces the network to learn with different neurons and prevents it from becoming too dependent on specific neurons.\n\n## Learning Objectives\n- Understand the concept and purpose of dropout\n- Learn how dropout works during training and inference\n- Implement dropout layer with proper scaling\n\n## Theory\nDuring training, dropout randomly sets a proportion of inputs to zero and scales up the remaining values to maintain the expected value. The mathematical formulation is:\n\nDuring training:\n\n$y = \\dfrac{x \\odot m}{1-p}$\n\nDuring inference:\n\n$y = x$\n\nDuring backpropagation:\n\n$grad = \\dfrac{grad \\odot m}{1-p}$\n\nWhere:\n- $x$ is the input vector\n- $m$ is a binary mask vector sampled from Bernoulli(p)\n- $\\odot$ represents element-wise multiplication\n- $p$ is the dropout rate (probability of keeping a neuron)\n\nThe mask $m$ is randomly generated for each forward pass during training and is stored in memory to be used in the corresponding backward pass. This ensures that the same neurons are dropped during both forward and backward propagation for a given input.\n\nThe scaling factor $\\frac{1}{1-p}$ during training ensures that the expected value of the output matches the input, making the network's behavior consistent between training and inference.\n\nDuring backpropagation, the gradients must also be scaled by the same factor $\\frac{1}{1-p}$ to maintain the correct gradient flow.\n\nDropout acts as a form of regularization by:\n1. Preventing co-adaptation of neurons, forcing them to learn more robust features that are useful in combination with many different random subsets of other neurons\n2. Creating an implicit ensemble of networks, as each forward pass uses a different subset of neurons, effectively training multiple networks that share parameters\n3. Reducing the effective capacity of the network during training, which helps prevent overfitting by making the model less likely to memorize the training data\n\nRead more at:\n\n1. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15(1), 1929-1958. [PDF](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)\n\n## Problem Statement\nImplement a dropout layer class that can be used during both training and inference phases of a neural network. The implementation should:\n\n1. Apply dropout during training by randomly zeroing out elements\n2. Scale the remaining values appropriately to maintain expected values\n3. Pass through inputs unchanged during inference\n4. Support backpropagation by storing and using the dropout mask\n\n### Requirements\nThe `DropoutLayer` class should implement:\n\n1. `__init__(p: float)`: Initialize with dropout probability p\n2. `forward(x: np.ndarray, training: bool = True) -> np.ndarray`: Apply dropout during forward pass\n3. `backward(grad: np.ndarray) -> np.ndarray`: Handle gradient flow during backpropagation\n\n### Input Parameters\n- `p`: Dropout rate (probability of keeping a neuron), must be between 0 and 1\n- `x`: Input tensor of any shape\n- `training`: Boolean flag indicating if in training mode\n- `grad`: Gradient tensor during backpropagation\n\n### Output\n- Forward pass: Tensor of same shape as input with dropout applied\n- Backward pass: Gradient tensor with dropout mask applied\n\n## Example\n```python\n# Example usage:\nx = np.array([1.0, 2.0, 3.0, 4.0])\ngrad = np.array([0.1, 0.2, 0.3, 0.4])\np = 0.5  # 50% dropout rate\n\n# During training\noutput_train = dropout_layer(x, p, training=True)\n\n# During inference\noutput_inference = dropout_layer(x, p, training=False)\n\n# Backward\ngrad_back = dropout.backward(grad)\n```\n\n## Tips\n- Use numpy's random binomial generator for creating the mask\n- Remember to scale up the output and gradients during training by 1/(1-p)\n- Test with different dropout rates (typically between 0.2 and 0.5)\n- Verify that the expected value of the output matches the input\n\n## Common Pitfalls\n- Using the same mask for all examples in a batch\n- Setting dropout rate too high (can lead to underfitting)\n\n---"
}