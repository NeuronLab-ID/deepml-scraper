{
  "learn_section": "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",
  "description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdGhhdCBjYWxjdWxhdGVzIHRoZSBkZXRlcm1pbmFudCBvZiBhIDR4NCBtYXRyaXggdXNpbmcgTGFwbGFjZSdzIEV4cGFuc2lvbiBtZXRob2QuIFRoZSBmdW5jdGlvbiBzaG91bGQgdGFrZSBhIHNpbmdsZSBhcmd1bWVudCwgYSA0eDQgbWF0cml4IHJlcHJlc2VudGVkIGFzIGEgbGlzdCBvZiBsaXN0cywgYW5kIHJldHVybiB0aGUgZGV0ZXJtaW5hbnQgb2YgdGhlIG1hdHJpeC4gVGhlIGVsZW1lbnRzIG9mIHRoZSBtYXRyaXggY2FuIGJlIGludGVnZXJzIG9yIGZsb2F0aW5nLXBvaW50IG51bWJlcnMuIEltcGxlbWVudCB0aGUgZnVuY3Rpb24gcmVjdXJzaXZlbHkgdG8gaGFuZGxlIHRoZSBjb21wdXRhdGlvbiBvZiBkZXRlcm1pbmFudHMgZm9yIHRoZSAzeDMgbWlub3IgbWF0cmljZXMu",
  "mdx_file": "99712fe7-8545-4194-bdd1-b2960e6d9248.mdx",
  "tinygrad_difficulty": "easy",
  "tinygrad_starter_code": "ZnJvbSB0aW55Z3JhZC50ZW5zb3IgaW1wb3J0IFRlbnNvcgoKZGVmIGRldGVybWluYW50XzR4NF90ZyhtYXRyaXgpIC0+IFRlbnNvcjoKICAgICIiIgogICAgQ29tcHV0ZSB0aGUgZGV0ZXJtaW5hbnQgb2YgYSA0w5c0IG1hdHJpeCB1c2luZyB0aW55Z3JhZC4KICAgIElucHV0IGNhbiBiZSBhIFB5dGhvbiBsaXN0LCBOdW1QeSBhcnJheSwgb3IgdGlueWdyYWQgVGVuc29yIG9mIHNoYXBlICg0LDQpLgogICAgUmV0dXJucyBhIDAtRCBUZW5zb3IgY29udGFpbmluZyB0aGUgZGV0ZXJtaW5hbnQuCiAgICAiIiIKICAgICMgWW91ciBpbXBsZW1lbnRhdGlvbiBoZXJlCiAgICBwYXNzCg==",
  "test_cases": [
    {
      "test": "print(determinant_4x4([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]))",
      "expected_output": "0"
    },
    {
      "test": "print(determinant_4x4([[4, 3, 2, 1], [3, 2, 1, 4], [2, 1, 4, 3], [1, 4, 3, 2]]))",
      "expected_output": "-160"
    }
  ],
  "pytorch_difficulty": "easy",
  "video": null,
  "difficulty": "hard",
  "likes": "0",
  "cuda_test_cases": [
    {
      "test": "#include <iostream>\n#include <vector>\nfloat determinant_4x4(const std::vector<std::vector<float>>& matrix);\nint main() { std::cout << determinant_4x4({{1,2,3,4},{5,6,7,8},{9,10,11,12},{13,14,15,16}}) << std::endl; return 0; }",
      "expected_output": "0"
    },
    {
      "test": "#include <iostream>\n#include <vector>\nfloat determinant_4x4(const std::vector<std::vector<float>>& matrix);\nint main() { std::cout << determinant_4x4({{4,3,2,1},{3,2,1,4},{2,1,4,3},{1,4,3,2}}) << std::endl; return 0; }",
      "expected_output": "-160"
    }
  ],
  "cuda_difficulty": "hard",
  "example": {
    "input": "a = [[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]]",
    "output": "0",
    "reasoning": "Using Laplace's Expansion, the determinant of a 4x4 matrix is calculated by expanding it into minors and cofactors along any row or column. Given the symmetrical and linear nature of this specific matrix, its determinant is 0. The calculation for a generic 4x4 matrix involves more complex steps, breaking it down into the determinants of 3x3 matrices."
  },
  "dislikes": "0",
  "category": "Linear Algebra",
  "starter_code": "def determinant_4x4(matrix: list[list[int|float]]) -> float:\n\t# Your recursive implementation here\n\tpass",
  "title": "Determinant of a 4x4 Matrix using Laplace's Expansion (hard)",
  "cuda_starter_code": "I2luY2x1ZGUgPGN1ZGFfcnVudGltZS5oPgojaW5jbHVkZSA8aW9zdHJlYW0+CiNpbmNsdWRlIDx2ZWN0b3I+CgpfX2RldmljZV9fIGZsb2F0IGRldDN4MyhmbG9hdCBhMDAsZmxvYXQgYTAxLGZsb2F0IGEwMixmbG9hdCBhMTAsZmxvYXQgYTExLGZsb2F0IGExMixmbG9hdCBhMjAsZmxvYXQgYTIxLGZsb2F0IGEyMik7Cl9fZ2xvYmFsX18gdm9pZCBkZXQ0eDRfa2VybmVsKGNvbnN0IGZsb2F0KiBtLCBmbG9hdCogY29mYWN0b3JzKTsKZmxvYXQgZGV0ZXJtaW5hbnRfNHg0KGNvbnN0IHN0ZDo6dmVjdG9yPHN0ZDo6dmVjdG9yPGZsb2F0Pj4mIG1hdHJpeCk7",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "import torch\nprint(determinant_4x4(torch.eye(4)))",
      "expected_output": "1.0"
    },
    {
      "test": "import torch\nprint(determinant_4x4(torch.diag(torch.tensor([2.0,3.0,4.0,5.0]))))",
      "expected_output": "120.0"
    },
    {
      "test": "import torch\nm = torch.tensor([[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[5.0,6.0,7.0,8.0],[9.0,10.0,11.0,12.0]])\nprint(determinant_4x4(m))",
      "expected_output": "0.0"
    }
  ],
  "tinygrad_test_cases": [
    {
      "test": "from tinygrad.tensor import Tensor\nprint(determinant_4x4_tg(Tensor(np.eye(4))).numpy().item())",
      "expected_output": "1.0"
    },
    {
      "test": "from tinygrad.tensor import Tensor\nprint(determinant_4x4_tg([[2.0,0,0,0],[0,3.0,0,0],[0,0,4.0,0],[0,0,0,5.0]]).numpy().item())",
      "expected_output": "120.0"
    },
    {
      "test": "from tinygrad.tensor import Tensor\nm = [[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0],[5.0,6.0,7.0,8.0],[9.0,10.0,11.0,12.0]]\nprint(determinant_4x4_tg(Tensor(m)).numpy().item())",
      "expected_output": "0.0"
    }
  ],
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpkZWYgZGV0ZXJtaW5hbnRfNHg0KG1hdHJpeCkgLT4gZmxvYXQ6CiAgICAiIiIKICAgIENvbXB1dGUgdGhlIGRldGVybWluYW50IG9mIGEgNMOXNCBtYXRyaXggdXNpbmcgUHlUb3JjaC4KICAgIElucHV0IGNhbiBiZSBhIFB5dGhvbiBsaXN0LCBOdW1QeSBhcnJheSwgb3IgdG9yY2ggVGVuc29yIG9mIHNoYXBlICg0LDQpLgogICAgUmV0dXJucyBhIFB5dGhvbiBmbG9hdC4KICAgICIiIgogICAgIyBDb252ZXJ0IHRvIHRlbnNvcgogICAgbSA9IHRvcmNoLmFzX3RlbnNvcihtYXRyaXgsIGR0eXBlPXRvcmNoLmZsb2F0KQogICAgIyBZb3VyIGltcGxlbWVudGF0aW9uIGhlcmUKICAgIHBhc3MK",
  "description_decoded": "Write a Python function that calculates the determinant of a 4x4 matrix using Laplace's Expansion method. The function should take a single argument, a 4x4 matrix represented as a list of lists, and return the determinant of the matrix. The elements of the matrix can be integers or floating-point numbers. Implement the function recursively to handle the computation of determinants for the 3x3 minor matrices.",
  "learn_section_decoded": "\n## Determinant of a 4x4 Matrix using Laplace's Expansion\n\nLaplace's Expansion, also known as cofactor expansion, is a method to calculate the determinant of a square matrix of any size. For a 4x4 matrix \\( A \\), this method involves expanding \\( A \\) into minors and cofactors along a chosen row or column.\n\nConsider a 4x4 matrix \\( A \\):\n$$\nA = \\begin{pmatrix}\na_{11} & a_{12} & a_{13} & a_{14} \\\\\na_{21} & a_{22} & a_{23} & a_{24} \\\\\na_{31} & a_{32} & a_{33} & a_{34} \\\\\na_{41} & a_{42} & a_{43} & a_{44}\n\\end{pmatrix}\n$$\n\nThe determinant of \\( A \\), \\( \\det(A) \\), can be calculated by selecting any row or column (e.g., the first row) and using the formula that involves the elements of that row (or column), their corresponding cofactors, and the determinants of the 3x3 minor matrices obtained by removing the row and column of each element. This process is recursive, as calculating the determinants of the 3x3 matrices involves further expansions.\n\nThe expansion formula for the first row is:\n$$\n\\det(A) = a_{11}C_{11} - a_{12}C_{12} + a_{13}C_{13} - a_{14}C_{14}\n$$\n\n### Explanation of Terms\n- **Cofactor \\( C_{ij} \\)**: The cofactor of element \\( a_{ij} \\) is given by:\n  $$\n  C_{ij} = (-1)^{i+j} \\det(\\text{Minor of } a_{ij})\n  $$\n  where the minor of \\( a_{ij} \\) is the determinant of the 3x3 matrix obtained by removing the \\( i \\)th row and \\( j \\)th column from \\( A \\).\n\n### Notes\n- The choice of row or column for expansion can be based on convenience, often selecting one with the most zeros to simplify calculations.\n- The process is recursive, breaking down the determinant calculation into smaller 3x3 determinants until reaching 2x2 determinants, which are simpler to compute.\n\nThis method is fundamental in linear algebra and provides a systematic approach for determinant calculation, especially for matrices larger than 3x3.\n",
  "tinygrad_starter_code_decoded": "from tinygrad.tensor import Tensor\n\ndef determinant_4x4_tg(matrix) -> Tensor:\n    \"\"\"\n    Compute the determinant of a 4Ã—4 matrix using tinygrad.\n    Input can be a Python list, NumPy array, or tinygrad Tensor of shape (4,4).\n    Returns a 0-D Tensor containing the determinant.\n    \"\"\"\n    # Your implementation here\n    pass\n"
}