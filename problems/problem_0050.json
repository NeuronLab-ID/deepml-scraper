{
  "description": "SW1wbGVtZW50IHRoZSBMYXNzbyBSZWdyZXNzaW9uIGFsZ29yaXRobSB1c2luZyBHcmFkaWVudCBEZXNjZW50LiBMYXNzbyBSZWdyZXNzaW9uIChMZWFzdCBBYnNvbHV0ZSBTaHJpbmthZ2UgYW5kIFNlbGVjdGlvbiBPcGVyYXRvcikgdXNlcyBMMSByZWd1bGFyaXphdGlvbiwgd2hpY2ggYWRkcyBhIHBlbmFsdHkgZXF1YWwgdG8gdGhlIGFic29sdXRlIHZhbHVlIG9mIHRoZSBjb2VmZmljaWVudHMgdG8gdGhlIGxvc3MgZnVuY3Rpb24uIFRoaXMgZW5jb3VyYWdlcyBzcGFyc2l0eSBpbiB0aGUgbW9kZWwsIGVmZmVjdGl2ZWx5IHBlcmZvcm1pbmcgZmVhdHVyZSBzZWxlY3Rpb24gYnkgc2hyaW5raW5nIHNvbWUgY29lZmZpY2llbnRzIHRvIHplcm8uCgpUaGUgb2JqZWN0aXZlIGZ1bmN0aW9uIG9mIExhc3NvIFJlZ3Jlc3Npb24gaXM6CiQkCkoodywgYikgPSBcZnJhY3sxfXsybn0gXHN1bV97aT0xfV57bn0gXGxlZnQoIHlfaSAtIFxsZWZ0KCBcc3VtX3tqPTF9XntwfSBYX3tpan0gd19qICsgYiBccmlnaHQpIFxyaWdodCleMiArIFxhbHBoYSBcc3VtX3tqPTF9XntwfSB8IHdfaiB8CiQkCgpXaGVyZToKLSAkbiQgaXMgdGhlIG51bWJlciBvZiBzYW1wbGVzCi0gJHAkIGlzIHRoZSBudW1iZXIgb2YgZmVhdHVyZXMKLSAkeV9pJCBpcyB0aGUgYWN0dWFsIHZhbHVlIGZvciB0aGUgJGkkLXRoIHNhbXBsZQotICRcaGF0e3l9X2kgPSBcc3VtX3tqPTF9XntwfSBYX3tpan0gd19qICsgYiQgaXMgdGhlIHByZWRpY3RlZCB2YWx1ZQotICR3X2okIGlzIHRoZSB3ZWlnaHQgZm9yIHRoZSAkaiQtdGggZmVhdHVyZQotICRiJCBpcyB0aGUgYmlhcyB0ZXJtIChub3QgcmVndWxhcml6ZWQpCi0gJFxhbHBoYSQgaXMgdGhlIHJlZ3VsYXJpemF0aW9uIHN0cmVuZ3RoCgpZb3VyIHRhc2sgaXMgdG8gaW1wbGVtZW50IGdyYWRpZW50IGRlc2NlbnQgdG8gbWluaW1pemUgdGhpcyBvYmplY3RpdmUgZnVuY3Rpb24sIHJldHVybmluZyB0aGUgb3B0aW1pemVkIHdlaWdodHMgYW5kIGJpYXMu",
  "mdx_file": "ae0bf0f7-0349-4ca1-85e3-8316c8faa74b.mdx",
  "id": "50",
  "test_cases": [
    {
      "test": "import numpy as np\n\nX = np.array([[0, 0], [1, 1], [2, 2]])\ny = np.array([0, 1, 2])\n\nalpha = 0.1\noutput = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)\nprint(output)",
      "expected_output": "(array([0.42371644, 0.42371644]), 0.15385068459377865)"
    },
    {
      "test": "import numpy as np\n\nX = np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([1, 2, 3, 4, 5])\n\nalpha = 0.1\noutput = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)\nprint(output)",
      "expected_output": "(array([0.27280148, 0.68108784]), 0.4082863608718005)"
    }
  ],
  "difficulty": "medium",
  "likes": "0",
  "video": "",
  "dislikes": "0",
  "example": {
    "input": "X = np.array([[0, 0], [1, 1], [2, 2]])\ny = np.array([0, 1, 2])\nalpha = 0.1\nweights, bias = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)",
    "output": "(array([0.42371644, 0.42371644]), 0.15385068459377865)",
    "reasoning": "The algorithm minimizes the Lasso objective by iteratively updating weights via gradient descent. With identical features (both columns equal), the L1 penalty distributes the weight equally (~0.424 each). The bias (0.154) adjusts the intercept. The regularization prevents overfitting by penalizing large weights."
  },
  "category": "Machine Learning",
  "starter_code": "import numpy as np\n\ndef l1_regularization_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    \"\"\"\n    Implement Lasso Regression using Gradient Descent.\n    \n    Args:\n        X: Feature matrix of shape (n_samples, n_features)\n        y: Target vector of shape (n_samples,)\n        alpha: L1 regularization strength (higher = more regularization)\n        learning_rate: Step size for gradient descent updates\n        max_iter: Maximum number of iterations\n        tol: Convergence tolerance - stops when L1 norm of weight gradient < tol\n    \n    Returns:\n        tuple: (weights, bias) where:\n            - weights: np.ndarray of shape (n_features,)\n            - bias: float\n    \n    Note:\n        - The bias term is NOT regularized\n        - Use subgradient for L1: sign(0) = 0\n    \"\"\"\n    n_samples, n_features = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    \n    # Your code here\n    pass\n",
  "learn_section": "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",
  "title": "Implement Lasso Regression using Gradient Descent",
  "contributor": [
    {
      "profile_link": "https://github.com/afrenkai",
      "name": "Artem Frenk"
    },
    {
      "profile_link": "https://github.com/JerryWu-code",
      "name": "JerryWu-code"
    }
  ],
  "createdAt": "December 15, 2025 at 7:32:49â€¯AM UTC-0500",
  "description_decoded": "Implement the Lasso Regression algorithm using Gradient Descent. Lasso Regression (Least Absolute Shrinkage and Selection Operator) uses L1 regularization, which adds a penalty equal to the absolute value of the coefficients to the loss function. This encourages sparsity in the model, effectively performing feature selection by shrinking some coefficients to zero.\n\nThe objective function of Lasso Regression is:\n$$\nJ(w, b) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( y_i - \\left( \\sum_{j=1}^{p} X_{ij} w_j + b \\right) \\right)^2 + \\alpha \\sum_{j=1}^{p} | w_j |\n$$\n\nWhere:\n- $n$ is the number of samples\n- $p$ is the number of features\n- $y_i$ is the actual value for the $i$-th sample\n- $\\hat{y}_i = \\sum_{j=1}^{p} X_{ij} w_j + b$ is the predicted value\n- $w_j$ is the weight for the $j$-th feature\n- $b$ is the bias term (not regularized)\n- $\\alpha$ is the regularization strength\n\nYour task is to implement gradient descent to minimize this objective function, returning the optimized weights and bias.",
  "learn_section_decoded": "## Understanding Lasso Regression and L1 Regularization\n\nLasso (Least Absolute Shrinkage and Selection Operator) Regression adds an L1 penalty to ordinary least squares regression. Unlike Ridge Regression (L2), Lasso can shrink coefficients to exactly zero, making it useful for **feature selection**.\n\n### Objective Function\n\n$$\nJ(w, b) = \\underbrace{\\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}_{\\text{MSE Loss}} + \\underbrace{\\alpha \\sum_{j=1}^{p} |w_j|}_{\\text{L1 Penalty}}\n$$\n\n### Gradient Derivation\n\n**For weights** (with L1 subgradient):\n$$\n\\frac{\\partial J}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}(\\hat{y}_i - y_i) + \\alpha \\cdot \\text{sign}(w_j)\n$$\n\n**For bias** (no regularization):\n$$\n\\frac{\\partial J}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)\n$$\n\n### Why No $\\frac{1}{n}$ in the L1 Gradient?\n\nA common point of confusion: the L1 penalty sums over **features** ($p$ terms), not samples ($n$ terms):\n$$\n\\frac{\\partial}{\\partial w_j} \\alpha \\sum_{k=1}^{p} |w_k| = \\alpha \\cdot \\text{sign}(w_j)\n$$\n\nOnly the $|w_j|$ term contributes when differentiating with respect to $w_j$. The summation doesn't introduce a normalizing factor.\n\n### The Subgradient at Zero\n\nThe function $|w|$ is not differentiable at $w = 0$. We use the **subgradient** convention:\n$$\n\\text{sign}(w) = \\begin{cases} +1 & w > 0 \\\\ 0 & w = 0 \\\\ -1 & w < 0 \\end{cases}\n$$\n\nSetting $\\text{sign}(0) = 0$ is a practical choice that allows gradient descent to continue.\n\n### Algorithm Steps\n\n1. **Initialize**: Set $w = \\mathbf{0}$, $b = 0$\n\n2. **Iterate** until convergence:\n   - Predict: $\\hat{y} = Xw + b$\n   - Compute gradients:\n     - $\\nabla_w = \\frac{1}{n} X^T (\\hat{y} - y) + \\alpha \\cdot \\text{sign}(w)$\n     - $\\nabla_b = \\frac{1}{n} \\sum (\\hat{y} - y)$\n   - Update: $w \\leftarrow w - \\eta \\nabla_w$, $b \\leftarrow b - \\eta \\nabla_b$\n\n3. **Convergence check**: Stop when $\\|\\nabla_w\\|_1 < \\text{tol}$\n\n### Lasso vs Ridge\n\n| Property | Lasso (L1) | Ridge (L2) |\n|----------|-----------|------------|\n| Penalty | $\\alpha \\sum |w_j|$ | $\\alpha \\sum w_j^2$ |\n| Sparsity | Yes (exact zeros) | No (small but non-zero) |\n| Feature Selection | Built-in | No |\n| Correlated Features | Tends to select one | Distributes weight evenly |\n"
}