{
  "description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gdG8gY2FsY3VsYXRlIHRoZSBjb3ZhcmlhbmNlIG1hdHJpeCBmb3IgYSBnaXZlbiBzZXQgb2YgdmVjdG9ycy4gVGhlIGZ1bmN0aW9uIHNob3VsZCB0YWtlIGEgbGlzdCBvZiBsaXN0cywgd2hlcmUgZWFjaCBpbm5lciBsaXN0IHJlcHJlc2VudHMgYSBmZWF0dXJlIHdpdGggaXRzIG9ic2VydmF0aW9ucywgYW5kIHJldHVybiBhIGNvdmFyaWFuY2UgbWF0cml4IGFzIGEgbGlzdCBvZiBsaXN0cy4gQWRkaXRpb25hbGx5LCBwcm92aWRlIHRlc3QgY2FzZXMgdG8gdmVyaWZ5IHRoZSBjb3JyZWN0bmVzcyBvZiB5b3VyIGltcGxlbWVudGF0aW9uLg==",
  "mdx_file": "96bf8f65-c61a-4c45-b85f-26a01610cb1b.mdx",
  "id": "10",
  "test_cases": [
    {
      "test": "print(calculate_covariance_matrix([[1, 2, 3], [4, 5, 6]]))",
      "expected_output": "[[1.0, 1.0], [1.0, 1.0]]"
    },
    {
      "test": "print(calculate_covariance_matrix([[1, 5, 6], [2, 3, 4], [7, 8, 9]]))",
      "expected_output": "[[7.0, 2.5, 2.5], [2.5, 1.0, 1.0], [2.5, 1.0, 1.0]]"
    }
  ],
  "difficulty": "easy",
  "pytorch_difficulty": "easy",
  "likes": "0",
  "video": "https://youtu.be/Mmuz3a4idg4",
  "cuda_difficulty": "hard",
  "cuda_test_cases": [
    {
      "test": "#include <iostream>\n#include <vector>\n\nstd::vector<std::vector<float>> calculate_covariance_matrix(const std::vector<std::vector<float>>& vectors);\n\nvoid print_matrix(const std::vector<std::vector<float>>& m) {\n    std::cout << \"[\";\n    for (int i = 0; i < m.size(); i++) {\n        std::cout << \"[\";\n        for (int j = 0; j < m[i].size(); j++) {\n            std::cout << m[i][j];\n            if (j < m[i].size() - 1) std::cout << \", \";\n        }\n        std::cout << \"]\";\n        if (i < m.size() - 1) std::cout << \", \";\n    }\n    std::cout << \"]\" << std::endl;\n}\n\nint main() {\n    std::vector<std::vector<float>> vectors = {{1, 2, 3}, {4, 5, 6}};\n    auto result = calculate_covariance_matrix(vectors);\n    print_matrix(result);\n    return 0;\n}",
      "expected_output": "[[1, 1], [1, 1]]"
    },
    {
      "test": "#include <iostream>\n#include <vector>\n\nstd::vector<std::vector<float>> calculate_covariance_matrix(const std::vector<std::vector<float>>& vectors);\n\nvoid print_matrix(const std::vector<std::vector<float>>& m) {\n    std::cout << \"[\";\n    for (int i = 0; i < m.size(); i++) {\n        std::cout << \"[\";\n        for (int j = 0; j < m[i].size(); j++) {\n            std::cout << m[i][j];\n            if (j < m[i].size() - 1) std::cout << \", \";\n        }\n        std::cout << \"]\";\n        if (i < m.size() - 1) std::cout << \", \";\n    }\n    std::cout << \"]\" << std::endl;\n}\n\nint main() {\n    std::vector<std::vector<float>> vectors = {{1, 5, 6}, {2, 3, 4}, {7, 8, 9}};\n    auto result = calculate_covariance_matrix(vectors);\n    print_matrix(result);\n    return 0;\n}",
      "expected_output": "[[7, 2.5, 2.5], [2.5, 1, 1], [2.5, 1, 1]]"
    }
  ],
  "dislikes": "0",
  "example": {
    "input": "[[1, 2, 3], [4, 5, 6]]",
    "output": "[[1.0, 1.0], [1.0, 1.0]]",
    "reasoning": "The covariance between the two features is calculated based on their deviations from the mean. For the given vectors, both covariances are 1.0, resulting in a symmetric covariance matrix."
  },
  "category": "Statistics",
  "starter_code": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n\t# Your code here\n\treturn []",
  "learn_section": "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",
  "cuda_starter_code": "I2luY2x1ZGUgPGN1ZGFfcnVudGltZS5oPgojaW5jbHVkZSA8aW9zdHJlYW0+CiNpbmNsdWRlIDx2ZWN0b3I+CgpfX2dsb2JhbF9fIHZvaWQgY29tcHV0ZV9tZWFuc19rZXJuZWwoCiAgICBjb25zdCBmbG9hdCogZGF0YSwKICAgIGZsb2F0KiBtZWFucywKICAgIGludCBuX2ZlYXR1cmVzLAogICAgaW50IG5fb2JzZXJ2YXRpb25zCikgewogICAgLy8gSW1wbGVtZW50IGtlcm5lbCB0byBjb21wdXRlIG1lYW4gb2YgZWFjaCBmZWF0dXJlCn0KCl9fZ2xvYmFsX18gdm9pZCBjb21wdXRlX2NvdmFyaWFuY2Vfa2VybmVsKAogICAgY29uc3QgZmxvYXQqIGRhdGEsCiAgICBjb25zdCBmbG9hdCogbWVhbnMsCiAgICBmbG9hdCogY292YXJpYW5jZSwKICAgIGludCBuX2ZlYXR1cmVzLAogICAgaW50IG5fb2JzZXJ2YXRpb25zCikgewogICAgLy8gSW1wbGVtZW50IGtlcm5lbCB0byBjb21wdXRlIGNvdmFyaWFuY2UgbWF0cml4CiAgICAvLyBjb3YoaSxqKSA9IHN1bSgoeF9pIC0gbWVhbl9pKSAqICh4X2ogLSBtZWFuX2opKSAvIChuLTEpCn0KCnN0ZDo6dmVjdG9yPHN0ZDo6dmVjdG9yPGZsb2F0Pj4gY2FsY3VsYXRlX2NvdmFyaWFuY2VfbWF0cml4KGNvbnN0IHN0ZDo6dmVjdG9yPHN0ZDo6dmVjdG9yPGZsb2F0Pj4mIHZlY3RvcnMpIHsKICAgIC8vIHZlY3RvcnM6IG5fZmVhdHVyZXMgeCBuX29ic2VydmF0aW9ucwogICAgLy8gMS4gQ29tcHV0ZSBtZWFucyBmb3IgZWFjaCBmZWF0dXJlCiAgICAvLyAyLiBDb21wdXRlIGNvdmFyaWFuY2UgbWF0cml4CiAgICByZXR1cm4ge307Cn0=",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    },
    {
      "profile_link": "https://github.com/Selbl",
      "name": "Selbl"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "import torch\nv = [[1.0,2.0,3.0],[4.0,5.0,6.0]]\ncov = calculate_covariance_matrix(v)\nprint(cov.detach().numpy().tolist())",
      "expected_output": "[[1.0, 1.0], [1.0, 1.0]]"
    },
    {
      "test": "import torch\nv = [[1.0,2.0,3.0],[3.0,3.0,3.0]]\ncov = calculate_covariance_matrix(v)\nprint(cov.detach().numpy().tolist())",
      "expected_output": "[[1.0, 0.0], [0.0, 0.0]]"
    }
  ],
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpkZWYgY2FsY3VsYXRlX2NvdmFyaWFuY2VfbWF0cml4KHZlY3RvcnMpIC0+IHRvcmNoLlRlbnNvcjoKICAgICIiIgogICAgQ2FsY3VsYXRlIHRoZSBjb3ZhcmlhbmNlIG1hdHJpeCBmb3IgZ2l2ZW4gZmVhdHVyZSB2ZWN0b3JzIHVzaW5nIFB5VG9yY2guCiAgICBJbnB1dDogMkQgYXJyYXktbGlrZSBvZiBzaGFwZSAobl9mZWF0dXJlcywgbl9vYnNlcnZhdGlvbnMpLgogICAgUmV0dXJucyBhIHRlbnNvciBvZiBzaGFwZSAobl9mZWF0dXJlcywgbl9mZWF0dXJlcykuCiAgICAiIiIKICAgIHZfdCA9IHRvcmNoLmFzX3RlbnNvcih2ZWN0b3JzLCBkdHlwZT10b3JjaC5mbG9hdCkKICAgICMgWW91ciBpbXBsZW1lbnRhdGlvbiBoZXJlCiAgICBwYXNzCg==",
  "title": "Calculate Covariance Matrix",
  "description_decoded": "Write a Python function to calculate the covariance matrix for a given set of vectors. The function should take a list of lists, where each inner list represents a feature with its observations, and return a covariance matrix as a list of lists. Additionally, provide test cases to verify the correctness of your implementation.",
  "learn_section_decoded": "## Understanding Covariance Matrix\n\nThe covariance matrix is a fundamental concept in statistics and machine learning, used to understand the relationship between multiple variables (features) in a dataset. It quantifies the degree to which two variables change together.\n\n### Key Concepts\n\n- **Covariance**: Measures the directional relationship between two random variables. A positive covariance indicates that the variables increase together, while a negative covariance indicates that one variable increases as the other decreases.\n- **Covariance Matrix**: For a dataset with $n$ features, the covariance matrix is an $n \\times n$ matrix where each element $(i, j)$ represents the covariance between the $i^{th}$ and $j^{th}$ features.\n\n### Covariance Formula\n\nThe covariance between two variables $X$ and $Y$ is calculated as:\n\n$$\n\\text{cov}(X, Y) = \\frac{\\sum_{k=1}^{m} (X_k - \\bar{X})(Y_k - \\bar{Y})}{m - 1}\n$$\n\nWhere:\n\n- $X_k$ and $Y_k$ are the individual observations of variables $X$ and $Y$.\n- $\\bar{X}$ and $\\bar{Y}$ are the means of $X$ and $Y$.\n- $m$ is the number of observations.\n\n### Constructing the Covariance Matrix\n\nGiven a dataset with $n$ features, the covariance matrix is constructed as follows:\n\n1. **Calculate the Mean**: Compute the mean of each feature.\n2. **Compute Covariance**: For each pair of features, calculate the covariance using the formula above.\n3. **Populate the Matrix**: Place the computed covariance values in the corresponding positions in the matrix. The diagonal elements represent the variance of each feature.\n\n$$\n\\text{Covariance Matrix} =\n\\begin{bmatrix}\n\\text{cov}(X_1, X_1) & \\text{cov}(X_1, X_2) & \\cdots & \\text{cov}(X_1, X_n) \\\\\n\\text{cov}(X_2, X_1) & \\text{cov}(X_2, X_2) & \\cdots & \\text{cov}(X_2, X_n) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\text{cov}(X_n, X_1) & \\text{cov}(X_n, X_2) & \\cdots & \\text{cov}(X_n, X_n) \\\\\n\\end{bmatrix}\n$$\n\n### Example Calculation\n\nConsider the following dataset with two features:\n\n$$\n\\begin{align*}\n\\text{Feature 1} &: [1, 2, 3] \\\\\n\\text{Feature 2} &: [4, 5, 6]\n\\end{align*}\n$$\n\n1. **Calculate Means**:\n   $$\n   \\bar{X}_1 = \\frac{1 + 2 + 3}{3} = 2.0 \\\\\n   \\bar{X}_2 = \\frac{4 + 5 + 6}{3} = 5.0\n   $$\n\n2. **Compute Covariances**:\n   $$\n   \\text{cov}(X_1, X_1) = \\frac{(1-2)^2 + (2-2)^2 + (3-2)^2}{3-1} = 1.0 \\\\\n   \\text{cov}(X_1, X_2) = \\frac{(1-2)(4-5) + (2-2)(5-5) + (3-2)(6-5)}{3-1} = 1.0 \\\\\n   \\text{cov}(X_2, X_2) = \\frac{(4-5)^2 + (5-5)^2 + (6-5)^2}{3-1} = 1.0\n   $$\n\n3. **Covariance Matrix**:\n   $$\n   \\begin{bmatrix}\n   1.0 & 1.0 \\\\\n   1.0 & 1.0 \n   \\end{bmatrix}\n   $$\n\n### Applications\n\nCovariance matrices are widely used in various fields, including:\n\n- **Principal Component Analysis (PCA)**: Reducing the dimensionality of datasets while preserving variance.\n- **Portfolio Optimization**: Understanding the variance and covariance between different financial assets.\n- **Multivariate Statistics**: Analyzing the relationships between multiple variables simultaneously.\n\nUnderstanding the covariance matrix is crucial for interpreting the relationships in multivariate data and for performing advanced statistical analyses.\n"
}