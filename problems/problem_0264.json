{
  "description": "SW1wbGVtZW50IHRoZSBoeXBlcmJvbGljIHRhbmdlbnQgKHRhbmgpIGFjdGl2YXRpb24gZnVuY3Rpb24sIGEgY2xhc3NpYyBhY3RpdmF0aW9uIGZ1bmN0aW9uIHVzZWQgaW4gbmV1cmFsIG5ldHdvcmtzLiBUaGUgdGFuaCBmdW5jdGlvbiBtYXBzIGFueSByZWFsLXZhbHVlZCBudW1iZXIgdG8gYSB2YWx1ZSBiZXR3ZWVuIC0xIGFuZCAxLiBZb3VyIHRhc2sgaXMgdG8gY29tcHV0ZSB0aGUgdGFuaCB2YWx1ZSBmb3IgYSBnaXZlbiBpbnB1dCB1c2luZyB0aGUgbWF0aGVtYXRpY2FsIGZvcm11bGEgaW52b2x2aW5nIGV4cG9uZW50aWFscy4gVGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gdGhlIHJlc3VsdCByb3VuZGVkIHRvIDQgZGVjaW1hbCBwbGFjZXMu",
  "id": "264",
  "test_cases": [
    {
      "test": "print(tanh(0))",
      "expected_output": "0.0"
    },
    {
      "test": "print(tanh(1))",
      "expected_output": "0.7616"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "tanh(1)",
    "output": "0.7616",
    "reasoning": "For x = 1, we compute e^1 = 2.7183 and e^(-1) = 0.3679. Then tanh(1) = (2.7183 - 0.3679) / (2.7183 + 0.3679) = 2.3504 / 3.0862 = 0.7616."
  },
  "category": "Deep Learning",
  "starter_code": "import math\n\ndef tanh(x: float) -> float:\n\t\"\"\"\n\tImplements the Tanh (hyperbolic tangent) activation function.\n\n\tArgs:\n\t\tx (float): Input value\n\n\tReturns:\n\t\tfloat: The tanh of the input, rounded to 4 decimal places\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Implement the Tanh Activation Function",
  "createdAt": "December 15, 2025 at 7:02:03â€¯AM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement the hyperbolic tangent (tanh) activation function, a classic activation function used in neural networks. The tanh function maps any real-valued number to a value between -1 and 1. Your task is to compute the tanh value for a given input using the mathematical formula involving exponentials. The function should return the result rounded to 4 decimal places.",
  "learn_section_decoded": "## Understanding the Tanh Activation Function\n\nThe hyperbolic tangent (tanh) activation function is one of the most widely used activation functions in neural networks, particularly in recurrent neural networks (RNNs) and hidden layers of feedforward networks.\n\n### Mathematical Definition\n\nThe tanh function is mathematically defined as:\n\n$$\n\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n$$\n\nWhere:\n- $x$ is the input to the function\n- $e$ is Euler's number (approximately 2.71828)\n\nThis can also be expressed in terms of the sigmoid function $\\sigma(x)$:\n\n$$\n\\tanh(x) = 2\\sigma(2x) - 1\n$$\n\n### Characteristics\n\n- **Output Range:** The output is bounded between -1 and 1, i.e., $\\tanh(x) \\in (-1, 1)$.\n- **Zero-Centered:** Unlike the sigmoid function, tanh is centered around zero. This means the output can be negative, positive, or zero.\n- **Symmetry:** The function is symmetric around the origin, meaning $\\tanh(-x) = -\\tanh(x)$.\n- **Saturation:** For large positive or negative inputs, the function saturates (approaches +1 or -1), which can lead to vanishing gradients.\n\n### Derivative\n\nThe derivative of tanh is useful for backpropagation:\n\n$$\n\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x)\n$$\n\nThis derivative has a maximum value of 1 at $x = 0$ and approaches 0 as $|x|$ increases.\n\n### Comparison with Sigmoid\n\n| Property | Sigmoid | Tanh |\n|----------|---------|------|\n| Range | (0, 1) | (-1, 1) |\n| Zero-centered | No | Yes |\n| Max gradient | 0.25 | 1.0 |\n\n### When to Use Tanh\n\n- **Hidden Layers:** Tanh is often preferred over sigmoid in hidden layers because its zero-centered output helps with gradient-based optimization.\n- **RNNs and LSTMs:** Commonly used in recurrent architectures to regulate the flow of information.\n- **Output Layer:** When the target values are in the range [-1, 1].\n\n### Limitations\n\n1. **Vanishing Gradients:** For very large or very small inputs, the gradient approaches zero, making learning slow.\n2. **Computational Cost:** Computing exponentials is more expensive than simpler functions like ReLU.\n\nDespite these limitations, tanh remains an important activation function in many deep learning architectures, especially where zero-centered outputs are beneficial."
}