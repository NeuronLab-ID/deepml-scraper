{
  "description": "SW1wbGVtZW50IHdlaWdodCBkZWNheSAoTDIgcmVndWxhcml6YXRpb24pIGZvciBuZXVyYWwgbmV0d29yayBwYXJhbWV0ZXJzLiBHaXZlbiBwYXJhbWV0ZXIgYXJyYXlzLCB0aGVpciBncmFkaWVudHMsIGEgbGVhcm5pbmcgcmF0ZSwgYW5kIGEgd2VpZ2h0IGRlY2F5IGZhY3RvciwgYXBwbHkgdGhlIHBhcmFtZXRlciB1cGRhdGUgdGhhdCBpbmNsdWRlcyBib3RoIGdyYWRpZW50IGRlc2NlbnQgYW5kIEwyIHJlZ3VsYXJpemF0aW9uLiBUaGUgZnVuY3Rpb24gc2hvdWxkIHRha2UgYSBib29sZWFuIGxpc3QgaW5kaWNhdGluZyB3aGljaCBwYXJhbWV0ZXIgZ3JvdXBzIHNob3VsZCBoYXZlIHdlaWdodCBkZWNheSBhcHBsaWVkICh0eXBpY2FsbHksIHdlaWdodCBkZWNheSBpcyBhcHBsaWVkIHRvIHdlaWdodHMgYnV0IG5vdCB0byBiaWFzZXMpLiBSZXR1cm4gdGhlIHVwZGF0ZWQgcGFyYW1ldGVycyBhZnRlciBhcHBseWluZyB0aGUgYXBwcm9wcmlhdGUgdXBkYXRlIHJ1bGUu",
  "id": "198",
  "test_cases": [
    {
      "test": "print([[round(x, 4) for x in arr] for arr in apply_weight_decay([[1.0, 2.0, 3.0]], [[0.1, 0.2, 0.3]], 0.1, 0.01, [True])])",
      "expected_output": "[[0.989, 1.978, 2.967]]"
    },
    {
      "test": "print([[round(x, 4) for x in arr] for arr in apply_weight_decay([[0.5]], [[0.1]], 0.1, 0.01, [False])])",
      "expected_output": "[[0.49]]"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "parameters=[[1.0, 2.0]], gradients=[[0.1, 0.2]], lr=0.1, weight_decay=0.01, apply_to_all=[True]",
    "output": "[[0.989, 1.978]]",
    "reasoning": "For the first parameter (1.0): $1.0 - 0.1 \\times 0.1 - 0.1 \\times 0.01 \\times 1.0 = 1.0 - 0.01 - 0.001 = 0.989$. For the second parameter (2.0): $2.0 - 0.1 \\times 0.2 - 0.1 \\times 0.01 \\times 2.0 = 2.0 - 0.02 - 0.002 = 1.978$."
  },
  "category": "Regularization",
  "starter_code": "def apply_weight_decay(parameters: list[list[float]], gradients: list[list[float]], \n                       lr: float, weight_decay: float, apply_to_all: list[bool]) -> list[list[float]]:\n\t\"\"\"\n\tApply weight decay (L2 regularization) to parameters.\n\t\n\tArgs:\n\t\tparameters: List of parameter arrays\n\t\tgradients: List of gradient arrays\n\t\tlr: Learning rate\n\t\tweight_decay: Weight decay factor\n\t\tapply_to_all: Boolean list indicating which parameter groups get weight decay\n\t\n\tReturns:\n\t\tUpdated parameters\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Implement Weight Decay as L2 Regularization",
  "createdAt": "November 9, 2025 at 4:01:49 PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement weight decay (L2 regularization) for neural network parameters. Given parameter arrays, their gradients, a learning rate, and a weight decay factor, apply the parameter update that includes both gradient descent and L2 regularization. The function should take a boolean list indicating which parameter groups should have weight decay applied (typically, weight decay is applied to weights but not to biases). Return the updated parameters after applying the appropriate update rule.",
  "learn_section_decoded": "### Understanding Weight Decay and L2 Regularization\n\nWeight decay is a regularization technique used to prevent overfitting in neural networks by penalizing large weights. It's closely related to L2 regularization, though they're implemented slightly differently.\n\n#### The Regularization Problem\n\nWithout regularization, neural networks can develop very large weights that lead to:\n- **Overfitting**: Model memorizes training data instead of learning generalizable patterns\n- **Poor generalization**: High training accuracy but low test accuracy\n- **Numerical instability**: Large weights can cause gradient explosions\n\n#### L2 Regularization in Loss Function\n\nL2 regularization adds a penalty term to the loss function:\n\n$$\nL_{total} = L_{original} + \\frac{\\lambda}{2} \\sum_{i} w_i^2\n$$\n\nWhere:\n- $L_{original}$ is the original loss (e.g., cross-entropy)\n- $\\lambda$ is the regularization strength (weight decay factor)\n- $w_i$ are the model weights\n- The $\\frac{1}{2}$ factor is for mathematical convenience in derivatives\n\n#### Weight Decay Update Rule\n\nWhen we take the gradient of the regularized loss and apply gradient descent, we get:\n\n$$\nw_{new} = w - \\eta \\frac{\\partial L_{total}}{\\partial w}\n$$\n\nExpanding the gradient:\n\n$$\n\\frac{\\partial L_{total}}{\\partial w} = \\frac{\\partial L_{original}}{\\partial w} + \\lambda w\n$$\n\nSubstituting back:\n\n$$\nw_{new} = w - \\eta \\left(\\frac{\\partial L_{original}}{\\partial w} + \\lambda w\\right)\n$$\n\nThis simplifies to:\n\n$$\nw_{new} = w - \\eta \\cdot g - \\eta \\cdot \\lambda \\cdot w\n$$\n\nWhere $g = \\frac{\\partial L_{original}}{\\partial w}$ is the gradient.\n\nThis can be rewritten as:\n\n$$\nw_{new} = (1 - \\eta \\lambda) w - \\eta \\cdot g\n$$\n\nThe term $(1 - \\eta \\lambda)$ causes the weights to \"decay\" toward zero at each step, hence the name \"weight decay.\"\n\n#### Key Components\n\n**Parameters:**\n- $w$: Current weight value\n- $g$: Gradient of the loss with respect to the weight\n- $\\eta$: Learning rate\n- $\\lambda$: Weight decay factor (typically 0.0001 to 0.01)\n\n**Update Formula:**\n```\nw_new = w - lr * grad - lr * weight_decay * w\n```\n\n#### Why Exclude Biases?\n\nBy convention, weight decay is **not applied to bias terms**. The reasons are:\n\n1. **Biases don't cause overfitting**: Biases shift the activation function but don't amplify inputs like weights do\n2. **Common practice**: Empirically, excluding biases from regularization works better\n3. **Theoretical justification**: L2 regularization penalizes complexity; biases don't add model complexity\n\n#### Example Walkthrough\n\nGiven:\n- Weight: $w = 1.0$\n- Gradient: $g = 0.1$\n- Learning rate: $\\eta = 0.1$\n- Weight decay: $\\lambda = 0.01$\n\n**Step-by-step calculation:**\n\n1. **Gradient descent term**: $\\eta \\cdot g = 0.1 \\times 0.1 = 0.01$\n\n2. **Weight decay term**: $\\eta \\cdot \\lambda \\cdot w = 0.1 \\times 0.01 \\times 1.0 = 0.001$\n\n3. **Updated weight**: \n   $$w_{new} = 1.0 - 0.01 - 0.001 = 0.989$$\n\n#### L2 Regularization vs Weight Decay\n\nWhile often used interchangeably, there's a subtle difference:\n\n| Aspect | L2 Regularization | Weight Decay |\n|--------|------------------|-------------|\n| **Implementation** | Add penalty to loss | Directly modify weight update |\n| **With momentum** | Different behavior | Simpler, more effective |\n| **With adaptive learning rates (Adam)** | Not equivalent | Often preferred |\n| **Mathematical equivalence** | Only for standard SGD | Direct implementation |\n\nFor standard SGD, they're mathematically equivalent. For advanced optimizers (Adam, RMSprop), weight decay is often preferred as \"decoupled weight decay.\"\n\n#### Practical Tips\n\n1. **Typical values**: Start with $\\lambda = 0.0001$ or $0.01$\n2. **Hyperparameter tuning**: Weight decay is a crucial hyperparameter to tune\n3. **Layer-specific decay**: Sometimes different layers use different decay rates\n4. **Monitor training**: Too much decay → underfitting; too little → overfitting\n\n#### Effect on Training\n\n**Without weight decay:**\n- Weights can grow arbitrarily large\n- Model may overfit to training data\n- High variance in predictions\n\n**With weight decay:**\n- Weights stay relatively small\n- Better generalization to unseen data\n- Smoother decision boundaries\n- More stable training\n\n#### Implementation in Popular Frameworks\n\n- **PyTorch**: `torch.optim.SGD(params, lr=0.1, weight_decay=0.01)`\n- **TensorFlow**: `tf.keras.optimizers.SGD(learning_rate=0.1, weight_decay=0.01)`\n- Both frameworks automatically exclude biases by default (if configured properly)\n\nWeight decay is one of the most effective and widely-used regularization techniques in deep learning, helping models generalize better while remaining simple to implement."
}