{
  "description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gY29tcHV0ZSBwYXJ0aWFsIGRlcml2YXRpdmVzIG9mIG11bHRpdmFyaWFibGUgZnVuY3Rpb25zIGF0IGEgZ2l2ZW4gcG9pbnQuIFBhcnRpYWwgZGVyaXZhdGl2ZXMgbWVhc3VyZSB0aGUgcmF0ZSBvZiBjaGFuZ2Ugd2l0aCByZXNwZWN0IHRvIG9uZSB2YXJpYWJsZSB3aGlsZSBob2xkaW5nIG90aGVycyBjb25zdGFudC4gR2l2ZW4gYSBmdW5jdGlvbiBuYW1lIGFuZCBhIHBvaW50LCByZXR1cm4gdGhlIHR1cGxlIG9mIGFsbCBwYXJ0aWFsIGRlcml2YXRpdmVzIGF0IHRoYXQgcG9pbnQu",
  "id": "215",
  "test_cases": [
    {
      "test": "result = compute_partial_derivatives('poly2d', (2.0, 3.0)); print(f\"{result[0]:.1f},{result[1]:.1f}\")",
      "expected_output": "21.0,16.0"
    },
    {
      "test": "result = compute_partial_derivatives('exp_sum', (1.0, 0.0)); print(f\"{result[0]:.6f}\")",
      "expected_output": "2.718282"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "func_name='poly2d', point=(2.0, 3.0)",
    "output": "(21.0, 16.0)",
    "reasoning": "f(x,y) = x²y + xy². ∂f/∂x = 2xy + y² = 2(2)(3) + 9 = 21. ∂f/∂y = x² + 2xy = 4 + 2(2)(3) = 16. Gradient at (2,3) is (21, 16)."
  },
  "category": "Calculus",
  "starter_code": "import numpy as np\n\ndef compute_partial_derivatives(func_name: str, point: tuple[float, ...]) -> tuple[float, ...]:\n\t\"\"\"\n\tCompute partial derivatives of multivariable functions.\n\t\n\tArgs:\n\t\tfunc_name: Function identifier\n\t\t\t'poly2d': f(x,y) = x²y + xy²\n\t\t\t'exp_sum': f(x,y) = e^(x+y)\n\t\t\t'product_sin': f(x,y) = x·sin(y)\n\t\t\t'poly3d': f(x,y,z) = x²y + yz²\n\t\t\t'squared_error': f(x,y) = (x-y)²\n\t\tpoint: Point (x, y) or (x, y, z) at which to evaluate\n\t\n\tReturns:\n\t\tTuple of partial derivatives (∂f/∂x, ∂f/∂y, ...) at point\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Partial Derivatives of Multivariable Functions",
  "createdAt": "November 24, 2025 at 6:07:45 PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement a function to compute partial derivatives of multivariable functions at a given point. Partial derivatives measure the rate of change with respect to one variable while holding others constant. Given a function name and a point, return the tuple of all partial derivatives at that point.",
  "learn_section_decoded": "### Partial Derivatives\n\nPartial derivatives measure the rate of change of a multivariable function with respect to one variable while holding others constant.\n\n#### Definition\n\nFor $f(x, y)$, the partial derivatives are:\n\n$$\n\\frac{\\partial f}{\\partial x} = \\lim_{h \\to 0} \\frac{f(x+h, y) - f(x, y)}{h}\n$$\n\n$$\n\\frac{\\partial f}{\\partial y} = \\lim_{h \\to 0} \\frac{f(x, y+h) - f(x, y)}{h}\n$$\n\n**Key idea**: Treat other variables as constants.\n\n#### Notation\n\nMultiple notations for partial derivatives:\n\n$$\n\\frac{\\partial f}{\\partial x} = \\partial_x f = f_x = D_x f\n$$\n\n#### Computing Partial Derivatives\n\n**Rule**: To compute $\\partial f/\\partial x$, treat all other variables as constants and differentiate normally.\n\n**Example**: $f(x, y) = x^2y + xy^2$\n\n**For $\\partial f/\\partial x$** (treat $y$ as constant):\n$$\n\\frac{\\partial f}{\\partial x} = 2xy + y^2\n$$\n\n**For $\\partial f/\\partial y$** (treat $x$ as constant):\n$$\n\\frac{\\partial f}{\\partial y} = x^2 + 2xy\n$$\n\n#### Calculation Steps\n\n**Function**: $f(x, y) = x^2y + xy^2$ at $(2, 3)$\n\n**Step 1**: Compute $\\partial f/\\partial x$\n$$\n\\frac{\\partial f}{\\partial x} = 2xy + y^2\n$$\n\n**Step 2**: Evaluate at $(2, 3)$\n$$\n\\frac{\\partial f}{\\partial x}\\bigg|_{(2,3)} = 2(2)(3) + 3^2 = 12 + 9 = 21\n$$\n\n**Step 3**: Compute $\\partial f/\\partial y$\n$$\n\\frac{\\partial f}{\\partial y} = x^2 + 2xy\n$$\n\n**Step 4**: Evaluate at $(2, 3)$\n$$\n\\frac{\\partial f}{\\partial y}\\bigg|_{(2,3)} = 2^2 + 2(2)(3) = 4 + 12 = 16\n$$\n\n**Result**: $\\nabla f(2, 3) = (21, 16)$\n\n#### The Gradient\n\nThe **gradient** is the vector of all partial derivatives:\n\n$$\n\\nabla f = \\left(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}, \\ldots\\right)\n$$\n\n**Properties**:\n- Points in direction of steepest ascent\n- Magnitude = rate of steepest increase\n- Perpendicular to level curves/surfaces\n\n#### Common Patterns\n\n**Power rule**: $f(x, y) = x^n y^m$\n$$\n\\frac{\\partial f}{\\partial x} = nx^{n-1}y^m, \\quad \\frac{\\partial f}{\\partial y} = mx^n y^{m-1}\n$$\n\n**Product rule**: $f(x, y) = g(x)h(y)$\n$$\n\\frac{\\partial f}{\\partial x} = g'(x)h(y), \\quad \\frac{\\partial f}{\\partial y} = g(x)h'(y)\n$$\n\n**Exponential**: $f(x, y) = e^{x+y}$\n$$\n\\frac{\\partial f}{\\partial x} = e^{x+y}, \\quad \\frac{\\partial f}{\\partial y} = e^{x+y}\n$$\n\n**Chain rule**: $f(x, y) = g(h(x, y))$\n$$\n\\frac{\\partial f}{\\partial x} = g'(h(x,y)) \\cdot \\frac{\\partial h}{\\partial x}\n$$\n\n#### Three or More Variables\n\nFor $f(x, y, z) = x^2y + yz^2$:\n\n$$\n\\begin{align}\n\\frac{\\partial f}{\\partial x} &= 2xy \\\\\n\\frac{\\partial f}{\\partial y} &= x^2 + z^2 \\\\\n\\frac{\\partial f}{\\partial z} &= 2yz\n\\end{align}\n$$\n\nAt $(1, 2, 3)$: $\\nabla f = (4, 10, 12)$\n\n#### Geometric Interpretation\n\n$\\partial f/\\partial x$ at $(x_0, y_0)$:\n- Slope of curve formed by slicing surface $z = f(x, y)$ with plane $y = y_0$\n- Rate of change moving in $x$-direction\n\n$\\partial f/\\partial y$ at $(x_0, y_0)$:\n- Slope of curve formed by slicing surface with plane $x = x_0$\n- Rate of change moving in $y$-direction\n\n#### Application: Gradient Descent\n\nTo minimize $f(x, y)$, update:\n\n$$\n\\begin{align}\nx_{\\text{new}} &= x_{\\text{old}} - \\alpha \\frac{\\partial f}{\\partial x} \\\\\ny_{\\text{new}} &= y_{\\text{old}} - \\alpha \\frac{\\partial f}{\\partial y}\n\\end{align}\n$$\n\nWhere $\\alpha$ is the learning rate.\n\n**Example**: Minimize $f(x, y) = (x-y)^2$\n\nPartial derivatives:\n$$\n\\frac{\\partial f}{\\partial x} = 2(x-y), \\quad \\frac{\\partial f}{\\partial y} = -2(x-y)\n$$\n\nAt $(5, 3)$: $\\nabla f = (4, -4)$\n\nWith $\\alpha = 0.1$:\n$$\n\\begin{align}\nx_{\\text{new}} &= 5 - 0.1(4) = 4.6 \\\\\ny_{\\text{new}} &= 3 - 0.1(-4) = 3.4\n\\end{align}\n$$\n\nMoving toward minimum at $x = y$.\n\n#### Higher-Order Partial Derivatives\n\n**Second-order partials**:\n$$\n\\frac{\\partial^2 f}{\\partial x^2}, \\quad \\frac{\\partial^2 f}{\\partial y^2}, \\quad \\frac{\\partial^2 f}{\\partial x \\partial y}\n$$\n\n**Clairaut's Theorem**: If continuous,\n$$\n\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x}\n$$\n\nMixed partials are equal (order doesn't matter).\n\n#### Machine Learning Applications\n\n**Loss function**: $L(w, b) = \\frac{1}{n}\\sum(y_i - (wx_i + b))^2$\n\nGradient:\n$$\n\\frac{\\partial L}{\\partial w} = -\\frac{2}{n}\\sum x_i(y_i - (wx_i + b))\n$$\n$$\n\\frac{\\partial L}{\\partial b} = -\\frac{2}{n}\\sum (y_i - (wx_i + b))\n$$\n\nUsed to update weights in gradient descent.\n\n**Neural networks**: Backpropagation computes $\\partial L/\\partial w_i$ for all weights.\n\n#### Key Differences from Single-Variable Calculus\n\n**Single variable**: One derivative $f'(x)$\n\n**Multiple variables**: Multiple partial derivatives, one per variable\n\n**Direction matters**: Change depends on which direction you move\n\n**Gradient**: Combines all partials into a vector"
}