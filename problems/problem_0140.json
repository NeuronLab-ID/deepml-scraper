{
  "description": "V3JpdGUgYSBQeXRob24gY2xhc3MgdG8gaW1wbGVtZW50IHRoZSBCZXJub3VsbGkgTmFpdmUgQmF5ZXMgY2xhc3NpZmllciBmb3IgYmluYXJ5ICgwLzEpIGZlYXR1cmUgZGF0YS4gWW91ciBjbGFzcyBzaG91bGQgaGF2ZSB0d28gbWV0aG9kczogYGZvcndhcmQoc2VsZiwgWCwgeSlgIHRvIHRyYWluIG9uIHRoZSBpbnB1dCBkYXRhIChYOiAyRCBOdW1QeSBhcnJheSBvZiBiaW5hcnkgZmVhdHVyZXMsIHk6IDFEIE51bVB5IGFycmF5IG9mIGNsYXNzIGxhYmVscykgYW5kIGBwcmVkaWN0KHNlbGYsIFgpYCB0byBvdXRwdXQgcHJlZGljdGVkIGxhYmVscyBmb3IgYSAyRCB0ZXN0IG1hdHJpeCBYLiBVc2UgTGFwbGFjZSBzbW9vdGhpbmcgKHBhcmFtZXRlcjogc21vb3RoaW5nPTEuMCkuIFJldHVybiBwcmVkaWN0aW9ucyBhcyBhIE51bVB5IGFycmF5LiBPbmx5IHVzZSBOdW1QeS4gUHJlZGljdGlvbnMgbXVzdCBiZSBiaW5hcnkgKDAgb3IgMSkgYW5kIHlvdSBtdXN0IGhhbmRsZSBjYXNlcyB3aGVyZSB0aGUgdHJhaW5pbmcgZGF0YSBjb250YWlucyBvbmx5IG9uZSBjbGFzcy4gQWxsIGxvZy9saWtlbGlob29kIGNhbGN1bGF0aW9ucyBzaG91bGQgdXNlIGxvZyBwcm9iYWJpbGl0aWVzIGZvciBudW1lcmljYWwgc3RhYmlsaXR5Lg==",
  "id": "140",
  "test_cases": [
    {
      "test": "import numpy as np\nmodel = NaiveBayes(smoothing=1.0)\nX = np.array([[1, 0, 1], [1, 1, 0], [0, 0, 1], [0, 1, 0], [1, 1, 1]])\ny = np.array([1, 1, 0, 0, 1])\nmodel.forward(X, y)\nprint(model.predict(np.array([[1, 0, 1]])))",
      "expected_output": "[1]"
    },
    {
      "test": "import numpy as np\nmodel = NaiveBayes(smoothing=1.0)\nX = np.array([[0], [1], [0], [1]])\ny = np.array([0, 1, 0, 1])\nmodel.forward(X, y)\nprint(model.predict(np.array([[0], [1]])))",
      "expected_output": "[0 1]"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "X = np.array([[1, 0, 1], [1, 1, 0], [0, 0, 1], [0, 1, 0], [1, 1, 1]]); y = np.array([1, 1, 0, 0, 1])\nmodel = NaiveBayes(smoothing=1.0)\nmodel.forward(X, y)\nprint(model.predict(np.array([[1, 0, 1]])))",
    "output": "[1]",
    "reasoning": "The model learns class priors and feature probabilities with Laplace smoothing. For [1, 0, 1], the posterior for class 1 is higher, so the model predicts 1."
  },
  "category": "Machine Learning",
  "starter_code": "import numpy as np\n\nclass NaiveBayes():\n    def __init__(self, smoothing=1.0):\n        # Initialize smoothing\n        pass\n\n    def forward(self, X, y):\n        # Fit model to binary features X and labels y\n        pass\n\n    def predict(self, X):\n        # Predict class labels for test set X\n        pass",
  "title": "Bernoulli Naive Bayes Classifier",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/Coder1010ayush",
      "name": "Coder1010ayush"
    }
  ],
  "description_decoded": "Write a Python class to implement the Bernoulli Naive Bayes classifier for binary (0/1) feature data. Your class should have two methods: `forward(self, X, y)` to train on the input data (X: 2D NumPy array of binary features, y: 1D NumPy array of class labels) and `predict(self, X)` to output predicted labels for a 2D test matrix X. Use Laplace smoothing (parameter: smoothing=1.0). Return predictions as a NumPy array. Only use NumPy. Predictions must be binary (0 or 1) and you must handle cases where the training data contains only one class. All log/likelihood calculations should use log probabilities for numerical stability.",
  "learn_section_decoded": "# **Naive Bayes Classifier**\n\n## **1. Definition**\n\nNaive Bayes is a **probabilistic machine learning algorithm** used for **classification tasks**. It is based on **Bayes' Theorem**, which describes the probability of an event based on prior knowledge of related events.\n\nThe algorithm assumes that:\n- **Features are conditionally independent** given the class label (the \"naive\" assumption).\n- It calculates the posterior probability for each class and assigns the class with the **highest posterior** to the sample.\n\n---\n\n## **2. Bayes' Theorem**\n\nBayes' Theorem is given by:\n\n$$\nP(C | X) = \\frac{P(X | C) \\times P(C)}{P(X)}\n$$\n\nWhere:\n- $P(C | X)$ **Posterior** probability: the probability of class $C $ given the feature vector $X$\n- $P(X | C)$ → **Likelihood**: the probability of the data $X$ given the class\n- $P(C)$ → **Prior** probability: the initial probability of class $C$ before observing any data\n- $ P(X)$ → **Evidence**: the total probability of the data across all classes (acts as a normalizing constant)\n\nSince $P(X)$ is the same for all classes during comparison, it can be ignored, simplifying the formula to:\n\n$$\nP(C | X) \\propto P(X | C) \\times P(C)\n$$\n---\n\n### 3 **Bernoulli Naive Bayes**\n- Used for **binary data** (features take only 0 or 1 values).\n- The likelihood is given by:\n\n$$\nP(X | C) = \\prod_{i=1}^{n} P(x_i | C)^{x_i} \\cdot (1 - P(x_i | C))^{1 - x_i}\n$$\n\n---\n\n## **4. Applications of Naive Bayes**\n\n- **Text Classification:** Spam detection, sentiment analysis, and news categorization.\n- **Document Categorization:** Sorting documents by topic.\n- **Fraud Detection:** Identifying fraudulent transactions or behaviors.\n- **Recommender Systems:** Classifying users into preference groups.\n\n--- "
}