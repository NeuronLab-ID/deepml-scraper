{
  "description": "R2l2ZW4gYW4gaW5pdGlhbCB2YWx1ZSAkUV8xJCwgYSBsaXN0IG9mICRrJCBvYnNlcnZlZCByZXdhcmRzICRSXzEsIFJfMiwgXGxkb3RzLCBSX2skLCBhbmQgYSBzdGVwIHNpemUgJFxhbHBoYSQsIGltcGxlbWVudCBhIGZ1bmN0aW9uIHRvIGNvbXB1dGUgdGhlIGV4cG9uZW50aWFsbHkgd2VpZ2h0ZWQgYXZlcmFnZSBhczoKCiQkKDEtXGFscGhhKV5rIFFfMSArIFxzdW1fe2k9MX1eayBcYWxwaGEgKDEtXGFscGhhKV57ay1pfSBSX2kkJAoKVGhpcyB3ZWlnaHRpbmcgZ2l2ZXMgbW9yZSBpbXBvcnRhbmNlIHRvIHJlY2VudCByZXdhcmRzLCB3aGlsZSB0aGUgaW5mbHVlbmNlIG9mIHRoZSBpbml0aWFsIGVzdGltYXRlICRRXzEkIGRlY2F5cyBvdmVyIHRpbWUuIERvICoqbm90KiogdXNlIHJ1bm5pbmcvaW5jcmVtZW50YWwgdXBkYXRlczsgaW5zdGVhZCwgY29tcHV0ZSBkaXJlY3RseSBmcm9tIHRoZSBmb3JtdWxhLiAoVGhpcyBpcyBjYWxsZWQgdGhlICpleHBvbmVudGlhbCByZWNlbmN5LXdlaWdodGVkIGF2ZXJhZ2UqLik=",
  "id": "161",
  "test_cases": [
    {
      "test": "Q1 = 10.0\nrewards = [4.0, 7.0, 13.0]\nalpha = 0.5\nprint(round(exp_weighted_average(Q1, rewards, alpha), 4))",
      "expected_output": "10.0"
    },
    {
      "test": "Q1 = 0.0\nrewards = [1.0, 1.0, 1.0, 1.0]\nalpha = 0.1\nprint(round(exp_weighted_average(Q1, rewards, alpha), 4))",
      "expected_output": "0.3439"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "Q1 = 2.0\nrewards = [5.0, 9.0]\nalpha = 0.3\nresult = exp_weighted_average(Q1, rewards, alpha)\nprint(round(result, 4))",
    "output": "5.003",
    "reasoning": "Here, k=2, so the result is: (1-0.3)^2*2.0 + 0.3*(1-0.3)^1*5.0 + 0.3*(1-0.3)^0*9.0 = 0.49*2.0 + 0.21*5.0 + 0.3*9.0 = 0.98 + 1.05 + 2.7 = 4.73 (actually, should be 0.49*2+0.3*0.7*5+0.3*9 = 0.98+1.05+2.7=4.73)"
  },
  "category": "Reinforcement Learning",
  "starter_code": "def exp_weighted_average(Q1, rewards, alpha):\n    \"\"\"\n    Q1: float, initial estimate\n    rewards: list or array of rewards, R_1 to R_k\n    alpha: float, step size (0 < alpha <= 1)\n    Returns: float, exponentially weighted average after k rewards\n    \"\"\"\n    # Your code here\n    pass\n",
  "title": "Exponential Weighted Average of Rewards",
  "learn_section": "IyMjIEV4cG9uZW50aWFsIFJlY2VuY3ktV2VpZ2h0ZWQgQXZlcmFnZQoKV2hlbiB0aGUgZW52aXJvbm1lbnQgaXMgbm9uc3RhdGlvbmFyeSwgaXQgaXMgYmV0dGVyIHRvIGdpdmUgbW9yZSB3ZWlnaHQgdG8gcmVjZW50IHJld2FyZHMuIFRoZSBmb3JtdWxhICQkKDEtXGFscGhhKV5rIFFfMSArIFxzdW1fe2k9MX1eayBcYWxwaGEgKDEtXGFscGhhKV57ay1pfSBSX2kkJCBjb21wdXRlcyB0aGUgZXhwZWN0ZWQgdmFsdWUgYnkgZXhwb25lbnRpYWxseSBkZWNheWluZyB0aGUgaW5mbHVlbmNlIG9mIG9sZCByZXdhcmRzIGFuZCB0aGUgaW5pdGlhbCBlc3RpbWF0ZS4gVGhlIHBhcmFtZXRlciAkXGFscGhhJCBjb250cm9scyBob3cgcXVpY2tseSBvbGQgaW5mb3JtYXRpb24gaXMgZm9yZ290dGVuOiBoaWdoZXIgJFxhbHBoYSQgZ2l2ZXMgbW9yZSB3ZWlnaHQgdG8gbmV3IHJld2FyZHMu",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "description_decoded": "Given an initial value $Q_1$, a list of $k$ observed rewards $R_1, R_2, \\ldots, R_k$, and a step size $\\alpha$, implement a function to compute the exponentially weighted average as:\n\n$$(1-\\alpha)^k Q_1 + \\sum_{i=1}^k \\alpha (1-\\alpha)^{k-i} R_i$$\n\nThis weighting gives more importance to recent rewards, while the influence of the initial estimate $Q_1$ decays over time. Do **not** use running/incremental updates; instead, compute directly from the formula. (This is called the *exponential recency-weighted average*.)",
  "learn_section_decoded": "### Exponential Recency-Weighted Average\n\nWhen the environment is nonstationary, it is better to give more weight to recent rewards. The formula $$(1-\\alpha)^k Q_1 + \\sum_{i=1}^k \\alpha (1-\\alpha)^{k-i} R_i$$ computes the expected value by exponentially decaying the influence of old rewards and the initial estimate. The parameter $\\alpha$ controls how quickly old information is forgotten: higher $\\alpha$ gives more weight to new rewards."
}