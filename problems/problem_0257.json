{
  "description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdG8gY29tcHV0ZSB0aGUgVGVtcG9yYWwgRGlmZmVyZW5jZSAoVEQpIGVycm9yIGZvciBhIHNpbmdsZSBzdGF0ZSB0cmFuc2l0aW9uIGluIHJlaW5mb3JjZW1lbnQgbGVhcm5pbmcuCgpUaGUgVEQgZXJyb3IgbWVhc3VyZXMgaG93IG11Y2ggdGhlIGN1cnJlbnQgdmFsdWUgZXN0aW1hdGUgZGlmZmVycyBmcm9tIGEgYmV0dGVyIGVzdGltYXRlIHRoYXQgaW5jb3Jwb3JhdGVzIHRoZSBpbW1lZGlhdGUgcmV3YXJkIGFuZCB0aGUgYm9vdHN0cmFwcGVkIHZhbHVlIG9mIHRoZSBuZXh0IHN0YXRlLgoKR2l2ZW46Ci0gYHZfc2A6IFRoZSBjdXJyZW50IGVzdGltYXRlIG9mIHRoZSB2YWx1ZSBmb3Igc3RhdGUgcwotIGByZXdhcmRgOiBUaGUgaW1tZWRpYXRlIHJld2FyZCByZWNlaXZlZCBhZnRlciB0cmFuc2l0aW9uaW5nIGZyb20gc3RhdGUgcyAgCi0gYHZfc19wcmltZWA6IFRoZSBjdXJyZW50IGVzdGltYXRlIG9mIHRoZSB2YWx1ZSBmb3IgdGhlIG5leHQgc3RhdGUgcycKLSBgZ2FtbWFgOiBUaGUgZGlzY291bnQgZmFjdG9yIChiZXR3ZWVuIDAgYW5kIDEpCi0gYGRvbmVgOiBBIGJvb2xlYW4gaW5kaWNhdGluZyBpZiB0aGUgbmV4dCBzdGF0ZSBpcyB0ZXJtaW5hbAoKUmV0dXJuIHRoZSBURCBlcnJvciBhcyBhIGZsb2F0LiBOb3RlIHRoYXQgd2hlbiB0aGUgZXBpc29kZSB0ZXJtaW5hdGVzIChkb25lPVRydWUpLCB0aGVyZSBpcyBubyBmdXR1cmUgdmFsdWUgdG8gYm9vdHN0cmFwIGZyb20gc2luY2UgdGhlIGVwaXNvZGUgZW5kcy4KCk9ubHkgdXNlIE51bVB5Lg==",
  "id": "257",
  "test_cases": [
    {
      "test": "import numpy as np\nresult = compute_td_error(5.0, 1.0, 10.0, 0.9, False)\nprint(round(result, 4))",
      "expected_output": "5.0"
    },
    {
      "test": "import numpy as np\nresult = compute_td_error(8.0, 10.0, 0.0, 0.9, True)\nprint(round(result, 4))",
      "expected_output": "2.0"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "v_s=5.0, reward=1.0, v_s_prime=10.0, gamma=0.9, done=False",
    "output": "5.0",
    "reasoning": "TD target = reward + gamma * V(s') = 1.0 + 0.9 * 10.0 = 10.0. TD error = TD target - V(s) = 10.0 - 5.0 = 5.0. The positive TD error indicates the current value estimate was too low."
  },
  "category": "Reinforcement Learning",
  "starter_code": "import numpy as np\n\ndef compute_td_error(v_s: float, reward: float, v_s_prime: float, gamma: float, done: bool) -> float:\n    \"\"\"\n    Compute the Temporal Difference (TD) error for a single transition.\n    \n    Args:\n        v_s: Current state value estimate V(s)\n        reward: Immediate reward received\n        v_s_prime: Next state value estimate V(s')\n        gamma: Discount factor (0 <= gamma <= 1)\n        done: True if s' is a terminal state\n    \n    Returns:\n        The TD error delta\n    \"\"\"\n    # Your code here\n    pass",
  "title": "Compute Temporal Difference Error",
  "createdAt": "December 14, 2025 at 12:13:46â€¯PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "learn_section": "IyAqKlRlbXBvcmFsIERpZmZlcmVuY2UgKFREKSBMZWFybmluZyoqCgpUZW1wb3JhbCBEaWZmZXJlbmNlIGxlYXJuaW5nIGlzIGEgY2VudHJhbCBpZGVhIGluIHJlaW5mb3JjZW1lbnQgbGVhcm5pbmcgdGhhdCBjb21iaW5lcyBpZGVhcyBmcm9tIE1vbnRlIENhcmxvIG1ldGhvZHMgYW5kIGR5bmFtaWMgcHJvZ3JhbW1pbmcuIFRoZSBURCBlcnJvciBpcyB0aGUgZnVuZGFtZW50YWwgYnVpbGRpbmcgYmxvY2sgZm9yIG1hbnkgUkwgYWxnb3JpdGhtcyBpbmNsdWRpbmcgVEQoMCksIFNBUlNBLCBhbmQgUS1sZWFybmluZy4KCi0tLQoKIyMgKipUaGUgVEQgRXJyb3IqKgoKVGhlIFREIGVycm9yIChhbHNvIGNhbGxlZCB0aGUgVEQgcmVzaWR1YWwpIG1lYXN1cmVzIHRoZSBkaWZmZXJlbmNlIGJldHdlZW4gb3VyIGN1cnJlbnQgdmFsdWUgZXN0aW1hdGUgYW5kIGEgYmV0dGVyIGVzdGltYXRlIHRoYXQgdXNlcyB0aGUgaW1tZWRpYXRlIHJld2FyZCBwbHVzIHRoZSBkaXNjb3VudGVkIHZhbHVlIG9mIHRoZSBuZXh0IHN0YXRlOgoKJCQKXGRlbHRhX3QgPSBSX3t0KzF9ICsgXGdhbW1hIFYoU197dCsxfSkgLSBWKFNfdCkKJCQKCldoZXJlOgotICRSX3t0KzF9JCBpcyB0aGUgcmV3YXJkIHJlY2VpdmVkIGFmdGVyIHRyYW5zaXRpb25pbmcgZnJvbSBzdGF0ZSAkU190JAotICRcZ2FtbWEkIGlzIHRoZSBkaXNjb3VudCBmYWN0b3IgKCQwIFxsZXEgXGdhbW1hIFxsZXEgMSQpCi0gJFYoU197dCsxfSkkIGlzIG91ciBjdXJyZW50IGVzdGltYXRlIG9mIHRoZSBuZXh0IHN0YXRlJ3MgdmFsdWUKLSAkVihTX3QpJCBpcyBvdXIgY3VycmVudCBlc3RpbWF0ZSBvZiB0aGUgY3VycmVudCBzdGF0ZSdzIHZhbHVlCgotLS0KCiMjICoqVGVybWluYWwgU3RhdGVzKioKCldoZW4gdGhlIGVwaXNvZGUgdGVybWluYXRlcyAodGhlIGFnZW50IHJlYWNoZXMgYSB0ZXJtaW5hbCBzdGF0ZSksIHRoZXJlIGlzIG5vIGZ1dHVyZSB0byBjb25zaWRlci4gSW4gdGhpcyBjYXNlLCB0aGUgVEQgdGFyZ2V0IHNpbXBsaWZpZXMgdG8ganVzdCB0aGUgaW1tZWRpYXRlIHJld2FyZDoKCiQkClxkZWx0YV90ID0gUl97dCsxfSAtIFYoU190KSBccXVhZCBcdGV4dHsoaWYgfSBTX3t0KzF9IFx0ZXh0eyBpcyB0ZXJtaW5hbCl9CiQkCgpUaGlzIGlzIGJlY2F1c2UgdGhlIHZhbHVlIG9mIGEgdGVybWluYWwgc3RhdGUgaXMgYWx3YXlzIDAgYnkgZGVmaW5pdGlvbi4KCi0tLQoKIyMgKipJbnR1aXRpb24qKgoKLSAqKlBvc2l0aXZlIFREIGVycm9yKiogKCRcZGVsdGEgPiAwJCk6IFRoZSBvdXRjb21lIHdhcyBiZXR0ZXIgdGhhbiBleHBlY3RlZC4gVGhlIGN1cnJlbnQgc3RhdGUgdmFsdWUgZXN0aW1hdGUgc2hvdWxkIGJlIGluY3JlYXNlZC4KLSAqKk5lZ2F0aXZlIFREIGVycm9yKiogKCRcZGVsdGEgPCAwJCk6IFRoZSBvdXRjb21lIHdhcyB3b3JzZSB0aGFuIGV4cGVjdGVkLiBUaGUgY3VycmVudCBzdGF0ZSB2YWx1ZSBlc3RpbWF0ZSBzaG91bGQgYmUgZGVjcmVhc2VkLgotICoqWmVybyBURCBlcnJvcioqICgkXGRlbHRhID0gMCQpOiBUaGUgZXN0aW1hdGUgd2FzIGV4YWN0bHkgY29ycmVjdCBmb3IgdGhpcyB0cmFuc2l0aW9uLgoKLS0tCgojIyAqKlREKDApIFVwZGF0ZSBSdWxlKioKClRoZSBURCBlcnJvciBpcyB1c2VkIHRvIHVwZGF0ZSB2YWx1ZSBlc3RpbWF0ZXM6CgokJApWKFNfdCkgXGxlZnRhcnJvdyBWKFNfdCkgKyBcYWxwaGEgXGNkb3QgXGRlbHRhX3QKJCQKCldoZXJlICRcYWxwaGEkIGlzIHRoZSBsZWFybmluZyByYXRlLiBUaGlzIHVwZGF0ZSBtb3ZlcyB0aGUgdmFsdWUgZXN0aW1hdGUgaW4gdGhlIGRpcmVjdGlvbiBvZiB0aGUgVEQgZXJyb3IuCgotLS0KCiMjICoqV2h5IFREIExlYXJuaW5nPyoqCgoxLiAqKk9ubGluZSBsZWFybmluZyoqOiBVbmxpa2UgTW9udGUgQ2FybG8sIFREIGNhbiB1cGRhdGUgYWZ0ZXIgZWFjaCBzdGVwIHdpdGhvdXQgd2FpdGluZyBmb3IgdGhlIGVwaXNvZGUgdG8gZW5kCjIuICoqQm9vdHN0cmFwcGluZyoqOiBURCB1c2VzIGV4aXN0aW5nIHZhbHVlIGVzdGltYXRlcyB0byB1cGRhdGUgb3RoZXIgZXN0aW1hdGVzCjMuICoqTG93ZXIgdmFyaWFuY2UqKjogQnkgYm9vdHN0cmFwcGluZywgVEQgdHlwaWNhbGx5IGhhcyBsb3dlciB2YXJpYW5jZSB0aGFuIE1vbnRlIENhcmxvIG1ldGhvZHMKNC4gKipGb3VuZGF0aW9uIGZvciBRLWxlYXJuaW5nKio6IFRoZSBzYW1lIFREIGVycm9yIGNvbmNlcHQgZXh0ZW5kcyB0byBhY3Rpb24tdmFsdWUgZnVuY3Rpb25zIGluIGFsZ29yaXRobXMgbGlrZSBRLWxlYXJuaW5nIGFuZCBTQVJTQQ==",
  "description_decoded": "Implement a function to compute the Temporal Difference (TD) error for a single state transition in reinforcement learning.\n\nThe TD error measures how much the current value estimate differs from a better estimate that incorporates the immediate reward and the bootstrapped value of the next state.\n\nGiven:\n- `v_s`: The current estimate of the value for state s\n- `reward`: The immediate reward received after transitioning from state s  \n- `v_s_prime`: The current estimate of the value for the next state s'\n- `gamma`: The discount factor (between 0 and 1)\n- `done`: A boolean indicating if the next state is terminal\n\nReturn the TD error as a float. Note that when the episode terminates (done=True), there is no future value to bootstrap from since the episode ends.\n\nOnly use NumPy.",
  "learn_section_decoded": "# **Temporal Difference (TD) Learning**\n\nTemporal Difference learning is a central idea in reinforcement learning that combines ideas from Monte Carlo methods and dynamic programming. The TD error is the fundamental building block for many RL algorithms including TD(0), SARSA, and Q-learning.\n\n---\n\n## **The TD Error**\n\nThe TD error (also called the TD residual) measures the difference between our current value estimate and a better estimate that uses the immediate reward plus the discounted value of the next state:\n\n$$\n\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\n$$\n\nWhere:\n- $R_{t+1}$ is the reward received after transitioning from state $S_t$\n- $\\gamma$ is the discount factor ($0 \\leq \\gamma \\leq 1$)\n- $V(S_{t+1})$ is our current estimate of the next state's value\n- $V(S_t)$ is our current estimate of the current state's value\n\n---\n\n## **Terminal States**\n\nWhen the episode terminates (the agent reaches a terminal state), there is no future to consider. In this case, the TD target simplifies to just the immediate reward:\n\n$$\n\\delta_t = R_{t+1} - V(S_t) \\quad \\text{(if } S_{t+1} \\text{ is terminal)}\n$$\n\nThis is because the value of a terminal state is always 0 by definition.\n\n---\n\n## **Intuition**\n\n- **Positive TD error** ($\\delta > 0$): The outcome was better than expected. The current state value estimate should be increased.\n- **Negative TD error** ($\\delta < 0$): The outcome was worse than expected. The current state value estimate should be decreased.\n- **Zero TD error** ($\\delta = 0$): The estimate was exactly correct for this transition.\n\n---\n\n## **TD(0) Update Rule**\n\nThe TD error is used to update value estimates:\n\n$$\nV(S_t) \\leftarrow V(S_t) + \\alpha \\cdot \\delta_t\n$$\n\nWhere $\\alpha$ is the learning rate. This update moves the value estimate in the direction of the TD error.\n\n---\n\n## **Why TD Learning?**\n\n1. **Online learning**: Unlike Monte Carlo, TD can update after each step without waiting for the episode to end\n2. **Bootstrapping**: TD uses existing value estimates to update other estimates\n3. **Lower variance**: By bootstrapping, TD typically has lower variance than Monte Carlo methods\n4. **Foundation for Q-learning**: The same TD error concept extends to action-value functions in algorithms like Q-learning and SARSA"
}