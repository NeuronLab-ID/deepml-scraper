{
  "description": "SW1wbGVtZW50IGEgbGVhcm5pbmcgcmF0ZSBzY2hlZHVsZSB0aGF0IGNvbWJpbmVzIGxpbmVhciB3YXJtdXAgYW5kIGNvc2luZSBkZWNheS4gRm9yIHRoZSBmaXJzdCBXIHN0ZXBzLCB0aGUgbGVhcm5pbmcgcmF0ZSBzaG91bGQgaW5jcmVhc2UgbGluZWFybHkgZnJvbSAwIHRvIGEgbWF4aW11bSBsZWFybmluZyByYXRlIChscl9tYXgpLiBBZnRlciB0aGUgd2FybXVwIHBoYXNlLCB0aGUgbGVhcm5pbmcgcmF0ZSBzaG91bGQgZGVjYXkgZm9sbG93aW5nIGEgY29zaW5lIGFubmVhbGluZyBzY2hlZHVsZSB1bnRpbCBpdCByZWFjaGVzIGEgbWluaW11bSBsZWFybmluZyByYXRlIChscl9taW4pIGF0IHN0ZXAgVC4gUmV0dXJuIGEgbGlzdCBvZiBsZWFybmluZyByYXRlcyBmb3IgZWFjaCBvZiB0aGUgVCB0b3RhbCB0cmFpbmluZyBzdGVwcy4=",
  "id": "196",
  "test_cases": [
    {
      "test": "print([round(x, 4) for x in warmup_cosine_schedule(T=10, W=3, lr_max=1.0, lr_min=0.0)])",
      "expected_output": "[0.0, 0.3333, 0.6667, 1.0, 0.9505, 0.8117, 0.6113, 0.3887, 0.1883, 0.0495]"
    },
    {
      "test": "print([round(x, 4) for x in warmup_cosine_schedule(T=5, W=2, lr_max=0.1, lr_min=0.01)])",
      "expected_output": "[0.0, 0.05, 0.1, 0.0775, 0.0325]"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "T=10, W=3, lr_max=1.0, lr_min=0.0",
    "output": "[0.0, 0.3333, 0.6667, 1.0, 0.9505, 0.8117, 0.6113, 0.3887, 0.1883, 0.0495]",
    "reasoning": "During warmup (steps 0-2), learning rate increases linearly from 0 to 1.0. After warmup, it follows a cosine decay from 1.0 down to 0.0 over the remaining 7 steps."
  },
  "category": "Optimization",
  "starter_code": "def warmup_cosine_schedule(T: int, W: int, lr_max: float, lr_min: float) -> list[float]:\n\t\"\"\"\n\tCompute learning rate schedule with linear warmup and cosine decay.\n\t\n\tArgs:\n\t\tT: Total number of training steps\n\t\tW: Number of warmup steps\n\t\tlr_max: Maximum learning rate (reached after warmup)\n\t\tlr_min: Minimum learning rate (reached at end of training)\n\t\n\tReturns:\n\t\tList of learning rates for each step\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Warmup + Cosine Decay Schedule",
  "createdAt": "November 9, 2025 at 3:44:53 PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "learn_section": "IyMjIFVuZGVyc3RhbmRpbmcgTGVhcm5pbmcgUmF0ZSBTY2hlZHVsZXMKCkxlYXJuaW5nIHJhdGUgc2NoZWR1bGVzIGFyZSBjcnVjaWFsIGZvciB0cmFpbmluZyBkZWVwIG5ldXJhbCBuZXR3b3JrcyBlZmZlY3RpdmVseS4gVGhlIHdhcm11cCArIGNvc2luZSBkZWNheSBzY2hlZHVsZSBpcyBhIHBvcHVsYXIgdGVjaG5pcXVlIHRoYXQgY29tYmluZXMgdHdvIHN0cmF0ZWdpZXM6CgojIyMjIDEuIExpbmVhciBXYXJtdXAgUGhhc2UKCkR1cmluZyB0aGUgZmlyc3QgJFckIHN0ZXBzICh3YXJtdXAgcGVyaW9kKSwgdGhlIGxlYXJuaW5nIHJhdGUgaW5jcmVhc2VzIGxpbmVhcmx5IGZyb20gMCB0byB0aGUgbWF4aW11bSBsZWFybmluZyByYXRlICRcZXRhX3ttYXh9JDoKCiQkClxldGFfdCA9IFxmcmFje3R9e1d9IFxjZG90IFxldGFfe21heH0gXHF1YWQgXHRleHR7Zm9yIH0gdCA8IFcKJCQKCldoZXJlOgotICR0JCBpcyB0aGUgY3VycmVudCB0cmFpbmluZyBzdGVwCi0gJFckIGlzIHRoZSBudW1iZXIgb2Ygd2FybXVwIHN0ZXBzCi0gJFxldGFfe21heH0kIGlzIHRoZSBtYXhpbXVtIGxlYXJuaW5nIHJhdGUKCioqV2h5IHdhcm11cD8qKiBTdGFydGluZyB3aXRoIGEgc21hbGwgbGVhcm5pbmcgcmF0ZSBwcmV2ZW50cyB0aGUgbW9kZWwgZnJvbSBtYWtpbmcgbGFyZ2UsIGRlc3RhYmlsaXppbmcgdXBkYXRlcyBlYXJseSBpbiB0cmFpbmluZyB3aGVuIGdyYWRpZW50cyBtYXkgYmUgbm9pc3kgb3IgdW5yZWxpYWJsZS4KCiMjIyMgMi4gQ29zaW5lIERlY2F5IFBoYXNlCgpBZnRlciB3YXJtdXAsIHRoZSBsZWFybmluZyByYXRlIGRlY2F5cyBmb2xsb3dpbmcgYSBjb3NpbmUgY3VydmUgZnJvbSAkXGV0YV97bWF4fSQgdG8gJFxldGFfe21pbn0kIG92ZXIgdGhlIHJlbWFpbmluZyAkVCAtIFckIHN0ZXBzOgoKJCQKXGV0YV90ID0gXGV0YV97bWlufSArIFxmcmFjezF9ezJ9KFxldGFfe21heH0gLSBcZXRhX3ttaW59KSBcbGVmdCgxICsgXGNvc1xsZWZ0KFxmcmFje3QgLSBXfXtUIC0gV30gXHBpXHJpZ2h0KVxyaWdodCkgXHF1YWQgXHRleHR7Zm9yIH0gdCBcZ2VxIFcKJCQKCldoZXJlOgotICRUJCBpcyB0aGUgdG90YWwgbnVtYmVyIG9mIHRyYWluaW5nIHN0ZXBzCi0gJFxldGFfe21pbn0kIGlzIHRoZSBtaW5pbXVtIGxlYXJuaW5nIHJhdGUKLSBUaGUgcHJvZ3Jlc3MgcmF0aW8gJFxmcmFje3QgLSBXfXtUIC0gV30kIHJhbmdlcyBmcm9tIDAgdG8gMQoKKipXaHkgY29zaW5lIGRlY2F5PyoqIFRoZSBzbW9vdGgsIGdyYWR1YWwgZGVjYXkgYWxsb3dzIHRoZSBtb2RlbCB0bzoKLSBNYWtlIGxhcmdlIGltcHJvdmVtZW50cyBlYXJseSBpbiB0cmFpbmluZyAod2hlbiBsZWFybmluZyByYXRlIGlzIGhpZ2gpCi0gRmluZS10dW5lIGluIGxhdGVyIHN0YWdlcyAod2hlbiBsZWFybmluZyByYXRlIGlzIGxvdykKLSBBdm9pZCBzdWRkZW4gZHJvcHMgaW4gbGVhcm5pbmcgcmF0ZSB0aGF0IGNvdWxkIGRpc3J1cHQgdHJhaW5pbmcKCiMjIyMgS2V5IFByb3BlcnRpZXMKCjEuICoqU21vb3RoIFRyYW5zaXRpb25zKio6IFRoZSBjb3NpbmUgZnVuY3Rpb24gcHJvdmlkZXMgYSBzbW9vdGggZGVjYXkgd2l0aG91dCBhYnJ1cHQgY2hhbmdlcwoyLiAqKkZhc3RlciBJbml0aWFsIERlY2F5Kio6IExlYXJuaW5nIHJhdGUgZGVjcmVhc2VzIG1vcmUgcXVpY2tseSBhdCB0aGUgYmVnaW5uaW5nIG9mIHRoZSBkZWNheSBwaGFzZQozLiAqKkdyYWR1YWwgRmluYWwgQXBwcm9hY2gqKjogVGhlIHJhdGUgb2YgZGVjYXkgc2xvd3MgYXMgdHJhaW5pbmcgcHJvZ3Jlc3NlcywgYWxsb3dpbmcgZm9yIGZpbmUtdHVuaW5nCjQuICoqTm8gSHlwZXJwYXJhbWV0ZXIgVHVuaW5nKio6IFVubGlrZSBzdGVwIGRlY2F5LCBubyBuZWVkIHRvIHNwZWNpZnkgd2hlbiB0byBkcm9wIHRoZSBsZWFybmluZyByYXRlCgojIyMjIFByYWN0aWNhbCBDb25zaWRlcmF0aW9ucwoKLSAqKldhcm11cCBEdXJhdGlvbioqOiBUeXBpY2FsbHkgMS0xMCUgb2YgdG90YWwgdHJhaW5pbmcgc3RlcHMKLSAqKk1heGltdW0gTFIqKjogVXN1YWxseSBkZXRlcm1pbmVkIHRocm91Z2ggaHlwZXJwYXJhbWV0ZXIgc2VhcmNoCi0gKipNaW5pbXVtIExSKio6IE9mdGVuIHNldCB0byAwIG9yIGEgc21hbGwgZnJhY3Rpb24gb2YgbWF4IExSIChlLmcuLCAwLjAxIMOXIG1heCBMUikKLSAqKkNvbW1vbiBVc2UqKjogUG9wdWxhciBpbiB0cmFuc2Zvcm1lciBtb2RlbHMgKEJFUlQsIEdQVCkgYW5kIGNvbXB1dGVyIHZpc2lvbiAoUmVzTmV0cykKClRoaXMgc2NoZWR1bGUgaXMgcGFydGljdWxhcmx5IGVmZmVjdGl2ZSBmb3IgdHJhaW5pbmcgbGFyZ2UgbW9kZWxzIHdoZXJlIHN0YWJpbGl0eSBpbiBlYXJseSB0cmFpbmluZyBhbmQgZmluZS10dW5pbmcgaW4gbGF0ZXIgc3RhZ2VzIGFyZSBib3RoIGltcG9ydGFudC4=",
  "description_decoded": "Implement a learning rate schedule that combines linear warmup and cosine decay. For the first W steps, the learning rate should increase linearly from 0 to a maximum learning rate (lr_max). After the warmup phase, the learning rate should decay following a cosine annealing schedule until it reaches a minimum learning rate (lr_min) at step T. Return a list of learning rates for each of the T total training steps.",
  "learn_section_decoded": "### Understanding Learning Rate Schedules\n\nLearning rate schedules are crucial for training deep neural networks effectively. The warmup + cosine decay schedule is a popular technique that combines two strategies:\n\n#### 1. Linear Warmup Phase\n\nDuring the first $W$ steps (warmup period), the learning rate increases linearly from 0 to the maximum learning rate $\\eta_{max}$:\n\n$$\n\\eta_t = \\frac{t}{W} \\cdot \\eta_{max} \\quad \\text{for } t < W\n$$\n\nWhere:\n- $t$ is the current training step\n- $W$ is the number of warmup steps\n- $\\eta_{max}$ is the maximum learning rate\n\n**Why warmup?** Starting with a small learning rate prevents the model from making large, destabilizing updates early in training when gradients may be noisy or unreliable.\n\n#### 2. Cosine Decay Phase\n\nAfter warmup, the learning rate decays following a cosine curve from $\\eta_{max}$ to $\\eta_{min}$ over the remaining $T - W$ steps:\n\n$$\n\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min}) \\left(1 + \\cos\\left(\\frac{t - W}{T - W} \\pi\\right)\\right) \\quad \\text{for } t \\geq W\n$$\n\nWhere:\n- $T$ is the total number of training steps\n- $\\eta_{min}$ is the minimum learning rate\n- The progress ratio $\\frac{t - W}{T - W}$ ranges from 0 to 1\n\n**Why cosine decay?** The smooth, gradual decay allows the model to:\n- Make large improvements early in training (when learning rate is high)\n- Fine-tune in later stages (when learning rate is low)\n- Avoid sudden drops in learning rate that could disrupt training\n\n#### Key Properties\n\n1. **Smooth Transitions**: The cosine function provides a smooth decay without abrupt changes\n2. **Faster Initial Decay**: Learning rate decreases more quickly at the beginning of the decay phase\n3. **Gradual Final Approach**: The rate of decay slows as training progresses, allowing for fine-tuning\n4. **No Hyperparameter Tuning**: Unlike step decay, no need to specify when to drop the learning rate\n\n#### Practical Considerations\n\n- **Warmup Duration**: Typically 1-10% of total training steps\n- **Maximum LR**: Usually determined through hyperparameter search\n- **Minimum LR**: Often set to 0 or a small fraction of max LR (e.g., 0.01 × max LR)\n- **Common Use**: Popular in transformer models (BERT, GPT) and computer vision (ResNets)\n\nThis schedule is particularly effective for training large models where stability in early training and fine-tuning in later stages are both important."
}