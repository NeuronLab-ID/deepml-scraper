{
  "description": "SW1wbGVtZW50IHRoZSBVcHBlciBDb25maWRlbmNlIEJvdW5kIChVQ0IpIGFjdGlvbiBzZWxlY3Rpb24gc3RyYXRlZ3kgZm9yIHRoZSBtdWx0aS1hcm1lZCBiYW5kaXQgcHJvYmxlbS4gV3JpdGUgYSBmdW5jdGlvbiB0aGF0LCBnaXZlbiB0aGUgY3VycmVudCBudW1iZXIgb2YgdGltZXMgZWFjaCBhY3Rpb24gaGFzIGJlZW4gc2VsZWN0ZWQsIHRoZSBhdmVyYWdlIHJld2FyZHMgZm9yIGVhY2ggYWN0aW9uLCBhbmQgdGhlIGN1cnJlbnQgdGltZXN0ZXAgdCwgcmV0dXJucyB0aGUgYWN0aW9uIHRvIHNlbGVjdCBhY2NvcmRpbmcgdG8gdGhlIFVDQjEgZm9ybXVsYS4gVXNlIG9ubHkgTnVtUHku",
  "id": "162",
  "test_cases": [
    {
      "test": "import numpy as np\ncounts = np.array([1, 1, 1, 1])  # Each action tried once\nvalues = np.array([1.0, 2.0, 1.5, 0.5])\nt = 4\nc = 2.0\nprint(ucb_action(counts, values, t, c))",
      "expected_output": "1"
    },
    {
      "test": "import numpy as np\ncounts = np.array([10, 10, 1, 10])\nvalues = np.array([0.7, 0.6, 2.0, 0.8])\nt = 31\nc = 1.0\nprint(ucb_action(counts, values, t, c))",
      "expected_output": "2"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "import numpy as np\ncounts = np.array([1, 1, 1, 1])\nvalues = np.array([1.0, 2.0, 1.5, 0.5])\nt = 4\nc = 2.0\nprint(ucb_action(counts, values, t, c))",
    "output": "1",
    "reasoning": "At t=4, each action has been tried once, but action 1 has the highest average reward (2.0) and the same confidence bound as the others, so it is chosen."
  },
  "category": "Reinforcement Learning",
  "starter_code": "import numpy as np\n\ndef ucb_action(counts, values, t, c):\n    \"\"\"\n    Choose an action using the UCB1 formula.\n    Args:\n      counts (np.ndarray): Number of times each action has been chosen\n      values (np.ndarray): Average reward of each action\n      t (int): Current timestep (starts from 1)\n      c (float): Exploration coefficient\n    Returns:\n      int: Index of action to select\n    \"\"\"\n    # TODO: Implement the UCB action selection\n    pass",
  "title": "Upper Confidence Bound (UCB) Action Selection",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "description_decoded": "Implement the Upper Confidence Bound (UCB) action selection strategy for the multi-armed bandit problem. Write a function that, given the current number of times each action has been selected, the average rewards for each action, and the current timestep t, returns the action to select according to the UCB1 formula. Use only NumPy.",
  "learn_section_decoded": "# **Upper Confidence Bound (UCB) Action Selection**\n\nThe **Upper Confidence Bound (UCB)** is a principled method for balancing exploration and exploitation in the multi-armed bandit problem. UCB assigns each action an optimistic estimate of its potential by considering both its current estimated value and the uncertainty around that estimate.\n\n---\n\n## **UCB1 Formula**\nGiven:\n- $Q(a)$: Average reward of action $a$\n- $N(a)$: Number of times action $a$ has been chosen\n- $t$: Total number of action selections so far\n- $c$: Exploration coefficient (higher $c$ â†’ more exploration)\n\nThe UCB value for each action $a$ is:\n\n$$\nUCB(a) = Q(a) + c \\cdot \\sqrt{\\frac{\\ln t}{N(a)}}\n$$\n\n- The first term ($Q(a)$) encourages exploitation (choose the best-known action)\n- The second term encourages exploration (prefer actions tried less often)\n\nAt each timestep, select the action with the highest $UCB(a)$ value.\n\n---\n\n## **Key Points**\n- UCB ensures every action is tried (because the exploration term is large for actions with low $N(a)$)\n- As $N(a)$ increases, the uncertainty shrinks and the choice relies more on the estimated value\n- $c$ tunes the trade-off: high $c$ -> more exploration, low $c$ -> more exploitation\n\n---\n\n## **When To Use UCB?**\n- In online learning tasks (multi-armed bandits)\n- In environments where you need to efficiently explore without random guessing\n- In real-world scenarios: ad placement, recommendation systems, A/B testing, clinical trials\n\n---\n\n## **Summary**\nUCB is a simple and powerful method for action selection. It works by always acting as if the best-case scenario (within a confidence bound) is true for every action, balancing learning new information and maximizing reward."
}