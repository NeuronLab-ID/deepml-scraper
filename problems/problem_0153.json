{
  "description": "V3JpdGUgYSBQeXRob24gY2xhc3MgU3RlcExSU2NoZWR1bGVyIHRvIGltcGxlbWVudCBhIGxlYXJuaW5nIHJhdGUgc2NoZWR1bGVyIGJhc2VkIG9uIHRoZSBTdGVwTFIgc3RyYXRlZ3kuIFlvdXIgY2xhc3Mgc2hvdWxkIGhhdmUgYW4gX19pbml0X18gbWV0aG9kIGltcGxlbWVudGVkIHRvIGluaXRpYWxpemUgd2l0aCBhbiBpbml0aWFsX2xyIChmbG9hdCksIHN0ZXBfc2l6ZSAoaW50KSwgYW5kIGdhbW1hIChmbG9hdCkgcGFyYW1ldGVyLiBJdCBzaG91bGQgYWxzbyBoYXZlIGEgKipnZXRfbHIoc2VsZiwgZXBvY2gpKiogbWV0aG9kIGltcGxlbWVudGVkIHRoYXQgcmV0dXJucyB0aGUgY3VycmVudCBsZWFybmluZyByYXRlIGZvciBhIGdpdmVuIGVwb2NoIChpbnQpLiBUaGUgbGVhcm5pbmcgcmF0ZSBzaG91bGQgYmUgZGVjcmVhc2VkIGJ5IGdhbW1hIGV2ZXJ5IHN0ZXBfc2l6ZSBlcG9jaHMuIFRoZSBhbnN3ZXIgc2hvdWxkIGJlIHJvdW5kZWQgdG8gNCBkZWNpbWFsIHBsYWNlcy4gT25seSB1c2Ugc3RhbmRhcmQgUHl0aG9uLg==",
  "id": "153",
  "test_cases": [
    {
      "test": "scheduler = StepLRScheduler(initial_lr=0.1, step_size=5, gamma=0.5)\nprint(scheduler.get_lr(epoch=0))",
      "expected_output": "0.1"
    },
    {
      "test": "scheduler = StepLRScheduler(initial_lr=0.1, step_size=5, gamma=0.5)\nprint(scheduler.get_lr(epoch=4))",
      "expected_output": "0.1"
    }
  ],
  "difficulty": "easy",
  "likes": "0",
  "video": "",
  "dislikes": "0",
  "example": {
    "input": "scheduler = StepLRScheduler(initial_lr=0.1, step_size=5, gamma=0.5)\nprint(scheduler.get_lr(epoch=0))\nprint(scheduler.get_lr(epoch=4))\nprint(scheduler.get_lr(epoch=5))\nprint(scheduler.get_lr(epoch=9))\nprint(scheduler.get_lr(epoch=10))",
    "output": "0.1\n0.1\n0.05\n0.05\n0.025",
    "reasoning": "The initial learning rate is 0.1. It stays 0.1 for epochs 0-4. At epoch 5, it decays by 0.5 to 0.05. It stays 0.05 for epochs 5-9. At epoch 10, it decays again to 0.025."
  },
  "category": "Machine Learning",
  "starter_code": "class StepLRScheduler:\n    def __init__(self, initial_lr, step_size, gamma):\n        # Initialize initial_lr, step_size, and gamma\n        pass\n\n    def get_lr(self, epoch):\n        # Calculate and return the learning rate for the given epoch\n        pass",
  "title": "StepLR Learning Rate Scheduler",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/komaksym",
      "name": "komaksym"
    }
  ],
  "description_decoded": "Write a Python class StepLRScheduler to implement a learning rate scheduler based on the StepLR strategy. Your class should have an __init__ method implemented to initialize with an initial_lr (float), step_size (int), and gamma (float) parameter. It should also have a **get_lr(self, epoch)** method implemented that returns the current learning rate for a given epoch (int). The learning rate should be decreased by gamma every step_size epochs. The answer should be rounded to 4 decimal places. Only use standard Python.",
  "learn_section_decoded": "# **Learning Rate Schedulers: StepLR**\n\n## **1. Definition**\nA **learning rate scheduler** is a component used in machine learning, especially in neural network training, to adjust the learning rate during the training process. The **learning rate** is a hyperparameter that determines the step size at each iteration while moving towards a minimum of a loss function.\n\n**StepLR (Step Learning Rate)** is a common type of learning rate scheduler that multiplicatively decays the learning rate by a fixed factor at predefined intervals (epochs). It is simple yet effective in stabilizing training and improving model performance.\n\n## **2. Why Use Learning Rate Schedulers?**\n* **Faster Convergence:** A higher initial learning rate can help quickly move through the loss landscape.\n* **Improved Performance:** A smaller learning rate towards the end of training allows for finer adjustments and helps in converging to a better local minimum, avoiding oscillations around the minimum.\n* **Stability:** Reducing the learning rate prevents large updates that could lead to divergence or instability.\n\n## **3. StepLR Mechanism**\nThe learning rate is reduced by a factor γ (gamma) every step size epochs.\n\nThe formula for the learning rate at a given epoch is:\n\n$$LR_e = LR_{\\text{initial}} \\times \\gamma^{\\lfloor e / \\text{step size} \\rfloor}$$\n\nWhere:\n* $LR_e$: The learning rate at epoch e.\n* $LR_{\\text{initial}}$: The initial learning rate.\n* γ (gamma): The multiplicative factor by which the learning rate is reduced (usually between 0 and 1, e.g., 0.1, 0.5).\n* step size: The interval (in epochs) after which the learning rate is decayed.\n* ⌊ · ⌋: The floor function, which rounds down to the nearest integer. This determines how many times the learning rate has been decayed.\n\n**Example:**\nIf initial learning rate = 0.1, step size = 5, and γ = 0.5:\n* Epoch 0-4: $LR_e = 0.1 \\times 0.5^{\\lfloor 0/5 \\rfloor} = 0.1 \\times 0.5^0 = 0.1$\n* Epoch 5-9: $LR_e = 0.1 \\times 0.5^{\\lfloor 5/5 \\rfloor} = 0.1 \\times 0.5^1 = 0.05$\n* Epoch 10-14: $LR_e = 0.1 \\times 0.5^{\\lfloor 10/5 \\rfloor} = 0.1 \\times 0.5^2 = 0.025$\n\n## **4. Applications of Learning Rate Schedulers**\nLearning rate schedulers, including StepLR, are widely used in training various machine learning models, especially deep neural networks, across diverse applications such as:\n* **Image Classification:** Training Convolutional Neural Networks (CNNs) for tasks like object recognition.\n* **Natural Language Processing (NLP):** Training Recurrent Neural Networks (RNNs) and Transformers for tasks like machine translation, text generation, and sentiment analysis.\n* **Speech Recognition:** Training models for converting spoken language to text.\n* **Reinforcement Learning:** Optimizing policies in reinforcement learning agents.\n* **Any optimization problem:** where gradient descent or its variants are used."
}