{
  "description": "Q2FsY3VsYXRlIHRoZSBjb21wdXRhdGlvbmFsIGNvc3Qgc2F2aW5ncyBvZiBhbiBNb0UgbGF5ZXIgY29tcGFyZWQgdG8gYSBkZW5zZSBsYXllciwgYXMgZGlzY3Vzc2VkIGluIHRoZSBwYXBlciAnT3V0cmFnZW91c2x5IExhcmdlIE5ldXJhbCBOZXR3b3JrczogVGhlIFNwYXJzZWx5LUdhdGVkIE1peHR1cmUtb2YtRXhwZXJ0cyBMYXllci4nIEdpdmVuIHRoZSBudW1iZXIgb2YgZXhwZXJ0cywgc3BhcnNpdHkgKG51bWJlciBvZiBhY3RpdmUgZXhwZXJ0cyksIGFuZCBpbnB1dC9vdXRwdXQgZGltZW5zaW9ucywgY29tcHV0ZSB0aGUgZmxvYXRpbmctcG9pbnQgb3BlcmF0aW9ucyAoRkxPUHMpIGZvciBib3RoIGFuZCBkZXRlcm1pbmUgdGhlIHNhdmluZ3MgcGVyY2VudGFnZS4=",
  "id": "123",
  "test_cases": [
    {
      "test": "print(round(compute_efficiency(1000, 2, 512, 512),1))",
      "expected_output": "99.8"
    },
    {
      "test": "print(round(compute_efficiency(10, 2, 256, 256),1))",
      "expected_output": "80.0"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "compute_efficiency(1000, 2, 512, 512)",
    "output": "99.8",
    "reasoning": "Dense layer FLOPs: 1000 * 512 * 512 = 262,144,000. MoE FLOPs: 2 * 512 * 512 = 524,288. Savings:  ((262,144,000 - 524,288) / 262,144,000) x 100 ≈ 99.8%."
  },
  "category": "Deep Learning",
  "starter_code": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate computational savings of MoE vs. dense layer.\n\n    Args:\n        n_experts: Total number of experts\n        k_active: Number of active experts (sparsity)\n        d_in: Input dimension\n        d_out: Output dimension\n\n    Returns:\n        Percentage savings in FLOPs\n    \"\"\"\n    pass",
  "learn_section": "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",
  "title": "Calculate Computational Efficiency of MoE",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "description_decoded": "Calculate the computational cost savings of an MoE layer compared to a dense layer, as discussed in the paper 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.' Given the number of experts, sparsity (number of active experts), and input/output dimensions, compute the floating-point operations (FLOPs) for both and determine the savings percentage.",
  "learn_section_decoded": "\n## Understanding MoE Efficiency\n\nThe paper *\"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\"* introduces the idea of activating only a few expert networks per input to drastically reduce computation. This is known as **conditional computation**, and it allows models to scale to billions of parameters without significantly increasing cost.\n\n---\n\n### Key Idea\n\nIn a **dense layer**, every input goes through the full set of parameters.  \nIn a **Mixture-of-Experts (MoE)** layer, only $k$ out of $n$ experts are active for each input.\n\n---\n\n### FLOPs Formulas\n\nLet:\n- $d_{in}$ = input dimension  \n- $d_{out}$ = output dimension  \n- $n$ = total experts  \n- $k$ = active experts per input  \n\nThen:\n- **Dense layer FLOPs**:  \n  $$\n  \\text{FLOPs}_{\\text{dense}} = n \\cdot d_{in} \\cdot d_{out}\n  $$\n- **MoE layer FLOPs**:  \n  $$\n  \\text{FLOPs}_{\\text{moe}} = k \\cdot d_{in} \\cdot d_{out}\n  $$\n- **Efficiency gain**:\n  $$\n  \\text{Savings}(\\%) = \\left( \\frac{\\text{FLOPs}_{\\text{dense}} - \\text{FLOPs}_{\\text{moe}}}{\\text{FLOPs}_{\\text{dense}}} \\right) \\cdot 100\n  $$\n\n---\n\n### Example\n\nSuppose:\n- $n = 1000$, $k = 2$  \n- $d_{in} = d_{out} = 512$  \n\nThen:\n- MoE FLOPs = $2 \\cdot 512 \\cdot 512 = 524,\\!288$  \n- Full dense (all 1000 experts): $1000 \\cdot 512 \\cdot 512 = 262,\\!144,\\!000$  \n- Savings:\n  $$\n  \\left( \\frac{262,\\!144,\\!000 - 524,\\!288}{262,\\!144,\\!000} \\right) \\cdot 100 \\approx 99.8\\%\n  $$\n\nThis means the MoE layer uses just **0.2%** of the computation compared to a full dense version — an enormous gain in efficiency.\n\n---\n\n### Summary\n\nBy activating only a small number of experts per input, MoE layers reduce computation while maintaining high model capacity. This makes it feasible to train outrageously large models efficiently.\n"
}