{
  "description": "SW1wbGVtZW50IEdyb3VwIFNlcXVlbmNlIFBvbGljeSBPcHRpbWl6YXRpb24gKEdTUE8pLCBhIHJlaW5mb3JjZW1lbnQgbGVhcm5pbmcgYWxnb3JpdGhtIGZvciB0cmFpbmluZyBsYXJnZSBsYW5ndWFnZSBtb2RlbHMuIFVubGlrZSB0b2tlbi1sZXZlbCBtZXRob2RzIGxpa2UgR1JQTyB0aGF0IHN1ZmZlciBmcm9tIHRyYWluaW5nIGluc3RhYmlsaXR5LCBHU1BPIHVzZXMgc2VxdWVuY2UtbGV2ZWwgaW1wb3J0YW5jZSByYXRpb3Mgd2l0aCBsZW5ndGggbm9ybWFsaXphdGlvbi4gR2l2ZW4gbG9nIHByb2JhYmlsaXRpZXMgZnJvbSBuZXcgYW5kIG9sZCBwb2xpY2llcyBhbG9uZyB3aXRoIHJld2FyZHMgZm9yIGEgZ3JvdXAgb2Ygc2VxdWVuY2VzLCBjb21wdXRlIHRoZSBjbGlwcGVkIG9iamVjdGl2ZS4gVGhlIGFsZ29yaXRobSBwcmV2ZW50cyBjYXRhc3Ryb3BoaWMgbW9kZWwgY29sbGFwc2UgYnkgYXBwbHlpbmcgY2xpcHBpbmcgYXQgdGhlIHNlcXVlbmNlIGxldmVsIHJhdGhlciB0aGFuIHRva2VuIGxldmVsLg==",
  "id": "209",
  "test_cases": [
    {
      "test": "import numpy as np; obj = gspo_objective([[0.0, -0.5, -1.0], [-0.3, -0.7]], [[0.0, -0.5, -1.0], [-0.3, -0.7]], [0.8, 0.6], epsilon=0.2); print(round(obj, 6))",
      "expected_output": "0.0"
    },
    {
      "test": "import numpy as np; obj = gspo_objective([[-0.2, -0.3], [-0.1, -0.4, -0.2]], [[-0.5, -0.6], [-0.4, -0.7, -0.5]], [0.9, 0.7], epsilon=0.2); print(round(obj, 6))",
      "expected_output": "-0.074929"
    }
  ],
  "difficulty": "hard",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "log_probs_new=[[1.0, 0.5], [0.8, 1.2]], log_probs_old=[[0.0, 0.0], [0.0, 0.0]], rewards=[0.9, 0.7], epsilon=0.2",
    "output": "Approximately -0.089",
    "reasoning": "First compute advantages: mean=0.8, std=0.1, so A=[1.0, -1.0]. For seq 1: log ratio sum = 1.5, length = 2, so s_1 = exp(1.5/2) = 2.117, clipped to 1.2, objective = 1.2 × 1.0 = 1.2. For seq 2: log ratio sum = 2.0, length = 2, so s_2 = exp(2.0/2) = 2.718, clipped to 1.2, objective = 1.2 × (-1.0) = -1.2. Average = 0.0. The length normalization prevents extreme importance ratios."
  },
  "category": "Reinforcement Learning",
  "starter_code": "import numpy as np\n\ndef gspo_objective(log_probs_new: list[list[float]], \n                   log_probs_old: list[list[float]], \n                   rewards: list[float], \n                   epsilon: float = 0.2) -> float:\n\t\"\"\"\n\tCompute GSPO (Group Sequence Policy Optimization) clipped objective.\n\t\n\tArgs:\n\t\tlog_probs_new: Log probability sequences from new policy\n\t\tlog_probs_old: Log probability sequences from old policy\n\t\trewards: Reward for each sequence\n\t\tepsilon: Clipping range for importance ratios\n\t\n\tReturns:\n\t\tAverage clipped objective value\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "GSPO: Group Sequence Policy Optimization",
  "createdAt": "November 18, 2025 at 12:12:33 PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "learn_section": "### Understanding GSPO

Group Sequence Policy Optimization (GSPO) is a breakthrough reinforcement learning algorithm from Alibaba's Qwen team that addresses critical stability issues in training large language models. It powers the latest Qwen3 models and represents a significant advancement over previous approaches.

#### The Problem with Token-Level Methods

Previous algorithms like GRPO (Group Relative Policy Optimization) use token-level importance ratios:

$$
w_{i,t}(\theta) = \frac{\pi_{\theta}(y_{i,t}|x, y_{i,<t})}{\pi_{\theta_{\text{old}}}(y_{i,t}|x, y_{i,<t})}
$$

**Critical flaw**: This ratio is based on a single sample $y_{i,t}$ from the distribution $\pi_{\theta_{\text{old}}}(\cdot|x, y_{i,<t})$, violating the fundamental principle of importance sampling. Importance sampling requires averaging over multiple samples to correct distributional mismatch:

$$
\mathbb{E}_{z\sim\pi_{\text{tar}}}[f(z)] = \mathbb{E}_{z\sim\pi_{\text{beh}}}\left[\frac{\pi_{\text{tar}}(z)}{\pi_{\text{beh}}(z)} f(z)\right]
$$

With only one sample per token position, the token-level weight introduces high-variance noise that:
- Accumulates over long sequences
- Is amplified by clipping
- Causes catastrophic, irreversible model collapse

**Core principle violated**: The unit of optimization must match the unit of reward. Since rewards apply to entire sequences, token-level correction is fundamentally mismatched.

#### GSPO's Sequence-Level Solution

GSPO defines importance ratios at the sequence level with length normalization:

$$
s_i(\theta) = \left(\frac{\pi_{\theta}(y_i|x)}{\pi_{\theta_{\text{old}}}(y_i|x)}\right)^{\frac{1}{|y_i|}}
$$

Expanding the sequence likelihood:

$$
s_i(\theta) = \exp\left(\frac{1}{|y_i|}\sum_{t=1}^{|y_i|} \log\frac{\pi_{\theta}(y_{i,t}|x, y_{i,<t})}{\pi_{\theta_{\text{old}}}(y_{i,t}|x, y_{i,<t})}\right)
$$

**Why length normalization?** Without it:
- A few token changes cause dramatic ratio fluctuations
- Different sequence lengths need different clipping ranges
- Numerical instability increases

With normalization, importance ratios stay in a controlled range regardless of sequence length.

#### GSPO Objective

The clipped objective is:

$$
J_{\text{GSPO}}(\theta) = \mathbb{E}_{x\sim D, \{y_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|x)}\left[\frac{1}{G}\sum_{i=1}^G \min\left(s_i(\theta)\hat{A}_i, \text{clip}(s_i(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_i\right)\right]
$$

Where advantages are computed from rewards:

$$
\hat{A}_i = \frac{r(x, y_i) - \text{mean}(\{r(x, y_i)\}_{i=1}^G)}{\text{std}(\{r(x, y_i)\}_{i=1}^G)}
$$

**Key components**:
1. **Group-based advantages**: Normalized rewards within a group of $G$ responses
2. **Sequence-level clipping**: Entire sequences are clipped, not individual tokens
3. **Pessimistic bound**: Take minimum of clipped and unclipped objectives

#### Algorithm Steps

**1. Compute Advantages**

Given a group of $G$ sequences with rewards $\{r_i\}_{i=1}^G$:

$$
\text{mean} = \frac{1}{G}\sum_{i=1}^G r_i, \quad \text{std} = \sqrt{\frac{1}{G}\sum_{i=1}^G (r_i - \text{mean})^2}
$$

$$
\hat{A}_i = \frac{r_i - \text{mean}}{\text{std}}
$$

**2. Compute Sequence Importance Ratios**

For each sequence $y_i$ of length $|y_i|$:

$$
\text{log\_ratio\_sum} = \sum_{t=1}^{|y_i|} \left(\log\pi_{\theta}(y_{i,t}|x, y_{i,<t}) - \log\pi_{\theta_{\text{old}}}(y_{i,t}|x, y_{i,<t})\right)
$$

$$
s_i = \exp\left(\frac{\text{log\_ratio\_sum}}{|y_i|}\right)
$$

**3. Apply Clipping**

Compute both objectives:

$$
\text{unclipped}_i = s_i \cdot \hat{A}_i
$$

$$
\text{clipped}_i = \text{clip}(s_i, 1-\epsilon, 1+\epsilon) \cdot \hat{A}_i
$$

Take pessimistic bound:

$$
\text{objective}_i = \min(\text{unclipped}_i, \text{clipped}_i)
$$

**4. Average**

$$
J_{\text{GSPO}} = \frac{1}{G}\sum_{i=1}^G \text{objective}_i
$$

#### Example Calculation

Given:
- 2 sequences: $y_1$ (length 2), $y_2$ (length 3)
- Log probs new: $[[-0.2, -0.3], [-0.1, -0.4, -0.2]]$
- Log probs old: $[[-0.5, -0.6], [-0.4, -0.7, -0.5]]$
- Rewards: $[0.9, 0.7]$
- $\epsilon = 0.2$

**Step 1: Advantages**

$$
\text{mean} = 0.8, \quad \text{std} = 0.1
$$

$$
\hat{A}_1 = \frac{0.9 - 0.8}{0.1} = 1.0, \quad \hat{A}_2 = \frac{0.7 - 0.8}{0.1} = -1.0
$$

**Step 2: Importance Ratios**

For $y_1$:
$$
\text{log\_ratio\_sum} = (-0.2 - (-0.5)) + (-0.3 - (-0.6)) = 0.3 + 0.3 = 0.6
$$

$$
s_1 = \exp(0.6 / 2) = \exp(0.3) = 1.3499
$$

For $y_2$:
$$
\text{log\_ratio\_sum} = 0.3 + 0.3 + 0.3 = 0.9
$$

$$
s_2 = \exp(0.9 / 3) = \exp(0.3) = 1.3499
$$

**Step 3: Clipping**

Both $s_1, s_2 > 1 + \epsilon = 1.2$, so both are clipped to 1.2.

For $y_1$ (positive advantage):
$$
\text{unclipped} = 1.3499 \times 1.0 = 1.3499
$$
$$
\text{clipped} = 1.2 \times 1.0 = 1.2
$$
$$
\text{objective}_1 = \min(1.3499, 1.2) = 1.2
$$

For $y_2$ (negative advantage):
$$
\text{unclipped} = 1.3499 \times (-1.0) = -1.3499
$$
$$
\text{clipped} = 1.2 \times (-1.0) = -1.2
$$
$$
\text{objective}_2 = \min(-1.3499, -1.2) = -1.3499
$$

**Step 4: Average**

$$
J_{\text{GSPO}} = \frac{1.2 + (-1.3499)}{2} = -0.0749
$$

#### Gradient Analysis

The key difference between GSPO and GRPO gradients:

**GSPO gradient**:
$$
\nabla_{\theta}J_{\text{GSPO}} = \mathbb{E}\left[\frac{1}{G}\sum_{i=1}^G \left(\frac{\pi_{\theta}(y_i|x)}{\pi_{\theta_{\text{old}}}(y_i|x)}\right)^{\frac{1}{|y_i|}} \hat{A}_i \cdot \frac{1}{|y_i|}\sum_{t=1}^{|y_i|} \nabla_{\theta}\log\pi_{\theta}(y_{i,t}|x, y_{i,<t})\right]
$$

**GRPO gradient**:
$$
\nabla_{\theta}J_{\text{GRPO}} = \mathbb{E}\left[\frac{1}{G}\sum_{i=1}^G \hat{A}_i \cdot \frac{1}{|y_i|}\sum_{t=1}^{|y_i|} \frac{\pi_{\theta}(y_{i,t}|x, y_{i,<t})}{\pi_{\theta_{\text{old}}}(y_{i,t}|x, y_{i,<t})} \nabla_{\theta}\log\pi_{\theta}(y_{i,t}|x, y_{i,<t})\right]
$$

**Critical distinction**: 
- **GSPO**: All tokens weighted equally by sequence-level ratio
- **GRPO**: Tokens weighted unequally by individual token ratios

GRPO's unequal weights vary in $(0, 1+\epsilon]$ or $[1-\epsilon, +\infty)$ depending on advantage sign. This variability accumulates and causes unpredictable training dynamics.

#### Why GSPO Works

**Theoretical soundness**: Importance ratio based on sequence likelihood aligns with importance sampling principles.

**Numerical stability**: Length normalization keeps ratios in controlled range.

**Training stability**: Eliminates high-variance token-level noise that causes collapse.

**MoE compatibility**: Resolves expert-activation volatility issues. In MoE models, ~10% of experts change between gradient updates for the same tokens, making token-level ratios unreliable. GSPO's sequence-level approach is robust to these changes since overall language modeling capability is maintained.

**Infrastructure simplification**: Sequence-level likelihoods tolerate precision discrepancies between training and inference engines, potentially eliminating need for recomputation.

#### Empirical Benefits

**Superior efficiency**: GSPO achieves better performance with same computational budget.

**Stable scaling**: Enables continuous improvement through increased compute, longer sequences, and regular query updates.

**Higher clipping tolerance**: Despite clipping 100× more tokens than GRPO (15% vs 0.13%), GSPO achieves better sample efficiency, confirming that GRPO's token-level gradients are inherently noisy.

**Production success**: Powers Qwen3 models, demonstrating effectiveness at scale.",
  "description_decoded": "Implement Group Sequence Policy Optimization (GSPO), a reinforcement learning algorithm for training large language models. Unlike token-level methods like GRPO that suffer from training instability, GSPO uses sequence-level importance ratios with length normalization. Given log probabilities from new and old policies along with rewards for a group of sequences, compute the clipped objective. The algorithm prevents catastrophic model collapse by applying clipping at the sequence level rather than token level.",
  "learn_section_decoded": "### Understanding GSPO\n\nGroup Sequence Policy Optimization (GSPO) is a breakthrough reinforcement learning algorithm from Alibaba's Qwen team that addresses critical stability issues in training large language models. It powers the latest Qwen3 models and represents a significant advancement over previous approaches.\n\n#### The Problem with Token-Level Methods\n\nPrevious algorithms like GRPO (Group Relative Policy Optimization) use token-level importance ratios:\n\n$$\nw_{i,t}(\\theta) = \\frac{\\pi_{\\theta}(y_{i,t}|x, y_{i,<t})}{\\pi_{\\theta_{\\text{old}}}(y_{i,t}|x, y_{i,<t})}\n$$\n\n**Critical flaw**: This ratio is based on a single sample $y_{i,t}$ from the distribution $\\pi_{\\theta_{\\text{old}}}(\\cdot|x, y_{i,<t})$, violating the fundamental principle of importance sampling. Importance sampling requires averaging over multiple samples to correct distributional mismatch:\n\n$$\n\\mathbb{E}_{z\\sim\\pi_{\\text{tar}}}[f(z)] = \\mathbb{E}_{z\\sim\\pi_{\\text{beh}}}\\left[\\frac{\\pi_{\\text{tar}}(z)}{\\pi_{\\text{beh}}(z)} f(z)\\right]\n$$\n\nWith only one sample per token position, the token-level weight introduces high-variance noise that:\n- Accumulates over long sequences\n- Is amplified by clipping\n- Causes catastrophic, irreversible model collapse\n\n**Core principle violated**: The unit of optimization must match the unit of reward. Since rewards apply to entire sequences, token-level correction is fundamentally mismatched.\n\n#### GSPO's Sequence-Level Solution\n\nGSPO defines importance ratios at the sequence level with length normalization:\n\n$$\ns_i(\\theta) = \\left(\\frac{\\pi_{\\theta}(y_i|x)}{\\pi_{\\theta_{\\text{old}}}(y_i|x)}\\right)^{\\frac{1}{|y_i|}}\n$$\n\nExpanding the sequence likelihood:\n\n$$\ns_i(\\theta) = \\exp\\left(\\frac{1}{|y_i|}\\sum_{t=1}^{|y_i|} \\log\\frac{\\pi_{\\theta}(y_{i,t}|x, y_{i,<t})}{\\pi_{\\theta_{\\text{old}}}(y_{i,t}|x, y_{i,<t})}\\right)\n$$\n\n**Why length normalization?** Without it:\n- A few token changes cause dramatic ratio fluctuations\n- Different sequence lengths need different clipping ranges\n- Numerical instability increases\n\nWith normalization, importance ratios stay in a controlled range regardless of sequence length.\n\n#### GSPO Objective\n\nThe clipped objective is:\n\n$$\nJ_{\\text{GSPO}}(\\theta) = \\mathbb{E}_{x\\sim D, \\{y_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(\\cdot|x)}\\left[\\frac{1}{G}\\sum_{i=1}^G \\min\\left(s_i(\\theta)\\hat{A}_i, \\text{clip}(s_i(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_i\\right)\\right]\n$$\n\nWhere advantages are computed from rewards:\n\n$$\n\\hat{A}_i = \\frac{r(x, y_i) - \\text{mean}(\\{r(x, y_i)\\}_{i=1}^G)}{\\text{std}(\\{r(x, y_i)\\}_{i=1}^G)}\n$$\n\n**Key components**:\n1. **Group-based advantages**: Normalized rewards within a group of $G$ responses\n2. **Sequence-level clipping**: Entire sequences are clipped, not individual tokens\n3. **Pessimistic bound**: Take minimum of clipped and unclipped objectives\n\n#### Algorithm Steps\n\n**1. Compute Advantages**\n\nGiven a group of $G$ sequences with rewards $\\{r_i\\}_{i=1}^G$:\n\n$$\n\\text{mean} = \\frac{1}{G}\\sum_{i=1}^G r_i, \\quad \\text{std} = \\sqrt{\\frac{1}{G}\\sum_{i=1}^G (r_i - \\text{mean})^2}\n$$\n\n$$\n\\hat{A}_i = \\frac{r_i - \\text{mean}}{\\text{std}}\n$$\n\n**2. Compute Sequence Importance Ratios**\n\nFor each sequence $y_i$ of length $|y_i|$:\n\n$$\n\\text{log\\_ratio\\_sum} = \\sum_{t=1}^{|y_i|} \\left(\\log\\pi_{\\theta}(y_{i,t}|x, y_{i,<t}) - \\log\\pi_{\\theta_{\\text{old}}}(y_{i,t}|x, y_{i,<t})\\right)\n$$\n\n$$\ns_i = \\exp\\left(\\frac{\\text{log\\_ratio\\_sum}}{|y_i|}\\right)\n$$\n\n**3. Apply Clipping**\n\nCompute both objectives:\n\n$$\n\\text{unclipped}_i = s_i \\cdot \\hat{A}_i\n$$\n\n$$\n\\text{clipped}_i = \\text{clip}(s_i, 1-\\epsilon, 1+\\epsilon) \\cdot \\hat{A}_i\n$$\n\nTake pessimistic bound:\n\n$$\n\\text{objective}_i = \\min(\\text{unclipped}_i, \\text{clipped}_i)\n$$\n\n**4. Average**\n\n$$\nJ_{\\text{GSPO}} = \\frac{1}{G}\\sum_{i=1}^G \\text{objective}_i\n$$\n\n#### Example Calculation\n\nGiven:\n- 2 sequences: $y_1$ (length 2), $y_2$ (length 3)\n- Log probs new: $[[-0.2, -0.3], [-0.1, -0.4, -0.2]]$\n- Log probs old: $[[-0.5, -0.6], [-0.4, -0.7, -0.5]]$\n- Rewards: $[0.9, 0.7]$\n- $\\epsilon = 0.2$\n\n**Step 1: Advantages**\n\n$$\n\\text{mean} = 0.8, \\quad \\text{std} = 0.1\n$$\n\n$$\n\\hat{A}_1 = \\frac{0.9 - 0.8}{0.1} = 1.0, \\quad \\hat{A}_2 = \\frac{0.7 - 0.8}{0.1} = -1.0\n$$\n\n**Step 2: Importance Ratios**\n\nFor $y_1$:\n$$\n\\text{log\\_ratio\\_sum} = (-0.2 - (-0.5)) + (-0.3 - (-0.6)) = 0.3 + 0.3 = 0.6\n$$\n\n$$\ns_1 = \\exp(0.6 / 2) = \\exp(0.3) = 1.3499\n$$\n\nFor $y_2$:\n$$\n\\text{log\\_ratio\\_sum} = 0.3 + 0.3 + 0.3 = 0.9\n$$\n\n$$\ns_2 = \\exp(0.9 / 3) = \\exp(0.3) = 1.3499\n$$\n\n**Step 3: Clipping**\n\nBoth $s_1, s_2 > 1 + \\epsilon = 1.2$, so both are clipped to 1.2.\n\nFor $y_1$ (positive advantage):\n$$\n\\text{unclipped} = 1.3499 \\times 1.0 = 1.3499\n$$\n$$\n\\text{clipped} = 1.2 \\times 1.0 = 1.2\n$$\n$$\n\\text{objective}_1 = \\min(1.3499, 1.2) = 1.2\n$$\n\nFor $y_2$ (negative advantage):\n$$\n\\text{unclipped} = 1.3499 \\times (-1.0) = -1.3499\n$$\n$$\n\\text{clipped} = 1.2 \\times (-1.0) = -1.2\n$$\n$$\n\\text{objective}_2 = \\min(-1.3499, -1.2) = -1.3499\n$$\n\n**Step 4: Average**\n\n$$\nJ_{\\text{GSPO}} = \\frac{1.2 + (-1.3499)}{2} = -0.0749\n$$\n\n#### Gradient Analysis\n\nThe key difference between GSPO and GRPO gradients:\n\n**GSPO gradient**:\n$$\n\\nabla_{\\theta}J_{\\text{GSPO}} = \\mathbb{E}\\left[\\frac{1}{G}\\sum_{i=1}^G \\left(\\frac{\\pi_{\\theta}(y_i|x)}{\\pi_{\\theta_{\\text{old}}}(y_i|x)}\\right)^{\\frac{1}{|y_i|}} \\hat{A}_i \\cdot \\frac{1}{|y_i|}\\sum_{t=1}^{|y_i|} \\nabla_{\\theta}\\log\\pi_{\\theta}(y_{i,t}|x, y_{i,<t})\\right]\n$$\n\n**GRPO gradient**:\n$$\n\\nabla_{\\theta}J_{\\text{GRPO}} = \\mathbb{E}\\left[\\frac{1}{G}\\sum_{i=1}^G \\hat{A}_i \\cdot \\frac{1}{|y_i|}\\sum_{t=1}^{|y_i|} \\frac{\\pi_{\\theta}(y_{i,t}|x, y_{i,<t})}{\\pi_{\\theta_{\\text{old}}}(y_{i,t}|x, y_{i,<t})} \\nabla_{\\theta}\\log\\pi_{\\theta}(y_{i,t}|x, y_{i,<t})\\right]\n$$\n\n**Critical distinction**: \n- **GSPO**: All tokens weighted equally by sequence-level ratio\n- **GRPO**: Tokens weighted unequally by individual token ratios\n\nGRPO's unequal weights vary in $(0, 1+\\epsilon]$ or $[1-\\epsilon, +\\infty)$ depending on advantage sign. This variability accumulates and causes unpredictable training dynamics.\n\n#### Why GSPO Works\n\n**Theoretical soundness**: Importance ratio based on sequence likelihood aligns with importance sampling principles.\n\n**Numerical stability**: Length normalization keeps ratios in controlled range.\n\n**Training stability**: Eliminates high-variance token-level noise that causes collapse.\n\n**MoE compatibility**: Resolves expert-activation volatility issues. In MoE models, ~10% of experts change between gradient updates for the same tokens, making token-level ratios unreliable. GSPO's sequence-level approach is robust to these changes since overall language modeling capability is maintained.\n\n**Infrastructure simplification**: Sequence-level likelihoods tolerate precision discrepancies between training and inference engines, potentially eliminating need for recomputation.\n\n#### Empirical Benefits\n\n**Superior efficiency**: GSPO achieves better performance with same computational budget.\n\n**Stable scaling**: Enables continuous improvement through increased compute, longer sequences, and regular query updates.\n\n**Higher clipping tolerance**: Despite clipping 100× more tokens than GRPO (15% vs 0.13%), GSPO achieves better sample efficiency, confirming that GRPO's token-level gradients are inherently noisy.\n\n**Production success**: Powers Qwen3 models, demonstrating effectiveness at scale."
}