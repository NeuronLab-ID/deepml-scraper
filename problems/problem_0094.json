{
  "description": "SW1wbGVtZW50IHRoZSBtdWx0aS1oZWFkIGF0dGVudGlvbiBtZWNoYW5pc20sIGEgY3JpdGljYWwgY29tcG9uZW50IG9mIHRyYW5zZm9ybWVyIG1vZGVscy4gWW91IG5lZWQgdG8gaW1wbGVtZW50IHRocmVlIGZ1bmN0aW9uczoKCjEuIGBjb21wdXRlX3FrdihYLCBXX3EsIFdfaywgV192KWA6IENvbXB1dGUgUXVlcnksIEtleSwgYW5kIFZhbHVlIG1hdHJpY2VzIGJ5IG11bHRpcGx5aW5nIGlucHV0IFggd2l0aCB3ZWlnaHQgbWF0cmljZXMKMi4gYHNlbGZfYXR0ZW50aW9uKFEsIEssIFYpYDogQ29tcHV0ZSBzY2FsZWQgZG90LXByb2R1Y3QgYXR0ZW50aW9uIGZvciBhIHNpbmdsZSBoZWFkCjMuIGBtdWx0aV9oZWFkX2F0dGVudGlvbihRLCBLLCBWLCBuX2hlYWRzKWA6IFNwbGl0IFEsIEssIFYgaW50byBtdWx0aXBsZSBoZWFkcywgY29tcHV0ZSBhdHRlbnRpb24gZm9yIGVhY2gsIGFuZCBjb25jYXRlbmF0ZSByZXN1bHRzCgpVc2UgbnVtZXJpY2FsbHkgc3RhYmxlIHNvZnRtYXggKHN1YnRyYWN0IG1heCBiZWZvcmUgZXhwb25lbnRpYXRpbmcpLg==",
  "id": "94",
  "test_cases": [
    {
      "test": "np.random.seed(42)\nX = np.random.permutation(np.arange(16)).reshape(4, 4)\nW_q = np.random.randint(0, 4, size=(4, 4))\nW_k = np.random.randint(0, 5, size=(4, 4))\nW_v = np.random.randint(0, 6, size=(4, 4))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nresult = multi_head_attention(Q, K, V, n_heads=2)\nprint(np.round(result).astype(int).tolist())",
      "expected_output": "[[103, 109, 46, 99], [103, 109, 46, 99], [103, 109, 46, 99], [103, 109, 46, 99]]"
    },
    {
      "test": "np.random.seed(42)\nX = np.random.permutation(np.arange(48)).reshape(6, 8)\nW_q = np.random.randint(0, 4, size=(8, 8))\nW_k = np.random.randint(0, 5, size=(8, 8))\nW_v = np.random.randint(0, 6, size=(8, 8))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nresult = multi_head_attention(Q, K, V, n_heads=4)\nprint(np.round(result[0]).astype(int).tolist())",
      "expected_output": "[500, 463, 399, 495, 377, 450, 531, 362]"
    }
  ],
  "difficulty": "hard",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "Q = K = V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]]), n_heads = 2",
    "output": "Shape: (2, 4) - Two sequence positions, 4 features (2 heads × 2 features per head)",
    "reasoning": "Multi-head attention splits the input into n_heads parts, computes self-attention on each part independently, then concatenates the results. This allows the model to attend to different representation subspaces simultaneously."
  },
  "category": "Deep Learning",
  "starter_code": "import numpy as np\nfrom typing import Tuple\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute Query, Key, and Value matrices.\n    \n    Args:\n        X: Input matrix of shape (seq_len, d_model)\n        W_q, W_k, W_v: Weight matrices of shape (d_model, d_model)\n    \n    Returns:\n        Q, K, V matrices each of shape (seq_len, d_model)\n    \"\"\"\n    # Your code here\n    pass\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute scaled dot-product self-attention.\n    \n    Args:\n        Q: Query matrix of shape (seq_len, d_k)\n        K: Key matrix of shape (seq_len, d_k)\n        V: Value matrix of shape (seq_len, d_k)\n    \n    Returns:\n        Attention output of shape (seq_len, d_k)\n    \"\"\"\n    # Your code here\n    pass\n\ndef multi_head_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, n_heads: int) -> np.ndarray:\n    \"\"\"\n    Compute multi-head attention.\n    \n    Args:\n        Q, K, V: Matrices of shape (seq_len, d_model)\n        n_heads: Number of attention heads\n    \n    Returns:\n        Attention output of shape (seq_len, d_model)\n    \"\"\"\n    # Your code here\n    pass",
  "learn_section": "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",
  "title": "Implement Multi-Head Attention",
  "contributor": [
    {
      "profile_link": "https://github.com/nzomi",
      "name": "nzomi"
    }
  ],
  "createdAt": "December 15, 2025 at 10:51:58 AM UTC-0500",
  "description_decoded": "Implement the multi-head attention mechanism, a critical component of transformer models. You need to implement three functions:\n\n1. `compute_qkv(X, W_q, W_k, W_v)`: Compute Query, Key, and Value matrices by multiplying input X with weight matrices\n2. `self_attention(Q, K, V)`: Compute scaled dot-product attention for a single head\n3. `multi_head_attention(Q, K, V, n_heads)`: Split Q, K, V into multiple heads, compute attention for each, and concatenate results\n\nUse numerically stable softmax (subtract max before exponentiating).",
  "learn_section_decoded": "## Understanding Multi-Head Attention\n\nMulti-head attention is a fundamental mechanism in transformer models, allowing the model to focus on different parts of the input sequence simultaneously.\n\n### The Three Functions\n\n#### 1. compute_qkv\n\nComputes Query, Key, and Value matrices by linear projection:\n$$Q = X \\cdot W_Q, \\quad K = X \\cdot W_K, \\quad V = X \\cdot W_V$$\n\n#### 2. self_attention (Scaled Dot-Product Attention)\n\nThe attention mechanism computes:\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n\nWhere $d_k$ is the dimension of the keys. The scaling factor $\\frac{1}{\\sqrt{d_k}}$ prevents the dot products from growing too large.\n\n**Numerically Stable Softmax:**\n$$\\text{softmax}(x)_i = \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}$$\n\nSubtracting the maximum prevents overflow in the exponential.\n\n#### 3. multi_head_attention\n\nSplits Q, K, V into multiple heads, computes attention for each, and concatenates:\n\n1. **Split**: Reshape $(\\text{seq\\_len}, d_{\\text{model}})$ → $(n_{\\text{heads}}, \\text{seq\\_len}, d_k)$ where $d_k = d_{\\text{model}} / n_{\\text{heads}}$\n2. **Attend**: Apply self-attention to each head independently\n3. **Concatenate**: Combine head outputs back to $(\\text{seq\\_len}, d_{\\text{model}})$\n\n### Why Multiple Heads?\n\nEach attention head can learn to focus on different aspects:\n- One head might focus on syntactic relationships\n- Another on semantic similarity\n- Another on positional patterns\n\nThis allows the model to capture richer representations than single-head attention.\n\n### Dimensions\n\n| Matrix | Shape |\n|--------|-------|\n| Input X | (seq_len, d_model) |\n| Q, K, V | (seq_len, d_model) |\n| Per-head Q, K, V | (seq_len, d_k) |\n| Attention scores | (seq_len, seq_len) |\n| Output | (seq_len, d_model) |\n\n### Applications\n\n- **Transformers**: BERT, GPT, T5\n- **Vision Transformers**: ViT for image classification\n- **Speech Recognition**: Conformer models\n- **Protein Folding**: AlphaFold"
}