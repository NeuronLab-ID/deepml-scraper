{
  "description": "SW1wbGVtZW50IHRoZSBTb2Z0c2lnbiBhY3RpdmF0aW9uIGZ1bmN0aW9uLCBhIHNtb290aCBhY3RpdmF0aW9uIGZ1bmN0aW9uIHVzZWQgaW4gbmV1cmFsIG5ldHdvcmtzLiBZb3VyIHRhc2sgaXMgdG8gY29tcHV0ZSB0aGUgU29mdHNpZ24gdmFsdWUgZm9yIGEgZ2l2ZW4gaW5wdXQsIGVuc3VyaW5nIHRoZSBvdXRwdXQgaXMgYm91bmRlZCBiZXR3ZWVuIC0xIGFuZCAxLg==",
  "id": "100",
  "test_cases": [
    {
      "test": "print(softsign(0))",
      "expected_output": "0.0"
    },
    {
      "test": "print(softsign(1))",
      "expected_output": "0.5"
    }
  ],
  "difficulty": "easy",
  "marimo_link": "https://open-deep-ml.github.io/DML-OpenProblem/problem-softsign",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "softsign(1)",
    "output": "0.5",
    "reasoning": "For x = 1, the Softsign activation is calculated as $\\frac{x}{1 + |x|}$."
  },
  "category": "Deep Learning",
  "starter_code": "def softsign(x: float) -> float:\n\t\"\"\"\n\tImplements the Softsign activation function.\n\n\tArgs:\n\t\tx (float): Input value\n\n\tReturns:\n\t\tfloat: The Softsign of the input\t\"\"\"\n\t# Your code here\n\tpass\n\treturn round(val,4)",
  "title": "Implement the Softsign Activation Function",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/Haleshot",
      "name": "Haleshot"
    }
  ],
  "description_decoded": "Implement the Softsign activation function, a smooth activation function used in neural networks. Your task is to compute the Softsign value for a given input, ensuring the output is bounded between -1 and 1.",
  "learn_section_decoded": "## Understanding the Softsign Activation Function\n\nThe Softsign activation function is a smooth, non-linear activation function used in neural networks. Itâ€™s similar to the hyperbolic tangent (tanh) function but with different properties, particularly in its tails which approach their limits more slowly.\n\n### Mathematical Definition\n\nThe Softsign function is mathematically defined as:\n\n$$\nSoftsign(x) = \\frac{x}{1 + |x|}\n$$\n\nWhere:\n- $x$ is the input to the function\n- $|x|$ represents the absolute value of $x$\n\n### Characteristics\n\n- **Output Range:** The output is bounded between -1 and 1, approaching these values asymptotically as $x$ approaches $\\pm \\infty$.\n- **Shape:** The function has an S-shaped curve, similar to tanh but with a smoother approach to its asymptotes.\n- **Gradient:** The gradient is smoother and more gradual compared to tanh, which can help prevent vanishing gradient problems in deep networks.\n- **Symmetry:** The function is symmetric around the origin $(0,0)$.\n\n### Key Properties\n\n- **Bounded Output:** Unlike ReLU, Softsign naturally bounds its output between -1 and 1.\n- **Smoothness:** The function is continuous and differentiable everywhere.\n- **No Saturation:** The gradients approach zero more slowly than in tanh or sigmoid functions.\n- **Zero-Centered:** The function crosses through the origin, making it naturally zero-centered.\n\nThis activation function can be particularly useful in scenarios where you need bounded outputs with more gradual saturation compared to tanh or sigmoid functions.\n"
}