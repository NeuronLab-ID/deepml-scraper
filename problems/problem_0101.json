{
  "description": "SW1wbGVtZW50IHRoZSBHUlBPIChHcm91cCBSZWxhdGl2ZSBQb2xpY3kgT3B0aW1pemF0aW9uKSBvYmplY3RpdmUgZnVuY3Rpb24gYXMgZGVmaW5lZCBpbiB0aGUgRGVlcFNlZWtNYXRoIHBhcGVyLiBZb3VyIHRhc2sgaXMgdG8gY29tcHV0ZSB0aGUgb2JqZWN0aXZlIGdpdmVuIGxpa2VsaWhvb2QgcmF0aW9zLCBhZHZhbnRhZ2VzLCBhbmQgcG9saWN5IHByb2JhYmlsaXRpZXMuIAoKQ3J1Y2lhbGx5LCB5b3UgbXVzdCB1c2UgdGhlIHVuYmlhc2VkIEtMIGRpdmVyZ2VuY2UgZXN0aW1hdG9yIHNwZWNpZmllZCBpbiB0aGUgR1JQTyBhbGdvcml0aG06IAokJERfe0tMfShccGlfXHRoZXRhIHx8IFxwaV97cmVmfSkgPSBcZnJhY3tccGlfe3JlZn19e1xwaV9cdGhldGF9IC0gXGxvZyBcZnJhY3tccGlfe3JlZn19e1xwaV9cdGhldGF9IC0gMSQkCgpUaGUgZmluYWwgb2JqZWN0aXZlIGlzIHRoZSBhdmVyYWdlIG9mIHRoZSBjbGlwcGVkIHN1cnJvZ2F0ZSBvYmplY3RpdmUgbWludXMgdGhlIGJldGEtd2VpZ2h0ZWQgS0wgcGVuYWx0eS4=",
  "id": "101",
  "test_cases": [
    {
      "test": "print(round(grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01),6))",
      "expected_output": "1.032700"
    },
    {
      "test": "print(round(grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05),6))",
      "expected_output": "0.999736"
    }
  ],
  "difficulty": "hard",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)",
    "output": "1.032700",
    "reasoning": "The function calculates the clipped advantage and subtracts the specific KL estimator defined in the DeepSeekMath paper: KL = r - log(r) - 1, where r = pi_ref / pi_theta."
  },
  "category": "Reinforcement Learning",
  "starter_code": "import numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n\t\"\"\"\n\tCompute the GRPO objective function.\n\n\tArgs:\n\t\trhos: List of likelihood ratios (rho_i).\n\t\tA: List of advantage estimates (A_i).\n\t\tpi_theta_old: List of old policy probabilities.\n\t\tpi_theta_ref: List of reference policy probabilities.\n\t\tepsilon: Clipping parameter.\n\t\tbeta: KL divergence penalty coefficient.\n\n\tReturns:\n\t\tThe computed GRPO objective value.\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Implement the GRPO Objective Function",
  "learn_section": "IyMjIFVuZGVyc3RhbmRpbmcgR1JQTyAoR3JvdXAgUmVsYXRpdmUgUG9saWN5IE9wdGltaXphdGlvbikKCkdSUE8gaXMgYSByZWluZm9yY2VtZW50IGxlYXJuaW5nIGFsZ29yaXRobSBpbnRyb2R1Y2VkIGluIHRoZSAqKkRlZXBTZWVrTWF0aCoqIHBhcGVyW2NpdGU6IDEsIDEzXS4gSXQgb3B0aW1pemVzIHBvbGljeSBwYXJhbWV0ZXJzIGJ5IGFnZ3JlZ2F0aW5nIGFkdmFudGFnZXMgYWNyb3NzIGEgZ3JvdXAgb2Ygb3V0cHV0cyBmb3IgdGhlIHNhbWUgcXVlc3Rpb24sIGVsaW1pbmF0aW5nIHRoZSBuZWVkIGZvciBhIHNlcGFyYXRlIHZhbHVlIGZ1bmN0aW9uIChDcml0aWMpLgoKCgojIyMgTWF0aGVtYXRpY2FsIERlZmluaXRpb24KClRoZSBHUlBPIG9iamVjdGl2ZSBmdW5jdGlvbiBjb21iaW5lcyBhIFBQTy1zdHlsZSBjbGlwcGVkIHN1cnJvZ2F0ZSB3aXRoIGEgc3BlY2lmaWMgS0wgZGl2ZXJnZW5jZSBwZW5hbHR5IDoKCiQkCkpfe0dSUE99KFx0aGV0YSkgPSBcbWF0aGJie0V9IFxsZWZ0WyBcZnJhY3sxfXtHfSBcc3VtX3tpPTF9XkcgXGxlZnQoIFxtaW5cbGVmdCggXHJob19pIEFfaSwgXHRleHR7Y2xpcH0oXHJob19pLCAxLVxlcHNpbG9uLCAxK1xlcHNpbG9uKSBBX2kgXHJpZ2h0KSAtIFxiZXRhIERfe0tMfShccGlfe1x0aGV0YX0gXHwgXHBpX3tyZWZ9KSBccmlnaHQpIFxyaWdodF0KJCQKCiMjIyBUaGUgS0wgRGl2ZXJnZW5jZSBFc3RpbWF0b3IKCkEgZGlzdGluY3QgZmVhdHVyZSBvZiBHUlBPIGlzIGl0cyB1c2FnZSBvZiBhbiB1bmJpYXNlZCBlc3RpbWF0b3IgZm9yIHRoZSBLTCBkaXZlcmdlbmNlIChhcHByb3hpbWF0aW5nICREX3tLTH0oXHBpX1x0aGV0YSB8fCBccGlfe3JlZn0pJCksIGFzIGRlZmluZWQgaW4gKipFcXVhdGlvbiA0Kiogb2YgdGhlIHBhcGVyIDoKCiQkCkRfe0tMfSBcYXBwcm94IFxmcmFje1xwaV97cmVmfShvX2kpfXtccGlfe1x0aGV0YX0ob19pKX0gLSBcbG9nIFxmcmFje1xwaV97cmVmfShvX2kpfXtccGlfe1x0aGV0YX0ob19pKX0gLSAxCiQkCgpUaGlzIGVzdGltYXRvciBlbnN1cmVzIHRoZSBwZW5hbHR5IGlzIGFsd2F5cyBwb3NpdGl2ZSBhbmQgcHJvdmlkZXMgc3RhYmxlIGdyYWRpZW50cy4KCiMjIyBLZXkgRGlmZmVyZW5jZXMgZnJvbSBTdGFuZGFyZCBQUE8KMS4gICoqR3JvdXAgQXZlcmFnaW5nOioqIEFkdmFudGFnZXMgYXJlIG5vcm1hbGl6ZWQgYWNyb3NzIGEgZ3JvdXAgb2YgJEckIHNhbXBsZWQgb3V0cHV0c1tjaXRlOiAzMDBdLgoyLiAgKipObyBWYWx1ZSBOZXR3b3JrOioqIFRoZSBiYXNlbGluZSBmb3IgYWR2YW50YWdlcyBpcyB0aGUgZ3JvdXAgYXZlcmFnZSByZXdhcmQsIG5vdCBhIGxlYXJuZWQgY3JpdGljW2NpdGU6IDI5OV0uCjMuICAqKktMIEZvcm11bGF0aW9uOioqIFRoZSBLTCBwZW5hbHR5IGlzIHBhcnQgb2YgdGhlIG9iamVjdGl2ZSBmdW5jdGlvbiAoRXF1YXRpb24gMykgcmF0aGVyIHRoYW4gdGhlIHJld2FyZCBmdW5jdGlvbiAoRXF1YXRpb24gMilbY2l0ZTogMzAzXS4K",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "createdAt": "November 28, 2025 at 9:59:56â€¯AM UTC-0500",
  "description_decoded": "Implement the GRPO (Group Relative Policy Optimization) objective function as defined in the DeepSeekMath paper. Your task is to compute the objective given likelihood ratios, advantages, and policy probabilities. \n\nCrucially, you must use the unbiased KL divergence estimator specified in the GRPO algorithm: \n$$D_{KL}(\\pi_\\theta || \\pi_{ref}) = \\frac{\\pi_{ref}}{\\pi_\\theta} - \\log \\frac{\\pi_{ref}}{\\pi_\\theta} - 1$$\n\nThe final objective is the average of the clipped surrogate objective minus the beta-weighted KL penalty.",
  "learn_section_decoded": "### Understanding GRPO (Group Relative Policy Optimization)\n\nGRPO is a reinforcement learning algorithm introduced in the **DeepSeekMath** paper[cite: 1, 13]. It optimizes policy parameters by aggregating advantages across a group of outputs for the same question, eliminating the need for a separate value function (Critic).\n\n\n\n### Mathematical Definition\n\nThe GRPO objective function combines a PPO-style clipped surrogate with a specific KL divergence penalty :\n\n$$\nJ_{GRPO}(\\theta) = \\mathbb{E} \\left[ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min\\left( \\rho_i A_i, \\text{clip}(\\rho_i, 1-\\epsilon, 1+\\epsilon) A_i \\right) - \\beta D_{KL}(\\pi_{\\theta} \\| \\pi_{ref}) \\right) \\right]\n$$\n\n### The KL Divergence Estimator\n\nA distinct feature of GRPO is its usage of an unbiased estimator for the KL divergence (approximating $D_{KL}(\\pi_\\theta || \\pi_{ref})$), as defined in **Equation 4** of the paper :\n\n$$\nD_{KL} \\approx \\frac{\\pi_{ref}(o_i)}{\\pi_{\\theta}(o_i)} - \\log \\frac{\\pi_{ref}(o_i)}{\\pi_{\\theta}(o_i)} - 1\n$$\n\nThis estimator ensures the penalty is always positive and provides stable gradients.\n\n### Key Differences from Standard PPO\n1.  **Group Averaging:** Advantages are normalized across a group of $G$ sampled outputs[cite: 300].\n2.  **No Value Network:** The baseline for advantages is the group average reward, not a learned critic[cite: 299].\n3.  **KL Formulation:** The KL penalty is part of the objective function (Equation 3) rather than the reward function (Equation 2)[cite: 303].\n"
}