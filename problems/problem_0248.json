{
  "description": "SW4gcHJvZHVjdGlvbiBNTCBzeXN0ZW1zLCBtb25pdG9yaW5nIG1vZGVsIGluZmVyZW5jZSBwZXJmb3JtYW5jZSBpcyBlc3NlbnRpYWwgZm9yIG1haW50YWluaW5nIHNlcnZpY2UgcXVhbGl0eS4gR2l2ZW4gYSBsaXN0IG9mIGluZmVyZW5jZSBsYXRlbmN5IG1lYXN1cmVtZW50cyAoaW4gbWlsbGlzZWNvbmRzKSwgY29tcHV0ZSBrZXkgc3RhdGlzdGljcyB0aGF0IGFyZSBjb21tb25seSB1c2VkIGluIE1MT3BzIGRhc2hib2FyZHM6CgoxLiAqKlRocm91Z2hwdXQqKjogVGhlIG51bWJlciBvZiByZXF1ZXN0cyB0aGF0IGNhbiBiZSBwcm9jZXNzZWQgcGVyIHNlY29uZCAoYXNzdW1pbmcgc2luZ2xlLXRocmVhZGVkIHNlcXVlbnRpYWwgcHJvY2Vzc2luZykKMi4gKipBdmVyYWdlIExhdGVuY3kqKjogVGhlIG1lYW4gbGF0ZW5jeSBhY3Jvc3MgYWxsIG1lYXN1cmVtZW50cwozLiAqKlBlcmNlbnRpbGVzIChwNTAsIHA5NSwgcDk5KSoqOiBUaGUgbGF0ZW5jeSB2YWx1ZXMgYmVsb3cgd2hpY2ggNTAlLCA5NSUsIGFuZCA5OSUgb2YgcmVxdWVzdHMgZmFsbAoKV3JpdGUgYSBmdW5jdGlvbiBgY2FsY3VsYXRlX2luZmVyZW5jZV9zdGF0cyhsYXRlbmNpZXNfbXMpYCB0aGF0IHRha2VzIGEgbGlzdCBvZiBsYXRlbmN5IG1lYXN1cmVtZW50cyBhbmQgcmV0dXJucyBhIGRpY3Rpb25hcnkgd2l0aCB0aGUgY29tcHV0ZWQgc3RhdGlzdGljcy4gVXNlIGxpbmVhciBpbnRlcnBvbGF0aW9uIGZvciBwZXJjZW50aWxlIGNhbGN1bGF0aW9ucy4KCklmIHRoZSBpbnB1dCBsaXN0IGlzIGVtcHR5LCByZXR1cm4gYW4gZW1wdHkgZGljdGlvbmFyeS4=",
  "id": "248",
  "test_cases": [
    {
      "test": "print(calculate_inference_stats([10, 20, 30, 40, 50]))",
      "expected_output": "{'throughput_per_sec': 33.33, 'avg_latency_ms': 30.0, 'p50_ms': 30.0, 'p95_ms': 48.0, 'p99_ms': 49.6}"
    },
    {
      "test": "print(calculate_inference_stats([100, 120, 150, 200, 250, 300, 400, 500, 600, 1000]))",
      "expected_output": "{'throughput_per_sec': 2.76, 'avg_latency_ms': 362.0, 'p50_ms': 275.0, 'p95_ms': 820.0, 'p99_ms': 964.0}"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "latencies_ms = [10, 20, 30, 40, 50]",
    "output": "{'throughput_per_sec': 33.33, 'avg_latency_ms': 30.0, 'p50_ms': 30.0, 'p95_ms': 48.0, 'p99_ms': 49.6}",
    "reasoning": "With 5 latency measurements [10, 20, 30, 40, 50], the average latency is 30ms. Throughput is calculated as 1000/30 = 33.33 requests/sec. The p50 (median) is 30ms. For p95, we calculate index 0.95 * 4 = 3.8, interpolating between positions 3 and 4 gives 40 + 0.8*(50-40) = 48ms. Similarly, p99 uses index 3.96 giving 49.6ms."
  },
  "category": "MLOps",
  "starter_code": "def calculate_inference_stats(latencies_ms: list) -> dict:\n    \"\"\"\n    Calculate inference statistics for model monitoring.\n    \n    Args:\n        latencies_ms: list of latency measurements in milliseconds\n    \n    Returns:\n        dict with keys: 'throughput_per_sec', 'avg_latency_ms', 'p50_ms', 'p95_ms', 'p99_ms'\n        All values rounded to 2 decimal places.\n    \"\"\"\n    pass",
  "title": "Calculate Model Inference Statistics for Monitoring",
  "createdAt": "December 14, 2025 at 11:47:18â€¯AM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "learn_section": "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",
  "description_decoded": "In production ML systems, monitoring model inference performance is essential for maintaining service quality. Given a list of inference latency measurements (in milliseconds), compute key statistics that are commonly used in MLOps dashboards:\n\n1. **Throughput**: The number of requests that can be processed per second (assuming single-threaded sequential processing)\n2. **Average Latency**: The mean latency across all measurements\n3. **Percentiles (p50, p95, p99)**: The latency values below which 50%, 95%, and 99% of requests fall\n\nWrite a function `calculate_inference_stats(latencies_ms)` that takes a list of latency measurements and returns a dictionary with the computed statistics. Use linear interpolation for percentile calculations.\n\nIf the input list is empty, return an empty dictionary.",
  "learn_section_decoded": "## Model Inference Monitoring in MLOps\n\nMonitoring model inference performance is a critical aspect of MLOps. When deploying machine learning models to production, you need to track several key metrics to ensure your service meets quality standards.\n\n### Key Metrics\n\n#### 1. Throughput\nThroughput measures how many requests your model can handle per unit time. For sequential (single-threaded) processing:\n\n$$\\text{Throughput} = \\frac{1000}{\\text{avg\\_latency\\_ms}} \\text{ requests/second}$$\n\nThis gives you the theoretical maximum requests per second if each request is processed one after another.\n\n#### 2. Average Latency\nThe arithmetic mean of all latency measurements:\n\n$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$$\n\nwhere $x_i$ is each individual latency measurement and $n$ is the total number of measurements.\n\n#### 3. Percentiles\nPercentiles help understand the distribution of latencies and are crucial for SLA definitions. The $p$-th percentile is the value below which $p\\%$ of observations fall.\n\nUsing linear interpolation, for a sorted array of $n$ values:\n\n$$\\text{index} = \\frac{p}{100} \\times (n - 1)$$\n\nThen interpolate between adjacent values:\n\n$$P_p = x_{\\lfloor i \\rfloor} + (i - \\lfloor i \\rfloor) \\times (x_{\\lceil i \\rceil} - x_{\\lfloor i \\rfloor})$$\n\n### Common Percentiles in MLOps\n\n- **p50 (median)**: The typical user experience\n- **p95**: 95% of requests are faster than this value\n- **p99**: Critical for SLA - only 1% of requests exceed this latency\n\n### Why Percentiles Matter\n\nAverage latency can be misleading when there are outliers. For example, if most requests take 10ms but 1% take 1000ms, the average might be 20ms, hiding the poor experience of those 1% of users. Percentiles give a clearer picture of the latency distribution."
}