{
  "description": "SW1wbGVtZW50IHRoZSBmb3J3YXJkIHBhc3Mgb2YgRmxhc2ggQXR0ZW50aW9uIHYxLCBhIG1lbW9yeS1lZmZpY2llbnQgYXR0ZW50aW9uIGFsZ29yaXRobS4gU3RhbmRhcmQgYXR0ZW50aW9uIHJlcXVpcmVzIE8oTl4yKSBtZW1vcnkgdG8gc3RvcmUgdGhlIGZ1bGwgYXR0ZW50aW9uIG1hdHJpeC4gRmxhc2ggQXR0ZW50aW9uIHJlZHVjZXMgdGhpcyB0byBPKE4pIGJ5IGNvbXB1dGluZyBhdHRlbnRpb24gaW4gYmxvY2tzIGFuZCB1c2luZyBvbmxpbmUgc29mdG1heCwgbmV2ZXIgbWF0ZXJpYWxpemluZyB0aGUgZnVsbCBOeE4gYXR0ZW50aW9uIG1hdHJpeC4gVGhlIGFsZ29yaXRobSBwcm9jZXNzZXMgUXVlcnksIEtleSwgYW5kIFZhbHVlIG1hdHJpY2VzIGluIHRpbGVzLCBtYWludGFpbmluZyBydW5uaW5nIHN0YXRpc3RpY3MgZm9yIG51bWVyaWNhbCBzdGFiaWxpdHku",
  "id": "208",
  "test_cases": [
    {
      "test": "import numpy as np; np.random.seed(42); Q = np.random.randn(4, 4); K = np.random.randn(4, 4); V = np.random.randn(4, 4); out = flash_attention_forward(Q, K, V, block_size=2); print(np.round(out[0], 4).tolist())",
      "expected_output": "[-0.8285, -0.716, -0.4417, 0.6186]"
    },
    {
      "test": "import numpy as np; np.random.seed(42); Q = np.random.randn(4, 4); K = np.random.randn(4, 4); V = np.random.randn(4, 4); out = flash_attention_forward(Q, K, V, block_size=1); print(np.round(out[1], 4).tolist())",
      "expected_output": "[-0.5894, -0.8379, -0.4217, 0.3902]"
    }
  ],
  "difficulty": "hard",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "Q=[[1,0],[0,1]], K=[[1,0],[0,1]], V=[[1,2],[3,4]], block_size=1",
    "output": "[[1.6605, 2.6605], [2.3395, 3.3395]]",
    "reasoning": "The algorithm processes attention in blocks. For this 2x2 case with block_size=1, it processes each query position separately, computing attention scores, applying online softmax with running max and sum for stability, and accumulating the output. The result matches standard attention but uses less memory."
  },
  "category": "Deep Learning",
  "starter_code": "import numpy as np\n\ndef flash_attention_forward(Q: np.ndarray, K: np.ndarray, V: np.ndarray, \n                           block_size: int = 2) -> np.ndarray:\n\t\"\"\"\n\tCompute attention output using Flash Attention v1 algorithm.\n\t\n\tArgs:\n\t\tQ: Query matrix (seq_len, d_model)\n\t\tK: Key matrix (seq_len, d_model)\n\t\tV: Value matrix (seq_len, d_model)\n\t\tblock_size: Size of blocks for tiled computation\n\t\n\tReturns:\n\t\tOutput matrix (seq_len, d_model)\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Flash Attention v1 - Forward Pass",
  "createdAt": "November 18, 2025 at 8:32:53 AM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement the forward pass of Flash Attention v1, a memory-efficient attention algorithm. Standard attention requires O(N^2) memory to store the full attention matrix. Flash Attention reduces this to O(N) by computing attention in blocks and using online softmax, never materializing the full NxN attention matrix. The algorithm processes Query, Key, and Value matrices in tiles, maintaining running statistics for numerical stability.",
  "learn_section_decoded": "### Understanding Flash Attention v1\n\nFlash Attention is a breakthrough algorithm that makes transformer attention much more efficient by reducing memory usage from O(N^2) to O(N) without approximation. It achieves this through clever use of tiling and online algorithms.\n\n#### The Memory Problem\n\nStandard attention computes:\n\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V\n$$\n\nFor sequence length $N$:\n1. Compute $S = QK^T$ -> $N \\times N$ matrix\n2. Apply softmax -> still $N \\times N$\n3. Multiply by $V$ -> final output\n\n**Problem**: The $N \\times N$ attention matrix requires $O(N^2)$ memory. For $N=1024$, that's 4MB just for attention weights. For $N=4096$, it's 64MB per attention head!\n\n#### Flash Attention Solution\n\n**Key insights**:\n1. Don't materialize the full attention matrix\n2. Compute attention in blocks (tiles)\n3. Use online softmax to maintain numerical stability\n4. Recompute in backward pass instead of storing\n\n#### Blocked Computation\n\nDivide $Q$, $K$, $V$ into blocks of size $B$:\n\n- Outer loop: iterate through $Q$ blocks ($Q_1, Q_2, \\ldots$)\n- Inner loop: for each $Q$ block, iterate through $K$, $V$ blocks\n- Compute attention incrementally\n\n**Memory usage**: Only store blocks of size $B \\times d$, giving $O(N)$ memory instead of $O(N^2)$.\n\n#### Online Softmax Algorithm\n\nThe challenge: computing softmax across blocks requires knowing the maximum and sum over all elements. Flash Attention uses an online algorithm:\n\n**Maintain running statistics**:\n- $m^{(i)}$ = running maximum\n- $l^{(i)}$ = running sum (exponential)\n\n**For each new block**:\n\n1. Compute block scores: $S_{ij} = \\frac{Q_i K_j^T}{\\sqrt{d}}$\n\n2. Update max: $m^{\\text{new}} = \\max(m^{\\text{old}}, \\max(S_{ij}))$\n\n3. Update sum with correction:\n   $$l^{\\text{new}} = e^{m^{\\text{old}} - m^{\\text{new}}} \\cdot l^{\\text{old}} + \\sum e^{S_{ij} - m^{\\text{new}}}$$\n\n4. Update output with correction:\n   $$O^{\\text{new}} = e^{m^{\\text{old}} - m^{\\text{new}}} \\cdot O^{\\text{old}} + e^{S_{ij} - m^{\\text{new}}} V_j$$\n\n5. After all blocks, normalize: $O = O / l$\n\n**Why correction terms?** When we discover a new maximum, previously computed exponentials need rescaling. The factor $e^{m^{\\text{old}} - m^{\\text{new}}}$ fixes this.\n\n#### Algorithm Steps\n\n```\nInitialize: O = 0, l = 0, m = -inf\n\nFor each Q block Q_i:\n  For each K, V block (K_j, V_j):\n    # Compute block attention\n    S_ij = scale * (Q_i @ K_j^T)\n    \n    # Update statistics\n    m_new = max(m_old, max(S_ij))\n    l_new = exp(m_old - m_new) * l_old + sum(exp(S_ij - m_new))\n    \n    # Update output with correction\n    O = O * exp(m_old - m_new) + exp(S_ij - m_new) @ V_j\n    \n    m = m_new\n    l = l_new\n\n# Final normalization\nO = O / l\n```\n\n#### Numerical Stability\n\n**Standard softmax trick**: Subtract maximum before exponential:\n\n$$\n\\text{softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}\n$$\n\nFlash Attention extends this to the online setting by:\n- Tracking running maximum $m$\n- Rescaling previous computations when $m$ increases\n- Never computing $e^{\\text{large number}}$\n\n#### Example: 4×4 Matrix with Block Size 2\n\n**Input**: $Q, K, V$ are $4 \\times 4$\n\n**Blocks**: Split into $2 \\times 2$ blocks\n\n**Iteration 1** ($Q$ block 1, $K$ block 1):\n- Compute $S_{11} = Q[0:2] @ K[0:2]^T$ (2×2)\n- Initialize $m = \\max(S_{11})$, $l = \\sum e^{S_{11} - m}$\n- Compute $O[0:2] = e^{S_{11} - m} @ V[0:2]$\n\n**Iteration 2** ($Q$ block 1, $K$ block 2):\n- Compute $S_{12} = Q[0:2] @ K[2:4]^T$ (2×2)\n- Update $m^{\\text{new}} = \\max(m^{\\text{old}}, \\max(S_{12}))$\n- Rescale previous output: $O[0:2] \\leftarrow O[0:2] \\cdot e^{m^{\\text{old}} - m^{\\text{new}}}$\n- Add new contribution: $O[0:2] \\leftarrow O[0:2] + e^{S_{12} - m^{\\text{new}}} @ V[2:4]$\n\n**Iterations 3-4**: Process $Q$ block 2 similarly\n\n**Final**: Normalize by $l$\n\n#### Performance Benefits\n\n**Memory**: O(N^2) -> O(N)\n\nFor $N=1024, d=64$:\n- Standard: 4MB attention matrix\n- Flash: 0.75MB (Q, K, V only)\n- **5.3× reduction**\n\n**Speed**: Despite recomputation, Flash Attention is faster!\n- Fewer memory reads/writes (bottleneck on GPUs)\n- Better use of fast SRAM vs slow HBM\n- 2-4× speedup in practice\n\n#### Key Properties\n\n**Exact**: Produces identical results to standard attention (no approximation)\n\n**Numerically stable**: Online softmax maintains stability across blocks\n\n**IO-aware**: Minimizes expensive memory transfers\n\n**Scalable**: Enables longer sequences (16K+ tokens)\n\n#### Applications\n\n**Long context transformers**: Process documents, books, code files\n\n**High-resolution vision**: Vision transformers on large images\n\n**Efficient training**: Reduces memory for larger batches\n\n**Production deployment**: Lower memory -> cheaper inference"
}