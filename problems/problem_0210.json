{
  "description": "SW1wbGVtZW50IHRoZSBjb21wbGV0ZSBEci4gR1JQTyAoR1JQTyBEb25lIFJpZ2h0KSBvYmplY3RpdmUgZnVuY3Rpb24gZm9yIHJlaW5mb3JjZW1lbnQgbGVhcm5pbmcgd2l0aCBsYXJnZSBsYW5ndWFnZSBtb2RlbHMuIERyLiBHUlBPIGZpeGVzIHR3byBjcml0aWNhbCBiaWFzZXMgaW4gR1JQTzogKDEpIHJlc3BvbnNlLWxldmVsIGxlbmd0aCBiaWFzIGZyb20gMS98b19pfCBub3JtYWxpemF0aW9uLCBhbmQgKDIpIHF1ZXN0aW9uLWxldmVsIGRpZmZpY3VsdHkgYmlhcyBmcm9tIHN0ZCBub3JtYWxpemF0aW9uLiBUaGUgb2JqZWN0aXZlIHVzZXMgdW5iaWFzZWQgYWR2YW50YWdlcyAocmV3YXJkIG1pbnVzIG1lYW4pIGFuZCBjb21wdXRlcyB0b2tlbi1sZXZlbCBjbGlwcGVkIGltcG9ydGFuY2UgcmF0aW9zIHN1bW1lZCBvdmVyIGFsbCB0b2tlbnMuIEdpdmVuIGxvZyBwcm9iYWJpbGl0aWVzIGZyb20gbmV3IGFuZCBvbGQgcG9saWNpZXMsIHJld2FyZHMsIGFuZCBjbGlwcGluZyBwYXJhbWV0ZXIgZXBzaWxvbiwgY29tcHV0ZSB0aGUgRHIuIEdSUE8gb2JqZWN0aXZlIHZhbHVlLg==",
  "id": "210",
  "test_cases": [
    {
      "test": "import numpy as np; obj = compute_dr_grpo_objective([[0.0, -0.5], [-0.3, -0.7]], [[0.0, -0.5], [-0.3, -0.7]], [0.8, 0.6], epsilon=0.2); print(round(obj, 6))",
      "expected_output": "0.0"
    },
    {
      "test": "import numpy as np; obj = compute_dr_grpo_objective([[-0.2, -0.3], [-0.1, -0.4]], [[-0.5, -0.6], [-0.4, -0.7]], [1.0, 0.0], epsilon=0.2); print(round(obj, 6))",
      "expected_output": "-0.074929"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "log_probs_new=[[-0.2, -0.3], [-0.1, -0.4]], log_probs_old=[[-0.5, -0.6], [-0.4, -0.7]], rewards=[1.0, 0.0], epsilon=0.2",
    "output": "-0.074929",
    "reasoning": "Step 1: Compute advantages. Mean = 0.5, so Â_1 = 0.5, Â_2 = -0.5. Step 2: For response 1, token 1: ratio = exp(-0.2-(-0.5)) = 1.35, obj = min(1.35*0.5, 1.2*0.5) = 0.6. Token 2: ratio = exp(-0.3-(-0.6)) = 1.35, obj = 0.6. Response 1 total = 1.2. Step 3: For response 2, token 1: ratio = exp(-0.1-(-0.4)) = 1.35, obj = min(1.35*(-0.5), 1.2*(-0.5)) = -0.675. Token 2: ratio = exp(-0.4-(-0.7)) = 1.35, obj = -0.675. Response 2 total = -1.35. Step 4: Average over G=2: (1.2 + (-1.35))/2 = -0.075."
  },
  "category": "Reinforcement Learning",
  "starter_code": "import numpy as np\n\ndef compute_dr_grpo_objective(log_probs_new: list[list[float]], \n                               log_probs_old: list[list[float]], \n                               rewards: list[float], \n                               epsilon: float = 0.2) -> float:\n\t\"\"\"\n\tCompute the Dr. GRPO (GRPO Done Right) clipped objective.\n\t\n\tArgs:\n\t\tlog_probs_new: Log probabilities from new policy π_θ\n\t\t              Each response: [log π_θ(o_1|q), log π_θ(o_2|q,o_1), ...]\n\t\tlog_probs_old: Log probabilities from old policy π_θ_old\n\t\trewards: Rewards R(q, o_i) for each response\n\t\tepsilon: Clipping parameter for importance ratios\n\t\n\tReturns:\n\t\tDr. GRPO objective value\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Dr. GRPO: Complete Objective Function",
  "createdAt": "November 19, 2025 at 7:03:40 PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement the complete Dr. GRPO (GRPO Done Right) objective function for reinforcement learning with large language models. Dr. GRPO fixes two critical biases in GRPO: (1) response-level length bias from 1/|o_i| normalization, and (2) question-level difficulty bias from std normalization. The objective uses unbiased advantages (reward minus mean) and computes token-level clipped importance ratios summed over all tokens. Given log probabilities from new and old policies, rewards, and clipping parameter epsilon, compute the Dr. GRPO objective value.",
  "learn_section_decoded": "### Understanding Dr. GRPO Objective\n\nDr. GRPO (GRPO Done Right) fixes two critical biases in GRPO that cause training instability when scaling RL for large language models.\n\n#### The Objective\n\n$$\nJ_{\\text{Dr. GRPO}}(\\theta) = \\mathbb{E}\\left[\\frac{1}{G}\\sum_{i=1}^G \\sum_{t=1}^{|o_i|} \\min\\left(w_{i,t}\\hat{A}_i, \\text{clip}(w_{i,t}, 1-\\epsilon, 1+\\epsilon)\\hat{A}_i\\right)\\right]\n$$\n\nWhere:\n- **Importance ratio**: $w_{i,t} = \\frac{\\pi_{\\theta}(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,<t})}$\n- **Unbiased advantage**: $\\hat{A}_i = R(q, o_i) - \\text{mean}(\\{R(q, o_1), \\ldots, R(q, o_G)\\})$\n\n#### GRPO vs Dr. GRPO\n\n**GRPO (Biased)**:\n$$\nJ_{\\text{GRPO}} = \\mathbb{E}\\left[\\frac{1}{G}\\sum_{i=1}^G \\frac{1}{|o_i|}\\sum_{t=1}^{|o_i|} \\min(w_{i,t}\\hat{A}_{i,t}, \\text{clip}(w_{i,t})\\hat{A}_{i,t})\\right]\n$$\n$$\n\\hat{A}_{i,t} = \\frac{R_i - \\text{mean}(R)}{\\text{std}(R)}\n$$\n\n**Two biases**:\n1. **Length bias** ($\\frac{1}{|o_i|}$): Favors short correct responses, penalizes long incorrect responses less\n2. **Difficulty bias** ($\\frac{1}{\\text{std}(R)}$): Upweights easy/hard questions with low variance\n\n**Dr. GRPO (Unbiased)**:\n- Remove $\\frac{1}{|o_i|}$ → No length bias\n- Remove $\\frac{1}{\\text{std}(R)}$ → No difficulty bias  \n- All tokens in response share same advantage $\\hat{A}_i$\n\n#### Algorithm\n\n**Input**: Log probabilities from policies, rewards, $\\epsilon$\n\n**Step 1**: Compute unbiased advantages\n$$\n\\hat{A}_i = R_i - \\frac{1}{G}\\sum_{j=1}^G R_j\n$$\n\n**Step 2**: For each response $i$ and token $t$:\n\n1. Importance ratio: $w_{i,t} = \\exp(\\log \\pi_{\\theta}(o_{i,t}|\\cdot) - \\log \\pi_{\\theta_{\\text{old}}}(o_{i,t}|\\cdot))$\n\n2. Unclipped: $\\text{obj}_{\\text{unc}} = w_{i,t} \\cdot \\hat{A}_i$\n\n3. Clipped: $\\text{obj}_{\\text{clip}} = \\text{clip}(w_{i,t}, 1-\\epsilon, 1+\\epsilon) \\cdot \\hat{A}_i$\n\n4. Objective: $\\text{obj}_t = \\min(\\text{obj}_{\\text{unc}}, \\text{obj}_{\\text{clip}})$\n\n**Step 3**: Average over group\n$$\nJ = \\frac{1}{G}\\sum_{i=1}^G \\sum_{t=1}^{|o_i|} \\text{obj}_t\n$$\n\n#### Example Calculation\n\n**Given**: Log probs new $[[-0.2, -0.3], [-0.1, -0.4]]$, old $[[-0.5, -0.6], [-0.4, -0.7]]$, rewards $[1.0, 0.0]$, $\\epsilon=0.2$\n\n**Step 1**: Advantages\n$$\n\\mu = 0.5, \\quad \\hat{A}_1 = 0.5, \\quad \\hat{A}_2 = -0.5\n$$\n\n**Step 2**: Response 1, Token 1\n- Log ratio: $-0.2 - (-0.5) = 0.3$\n- Ratio: $w_{1,1} = e^{0.3} = 1.35$\n- Since $\\hat{A}_1 > 0$ and $w_{1,1} > 1.2$, clip to $1.2$\n- Objective: $\\min(1.35 \\times 0.5, 1.2 \\times 0.5) = 0.6$\n\n**Step 3**: Response 1, Token 2\n- Same ratio $1.35$, same result: $0.6$\n- **Response 1 total**: $0.6 + 0.6 = 1.2$\n\n**Step 4**: Response 2 (both tokens)\n- Ratio: $w_{2,t} = e^{0.3} = 1.35$\n- Since $\\hat{A}_2 < 0$, unclipped is more negative\n- Per token: $\\min(1.35 \\times (-0.5), 1.2 \\times (-0.5)) = -0.675$\n- **Response 2 total**: $2 \\times (-0.675) = -1.35$\n\n**Step 5**: Average\n$$\nJ = \\frac{1.2 + (-1.35)}{2} = -0.075\n$$\n\n#### Clipping Logic\n\n**Positive advantage** ($\\hat{A}_i > 0$, correct):\n- If $w_{i,t} > 1+\\epsilon$: Clip to prevent excessive increase → takes clipped value\n- Pessimistic bound prevents over-optimism\n\n**Negative advantage** ($\\hat{A}_i < 0$, incorrect):\n- If $w_{i,t} > 1+\\epsilon$: Unclipped is more negative → takes unclipped value  \n- Pessimistic bound ensures proper penalization\n\n#### Key Properties\n\n**1. No length bias**: Longer responses contribute proportionally (sum over all tokens, no $\\frac{1}{|o_i|}$)\n\n**2. No difficulty bias**: All questions weighted equally (no $\\frac{1}{\\text{std}(R)}$)\n\n**3. Gradient stability**: All tokens in response share same advantage\n\n**4. PPO consistency**: Recovers theoretically sound objective\n\n#### Implementation Tips\n\n**Numerical stability**: Use log-space\n```python\nratio_t = np.exp(log_probs_new[i][t] - log_probs_old[i][t])\n```\n\n**Vectorization**: Process tokens in batch\n```python\nratios = np.exp(log_probs_new[i] - log_probs_old[i])\nobjs = np.minimum(ratios * advantage, \n                  np.clip(ratios, 1-eps, 1+eps) * advantage)\ntotal += np.sum(objs)\n```\n\n**Standard hyperparameters**: $\\epsilon = 0.2$, $G = 8$\n\n#### Results\n\n**Dr. GRPO achieves**:\n- Better token efficiency (prevents wild length growth)\n- Reduced overthinking (~1.0k vs ~1.4k tokens for incorrect)\n- State-of-the-art: 43.3% on AIME 2024 with 7B model\n- Training stability (no catastrophic collapse)"
}