{
  "description": "V3JpdGUgYSBQeXRob24gZnVuY3Rpb24gYHByZWNpc2lvbmAgdGhhdCBjYWxjdWxhdGVzIHRoZSBwcmVjaXNpb24gbWV0cmljIGdpdmVuIHR3byBudW1weSBhcnJheXM6IGB5X3RydWVgIGFuZCBgeV9wcmVkYC4gVGhlIGB5X3RydWVgIGFycmF5IGNvbnRhaW5zIHRoZSB0cnVlIGJpbmFyeSBsYWJlbHMsIGFuZCB0aGUgYHlfcHJlZGAgYXJyYXkgY29udGFpbnMgdGhlIHByZWRpY3RlZCBiaW5hcnkgbGFiZWxzLiBQcmVjaXNpb24gaXMgZGVmaW5lZCBhcyB0aGUgcmF0aW8gb2YgdHJ1ZSBwb3NpdGl2ZXMgdG8gdGhlIHN1bSBvZiB0cnVlIHBvc2l0aXZlcyBhbmQgZmFsc2UgcG9zaXRpdmVzLg==",
  "mdx_file": "41c46d39-b6a0-4b90-b4a5-c8785639b533.mdx",
  "test_cases": [
    {
      "test": "import numpy as np\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\nresult = precision(y_true, y_pred)\nprint(result)",
      "expected_output": "1.0"
    },
    {
      "test": "import numpy as np\ny_true = np.array([1, 0, 1, 1, 0, 0])\ny_pred = np.array([1, 0, 0, 0, 0, 1])\nresult = precision(y_true, y_pred)\nprint(result)",
      "expected_output": "0.5"
    }
  ],
  "difficulty": "easy",
  "pytorch_difficulty": "easy",
  "likes": "0",
  "video": "https://youtu.be/u99yBNF4vE0",
  "example": {
    "input": "import numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\n\nresult = precision(y_true, y_pred)\nprint(result)",
    "output": "1.0",
    "reasoning": "- True Positives (TP) = 3\n- False Positives (FP) = 0\n- Precision = TP / (TP + FP) = 3 / (3 + 0) = 1.0"
  },
  "dislikes": "0",
  "category": "Machine Learning",
  "starter_code": "import numpy as np\ndef precision(y_true, y_pred):\n\t# Your code here\n\tpass\n",
  "title": "Implement Precision Metric",
  "learn_section": "CiMjIFVuZGVyc3RhbmRpbmcgUHJlY2lzaW9uIGluIENsYXNzaWZpY2F0aW9uCgpQcmVjaXNpb24gaXMgYSBrZXkgbWV0cmljIHVzZWQgaW4gdGhlIGV2YWx1YXRpb24gb2YgY2xhc3NpZmljYXRpb24gbW9kZWxzLCBwYXJ0aWN1bGFybHkgaW4gYmluYXJ5IGNsYXNzaWZpY2F0aW9uLiBJdCBwcm92aWRlcyBpbnNpZ2h0IGludG8gdGhlIGFjY3VyYWN5IG9mIHRoZSBwb3NpdGl2ZSBwcmVkaWN0aW9ucyBtYWRlIGJ5IHRoZSBtb2RlbC4KCiMjIyBNYXRoZW1hdGljYWwgRGVmaW5pdGlvbgpQcmVjaXNpb24gaXMgZGVmaW5lZCBhcyB0aGUgcmF0aW8gb2YgdHJ1ZSBwb3NpdGl2ZXMgKFRQKSB0byB0aGUgc3VtIG9mIHRydWUgcG9zaXRpdmVzIGFuZCBmYWxzZSBwb3NpdGl2ZXMgKEZQKToKJCQKXHRleHR7UHJlY2lzaW9ufSA9IFxmcmFje1x0ZXh0e1RQfX17XHRleHR7VFB9ICsgXHRleHR7RlB9fQokJAoKV2hlcmU6Ci0gKipUcnVlIFBvc2l0aXZlcyAoVFApKio6IFRoZSBudW1iZXIgb2YgcG9zaXRpdmUgc2FtcGxlcyB0aGF0IGFyZSBjb3JyZWN0bHkgaWRlbnRpZmllZCBhcyBwb3NpdGl2ZS4KLSAqKkZhbHNlIFBvc2l0aXZlcyAoRlApKio6IFRoZSBudW1iZXIgb2YgbmVnYXRpdmUgc2FtcGxlcyB0aGF0IGFyZSBpbmNvcnJlY3RseSBpZGVudGlmaWVkIGFzIHBvc2l0aXZlLgoKIyMjIENoYXJhY3RlcmlzdGljcyBvZiBQcmVjaXNpb24KLSAqKlJhbmdlKio6IFByZWNpc2lvbiByYW5nZXMgZnJvbSAwIHRvIDEsIHdoZXJlIDEgaW5kaWNhdGVzIHBlcmZlY3QgcHJlY2lzaW9uIChubyBmYWxzZSBwb3NpdGl2ZXMpIGFuZCAwIGluZGljYXRlcyBubyB0cnVlIHBvc2l0aXZlcy4KLSAqKkludGVycHJldGF0aW9uKio6IEhpZ2ggcHJlY2lzaW9uIG1lYW5zIHRoYXQgdGhlIG1vZGVsIGhhcyBhIGxvdyBmYWxzZSBwb3NpdGl2ZSByYXRlLCBtZWFuaW5nIGl0IHJhcmVseSBsYWJlbHMgbmVnYXRpdmUgc2FtcGxlcyBhcyBwb3NpdGl2ZS4KLSAqKlVzZSBDYXNlKio6IFByZWNpc2lvbiBpcyBwYXJ0aWN1bGFybHkgdXNlZnVsIHdoZW4gdGhlIGNvc3Qgb2YgZmFsc2UgcG9zaXRpdmVzIGlzIGhpZ2gsIHN1Y2ggYXMgaW4gbWVkaWNhbCBkaWFnbm9zaXMgb3IgZnJhdWQgZGV0ZWN0aW9uLgoKSW4gdGhpcyBwcm9ibGVtLCB5b3Ugd2lsbCBpbXBsZW1lbnQgYSBmdW5jdGlvbiB0byBjYWxjdWxhdGUgcHJlY2lzaW9uIGdpdmVuIHRoZSB0cnVlIGxhYmVscyBhbmQgcHJlZGljdGVkIGxhYmVscyBvZiBhIGJpbmFyeSBjbGFzc2lmaWNhdGlvbiB0YXNrLgo=",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "import torch\ny_true = torch.tensor([1, 0, 1, 1, 0, 1])\ny_pred = torch.tensor([1, 0, 1, 0, 0, 1])\nresult = precision(y_true, y_pred)\nprint(float(result))",
      "expected_output": "1.0"
    },
    {
      "test": "import torch\ny_true = torch.tensor([1, 0, 1, 1, 0, 0])\ny_pred = torch.tensor([1, 0, 0, 0, 0, 1])\nresult = precision(y_true, y_pred)\nprint(float(result))",
      "expected_output": "0.5"
    },
    {
      "test": "import torch\ny_true = torch.tensor([0, 0, 0, 0])\ny_pred = torch.tensor([0, 0, 0, 0])\nresult = precision(y_true, y_pred)\nprint(float(result))",
      "expected_output": "0.0"
    }
  ],
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpkZWYgcHJlY2lzaW9uKHlfdHJ1ZTogdG9yY2guVGVuc29yLCB5X3ByZWQ6IHRvcmNoLlRlbnNvcikgLT4gdG9yY2guVGVuc29yOgogICAgIiIiCiAgICBDYWxjdWxhdGVzIHRoZSBwcmVjaXNpb24gbWV0cmljIGZvciBiaW5hcnkgY2xhc3NpZmljYXRpb24uCiAgICAKICAgIFByZWNpc2lvbiBpcyBkZWZpbmVkIGFzIHRoZSByYXRpbyBvZiB0cnVlIHBvc2l0aXZlcyB0byB0aGUgc3VtIG9mIAogICAgdHJ1ZSBwb3NpdGl2ZXMgYW5kIGZhbHNlIHBvc2l0aXZlcy4KICAgIAogICAgQXJnczoKICAgICAgICB5X3RydWU6IFRydWUgYmluYXJ5IGxhYmVscyAoMUQgdGVuc29yKQogICAgICAgIHlfcHJlZDogUHJlZGljdGVkIGJpbmFyeSBsYWJlbHMgKDFEIHRlbnNvcikKICAgIAogICAgUmV0dXJuczoKICAgICAgICBQcmVjaXNpb24gdmFsdWUgYXMgYSBzY2FsYXIgdGVuc29yCiAgICAiIiIKICAgICMgWW91ciBpbXBsZW1lbnRhdGlvbiBoZXJlCiAgICBwYXNzCg==",
  "description_decoded": "Write a Python function `precision` that calculates the precision metric given two numpy arrays: `y_true` and `y_pred`. The `y_true` array contains the true binary labels, and the `y_pred` array contains the predicted binary labels. Precision is defined as the ratio of true positives to the sum of true positives and false positives.",
  "learn_section_decoded": "\n## Understanding Precision in Classification\n\nPrecision is a key metric used in the evaluation of classification models, particularly in binary classification. It provides insight into the accuracy of the positive predictions made by the model.\n\n### Mathematical Definition\nPrecision is defined as the ratio of true positives (TP) to the sum of true positives and false positives (FP):\n$$\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n$$\n\nWhere:\n- **True Positives (TP)**: The number of positive samples that are correctly identified as positive.\n- **False Positives (FP)**: The number of negative samples that are incorrectly identified as positive.\n\n### Characteristics of Precision\n- **Range**: Precision ranges from 0 to 1, where 1 indicates perfect precision (no false positives) and 0 indicates no true positives.\n- **Interpretation**: High precision means that the model has a low false positive rate, meaning it rarely labels negative samples as positive.\n- **Use Case**: Precision is particularly useful when the cost of false positives is high, such as in medical diagnosis or fraud detection.\n\nIn this problem, you will implement a function to calculate precision given the true labels and predicted labels of a binary classification task.\n"
}