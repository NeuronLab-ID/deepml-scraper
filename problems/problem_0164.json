{
  "description": "QSBnYW1ibGVyIGhhcyB0aGUgY2hhbmNlIHRvIGJldCBvbiBhIHNlcXVlbmNlIG9mIGNvaW4gZmxpcHMuIElmIHRoZSBjb2luIGxhbmRzIGhlYWRzLCB0aGUgZ2FtYmxlciB3aW5zIHRoZSBhbW91bnQgc3Rha2VkOyBpZiB0YWlscywgdGhlIGdhbWJsZXIgbG9zZXMgdGhlIHN0YWtlLiBUaGUgZ29hbCBpcyB0byByZWFjaCAxMDAsIHN0YXJ0aW5nIGZyb20gYSBnaXZlbiBjYXBpdGFsICRzJCAod2l0aCAkMCA8IHMgPCAxMDAkKS4gVGhlIGdhbWUgZW5kcyB3aGVuIHRoZSBnYW1ibGVyIHJlYWNoZXMgJDAkIChiYW5rcnVwdGN5KSBvciAkMTAwJCAoZ29hbCkuIE9uIGVhY2ggZmxpcCwgdGhlIGdhbWJsZXIgY2FuIGJldCBhbnkgaW50ZWdlciBhbW91bnQgZnJvbSAkMSQgdXAgdG8gJFxtaW4ocywgMTAwLXMpJC4KClRoZSBwcm9iYWJpbGl0eSBvZiBoZWFkcyBpcyAkcF9oJCAoa25vd24pLiBSZXdhcmQgaXMgJCsxJCBpZiB0aGUgZ2FtYmxlciByZWFjaGVzICQxMDAkIGluIGEgdHJhbnNpdGlvbiwgJDAkIG90aGVyd2lzZS4KCioqWW91ciBUYXNrOioqCldyaXRlIGEgZnVuY3Rpb24gYGdhbWJsZXJfdmFsdWVfaXRlcmF0aW9uKHBoLCB0aGV0YT0xZS05KWAgdGhhdDoKLSBDb21wdXRlcyB0aGUgb3B0aW1hbCBzdGF0ZS12YWx1ZSBmdW5jdGlvbiAkVihzKSQgZm9yIGFsbCAkcyA9IDEsIC4uLiwgOTkkIHVzaW5nIHZhbHVlIGl0ZXJhdGlvbi4KLSBSZXR1cm5zIHRoZSBvcHRpbWFsIHBvbGljeSBhcyBhIG1hcHBpbmcgZnJvbSBzdGF0ZSAkcyQgdG8gdGhlIG9wdGltYWwgc3Rha2UgJGFeKiQgKGNhbiByZXR1cm4gYW55IG9wdGltYWwgc3Rha2UgaWYgdGhlcmUgYXJlIHRpZXMpLgoKKipJbnB1dHM6KioKLSBgcGhgOiBwcm9iYWJpbGl0eSBvZiBoZWFkcyAoZmxvYXQgYmV0d2VlbiAwIGFuZCAxKQotIGB0aGV0YWA6IHRocmVzaG9sZCBmb3IgdmFsdWUgaXRlcmF0aW9uIGNvbnZlcmdlbmNlIChkZWZhdWx0ICQxZS05JCkKCioqUmV0dXJuczoqKgotIGBWYDogYXJyYXkvbGlzdCBvZiBsZW5ndGggMTAxLCAkVltzXSQgaXMgdGhlIHZhbHVlIGZvciBzdGF0ZSAkcyQKLSBgcG9saWN5YDogYXJyYXkvbGlzdCBvZiBsZW5ndGggMTAxLCAkcG9saWN5W3NdJCBpcyB0aGUgb3B0aW1hbCBzdGFrZSBpbiBzdGF0ZSAkcyQgKDAgaWYgJHM9MCQgb3IgJHM9MTAwJCkK",
  "id": "164",
  "test_cases": [
    {
      "test": "ph = 0.4\nV, policy = gambler_value_iteration(ph)\nprint(round(V[50], 4))\nprint(policy[50])",
      "expected_output": "0.4\n50"
    },
    {
      "test": "ph = 0.25\nV, policy = gambler_value_iteration(ph)\nprint(round(V[80], 4))\nprint(policy[80])",
      "expected_output": "0.4534\n5"
    }
  ],
  "difficulty": "hard",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "ph = 0.4\nV, policy = gambler_value_iteration(ph)\nprint(round(V[50], 4))\nprint(policy[50])",
    "output": "0.0178\n1",
    "reasoning": "From state 50, the optimal action is to bet 1, with a probability of reaching 100 of about 0.0178 when ph=0.4."
  },
  "category": "Reinforcement Learning",
  "starter_code": "def gambler_value_iteration(ph, theta=1e-9):\n    \"\"\"\n    Computes the optimal value function and policy for the Gambler's Problem.\n    Args:\n      ph: probability of heads\n      theta: convergence threshold\n    Returns:\n      V: list of values for all states 0..100\n      policy: list of optimal stakes for all states 0..100\n    \"\"\"\n    # Your code here\n    pass",
  "title": "Gambler's Problem: Value Iteration",
  "learn_section": "IyAqKkdhbWJsZXIncyBQcm9ibGVtIGFuZCBWYWx1ZSBJdGVyYXRpb24qKgoKSW4gdGhlIEdhbWJsZXIncyBQcm9ibGVtLCBhIGdhbWJsZXIgcmVwZWF0ZWRseSBiZXRzIG9uIGEgY29pbiBmbGlwIHdpdGggcHJvYmFiaWxpdHkgJHBfaCQgb2YgaGVhZHMuIFRoZSBnb2FsIGlzIHRvIHJlYWNoIDEwMCBzdGFydGluZyBmcm9tIHNvbWUgY2FwaXRhbCAkcyQuIEF0IGVhY2ggc3RhdGUsIHRoZSBnYW1ibGVyIGNob29zZXMgYSBzdGFrZSAkYSQgKGJldHdlZW4gJDEkIGFuZCAkXG1pbihzLCAxMDAtcykkKS4gSWYgaGVhZHMsIHRoZSBnYW1ibGVyIGdhaW5zICRhJDsgaWYgdGFpbHMsIGxvc2VzICRhJC4gVGhlIGdhbWUgZW5kcyBhdCAkMCQgb3IgJDEwMCQuCgpUaGUgb2JqZWN0aXZlIGlzIHRvIGZpbmQgdGhlIHBvbGljeSB0aGF0IG1heGltaXplcyB0aGUgcHJvYmFiaWxpdHkgb2YgcmVhY2hpbmcgMTAwICh0aGUgc3RhdGUtdmFsdWUgZnVuY3Rpb24gJFYocykkIGdpdmVzIHRoaXMgcHJvYmFiaWxpdHkpLiBUaGUgdmFsdWUgaXRlcmF0aW9uIHVwZGF0ZSBpczoKCiQkClYocykgPSBcbWF4X3thIFxpbiBcdGV4dHtBY3Rpb25zfShzKX0gXEJpZ1sgcF9oIChcdGV4dHtyZXdhcmR9ICsgVihzICsgYSkpICsgKDEtcF9oKVYocy1hKSBcQmlnXQokJAoKd2hlcmUgdGhlIHJld2FyZCBpcyAkKzEkIG9ubHkgaWYgJHMgKyBhID0gMTAwJC4KCkFmdGVyIGNvbnZlcmdlbmNlLCB0aGUgZ3JlZWR5IHBvbGljeSBjaG9vc2VzIHRoZSBzdGFrZSBtYXhpbWl6aW5nIHRoaXMgdmFsdWUuIFRoaXMgaXMgYSBjbGFzc2ljIGVwaXNvZGljIE1EUCwgYW5kIHRoZSBvcHRpbWFsIHBvbGljeSBtYXkgbm90IGJlIHVuaXF1ZSAodGllcyBhcmUgcG9zc2libGUpLg==",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "description_decoded": "A gambler has the chance to bet on a sequence of coin flips. If the coin lands heads, the gambler wins the amount staked; if tails, the gambler loses the stake. The goal is to reach 100, starting from a given capital $s$ (with $0 < s < 100$). The game ends when the gambler reaches $0$ (bankruptcy) or $100$ (goal). On each flip, the gambler can bet any integer amount from $1$ up to $\\min(s, 100-s)$.\n\nThe probability of heads is $p_h$ (known). Reward is $+1$ if the gambler reaches $100$ in a transition, $0$ otherwise.\n\n**Your Task:**\nWrite a function `gambler_value_iteration(ph, theta=1e-9)` that:\n- Computes the optimal state-value function $V(s)$ for all $s = 1, ..., 99$ using value iteration.\n- Returns the optimal policy as a mapping from state $s$ to the optimal stake $a^*$ (can return any optimal stake if there are ties).\n\n**Inputs:**\n- `ph`: probability of heads (float between 0 and 1)\n- `theta`: threshold for value iteration convergence (default $1e-9$)\n\n**Returns:**\n- `V`: array/list of length 101, $V[s]$ is the value for state $s$\n- `policy`: array/list of length 101, $policy[s]$ is the optimal stake in state $s$ (0 if $s=0$ or $s=100$)\n",
  "learn_section_decoded": "# **Gambler's Problem and Value Iteration**\n\nIn the Gambler's Problem, a gambler repeatedly bets on a coin flip with probability $p_h$ of heads. The goal is to reach 100 starting from some capital $s$. At each state, the gambler chooses a stake $a$ (between $1$ and $\\min(s, 100-s)$). If heads, the gambler gains $a$; if tails, loses $a$. The game ends at $0$ or $100$.\n\nThe objective is to find the policy that maximizes the probability of reaching 100 (the state-value function $V(s)$ gives this probability). The value iteration update is:\n\n$$\nV(s) = \\max_{a \\in \\text{Actions}(s)} \\Big[ p_h (\\text{reward} + V(s + a)) + (1-p_h)V(s-a) \\Big]\n$$\n\nwhere the reward is $+1$ only if $s + a = 100$.\n\nAfter convergence, the greedy policy chooses the stake maximizing this value. This is a classic episodic MDP, and the optimal policy may not be unique (ties are possible)."
}