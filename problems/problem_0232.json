{
  "description": "SW1wbGVtZW50IFBUWCAoUHJlLXRyYWluaW5nKSBMb3NzIHRvIHByZXZlbnQgY2F0YXN0cm9waGljIGZvcmdldHRpbmcgZHVyaW5nIFJMSEYuIFBUWCBMb3NzIGNvbWJpbmVzIHJlaW5mb3JjZW1lbnQgbGVhcm5pbmcgb2JqZWN0aXZlcyB3aXRoIGNyb3NzLWVudHJvcHkgbG9zcyBvbiBwcmUtdHJhaW5pbmcgZGF0YSwgbWFpbnRhaW5pbmcgZ2VuZXJhbCBsYW5ndWFnZSBjYXBhYmlsaXRpZXMgd2hpbGUgb3B0aW1pemluZyBmb3IgcmV3YXJkLiBHaXZlbiBSTCBsb3NzLCBtb2RlbCBsb2dpdHMgb24gcHJlLXRyYWluaW5nIGJhdGNoLCB0cnVlIGxhYmVscywgYW5kIGJldGEgY29lZmZpY2llbnQsIGNvbXB1dGUgdG90YWwgbG9zcy4gVXNlZCBpbiBJbnN0cnVjdEdQVCwgQ2hhdEdQVCwgS2ltaSBLMiwgYW5kIG90aGVyIFJMSEYgc3lzdGVtcy4=",
  "id": "232",
  "test_cases": [
    {
      "test": "logits = np.array([[10.0, 0.0, 0.0], [0.0, 10.0, 0.0]]); labels = np.array([0, 1]); total, ce, weighted = compute_ptx_loss(0.5, logits, labels, 0.1); print(f'{total:.6f}')",
      "expected_output": "0.500009"
    },
    {
      "test": "logits = np.array([[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]); labels = np.array([0, 2]); total, ce, weighted = compute_ptx_loss(0.3, logits, labels, 0.2); print(f'{total:.6f}')",
      "expected_output": "0.519722"
    }
  ],
  "difficulty": "medium",
  "pytorch_difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "rl_loss=0.5, logits=[[10,0,0],[0,10,0]], labels=[0,1], beta=0.1",
    "output": "(0.500009, 0.000091, 0.000009)",
    "reasoning": "Perfect predictions. Softmax gives ~[1,0,0] and ~[0,1,0]. CE: -log(1)0 for both. Average CE0. Total: 0.5 + 0.1*0  0.5. Minimal PTX penalty for correct predictions."
  },
  "category": "Deep Learning",
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubi5mdW5jdGlvbmFsIGFzIEYKCmRlZiBjb21wdXRlX3B0eF9sb3NzX3B5dG9yY2goCiAgICBybF9sb3NzOiB0b3JjaC5UZW5zb3IsCiAgICBwcmV0cmFpbl9sb2dpdHM6IHRvcmNoLlRlbnNvciwKICAgIHByZXRyYWluX2xhYmVsczogdG9yY2guVGVuc29yLAogICAgYmV0YV9wdHg6IGZsb2F0ID0gMC4xCikgLT4gdHVwbGVbdG9yY2guVGVuc29yLCB0b3JjaC5UZW5zb3IsIHRvcmNoLlRlbnNvcl06CgkiIiIKCUNvbXB1dGUgUFRYIExvc3MgdXNpbmcgUHlUb3JjaC4KCQoJQXJnczoKCQlybF9sb3NzOiBSTCBsb3NzIHRlbnNvciAoc2NhbGFyKQoJCXByZXRyYWluX2xvZ2l0czogU2hhcGUgKGJhdGNoX3NpemUsIHZvY2FiX3NpemUpCgkJcHJldHJhaW5fbGFiZWxzOiBTaGFwZSAoYmF0Y2hfc2l6ZSwpCgkJYmV0YV9wdHg6IFdlaWdodCBjb2VmZmljaWVudAoJCglSZXR1cm5zOgoJCSh0b3RhbF9sb3NzLCBjZV9sb3NzLCB3ZWlnaHRlZF9jZV9sb3NzKSBhcyB0ZW5zb3JzCgkJCglIaW50czoKCQktIFVzZSBGLmNyb3NzX2VudHJvcHkoKSBmb3IgbnVtZXJpY2FsIHN0YWJpbGl0eQoJCS0gY3Jvc3NfZW50cm9weSBhdXRvbWF0aWNhbGx5IGFwcGxpZXMgc29mdG1heAoJCS0gcmVkdWN0aW9uPSdtZWFuJyBmb3IgYmF0Y2ggYXZlcmFnaW5nCgkiIiIKCSMgWW91ciBjb2RlIGhlcmUKCXBhc3M=",
  "title": "PTX Loss for Catastrophic Forgetting Prevention (RLHF)",
  "createdAt": "December 11, 2025 at 2:06:00â€¯PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "logits = torch.tensor([[10.0, 0.0, 0.0], [0.0, 10.0, 0.0]]); labels = torch.tensor([0, 1], dtype=torch.long); rl_loss = torch.tensor(0.5); total, ce, weighted = compute_ptx_loss_pytorch(rl_loss, logits, labels, 0.1); print(f'{total.item():.6f}')",
      "expected_output": "0.500009"
    },
    {
      "test": "logits = torch.tensor([[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]); labels = torch.tensor([0, 2], dtype=torch.long); rl_loss = torch.tensor(0.3); total, ce, weighted = compute_ptx_loss_pytorch(rl_loss, logits, labels, 0.2); print(f'{total.item():.6f}')",
      "expected_output": "0.519722"
    },
    {
      "test": "logits = torch.tensor([[0.0, 10.0], [10.0, 0.0]]); labels = torch.tensor([0, 0], dtype=torch.long); rl_loss = torch.tensor(1.0); total, ce, weighted = compute_ptx_loss_pytorch(rl_loss, logits, labels, 0.15); print(f'{total.item():.6f}')",
      "expected_output": "1.750007"
    },
    {
      "test": "logits = torch.tensor([[2.0, 1.0], [1.0, 2.0]]); labels = torch.tensor([0, 1], dtype=torch.long); rl_loss = torch.tensor(0.8); total, ce, weighted = compute_ptx_loss_pytorch(rl_loss, logits, labels, 0.5); print(f'{weighted.item():.6f}')",
      "expected_output": "0.156631"
    },
    {
      "test": "logits = torch.tensor([[3.0, 1.0, 0.0], [1.0, 3.0, 0.0], [0.0, 1.0, 3.0], [2.0, 2.0, 2.0]]); labels = torch.tensor([0, 1, 2, 1], dtype=torch.long); rl_loss = torch.tensor(0.6); total, ce, weighted = compute_ptx_loss_pytorch(rl_loss, logits, labels, 0.1); print(f'{ce.item():.6f}')",
      "expected_output": "0.402038"
    }
  ],
  "learn_section": "### PTX (Pre-training) Loss for RLHF

PTX (Pre-training) Loss prevents catastrophic forgetting during reinforcement learning from human feedback (RLHF) by combining RL objectives with language modeling on pre-training data.

#### The Catastrophic Forgetting Problem

When fine-tuning language models with reinforcement learning:

**Without PTX Loss**:
- Model optimizes purely for reward signal
- Forgets basic language capabilities (grammar, spelling, factual knowledge)
- Loses general intelligence while maximizing specific reward
- Results in degraded overall performance

**Example**: A chatbot trained only with RL might:
- Forget how to spell common words
- Lose factual knowledge
- Generate grammatically incorrect sentences
- Only produce responses that game the reward model

#### PTX Loss Formula

$$
L_{\text{total}} = L_{\text{RL}}(\theta) + \beta_{\text{ptx}} \cdot L_{\text{CE}}(\theta, D_{\text{pretrain}})
$$

Where:
- $L_{\text{RL}}(\theta)$ = Reinforcement learning loss (e.g., PPO objective)
- $L_{\text{CE}}(\theta, D_{\text{pretrain}})$ = Cross-entropy loss on pre-training data
- $\beta_{\text{ptx}}$ = Weight coefficient (typically 0.05-0.2)
- $\theta$ = Model parameters

**Key insight**: Maintains language modeling ability while optimizing for reward.

#### Cross-Entropy Loss Component

The cross-entropy loss on pre-training data:

$$
L_{\text{CE}} = -\frac{1}{N} \sum_{i=1}^{N} \log P(y_i | x_i; \theta)
$$

**Computation**:

**Step 1**: Apply softmax to logits
$$
P(y_j) = \frac{e^{z_j}}{\sum_k e^{z_k}}
$$

**Step 2**: Compute negative log-likelihood
$$
\text{CE}_i = -\log P(y_{\text{true},i})
$$

**Step 3**: Average over batch
$$
L_{\text{CE}} = \frac{1}{N} \sum_{i=1}^{N} \text{CE}_i
$$

#### Example Calculation

**Scenario**: Model with perfect predictions on pre-training data

**Given**:
- RL loss: 0.5
- Logits: [[10, 0, 0], [0, 10, 0]] (very confident)
- True labels: [0, 1] (correct)
- Beta_ptx: 0.1

**Step 1**: Compute probabilities
- Sample 1: Softmax([10, 0, 0])  [1.0, 0.0, 0.0]
- Sample 2: Softmax([0, 10, 0])  [0.0, 1.0, 0.0]

**Step 2**: Compute CE
- Sample 1: -log(1.0)  0
- Sample 2: -log(1.0)  0
- Average: 0

**Step 3**: Total loss
$$
L_{\text{total}} = 0.5 + 0.1 \times 0 = 0.5
$$

Perfect predictions  minimal PTX penalty

#### Effect of Prediction Quality

**High confidence, correct** (logits [10, 0, 0], label 0):
- Probability: [~1.0, ~0.0, ~0.0]
- CE: -log(1.0)  0
- Low penalty (good!)

**Uniform distribution** (logits [1, 1, 1], label 0):
- Probability: [1/3, 1/3, 1/3]
- CE: -log(1/3)  1.099
- Medium penalty

**High confidence, wrong** (logits [0, 10], label 0):
- Probability: [~0.0, ~1.0]
- CE: -log(0.0) \  very large
- High penalty (prevents forgetting!)

#### Choosing Beta_ptx

The coefficient $\beta_{\text{ptx}}$ controls the trade-off:

**Beta = 0.0**: No forgetting prevention
- Pure RL optimization
- Fast reward improvement
- Risk of catastrophic forgetting

**Beta = 0.05-0.1**: Light preservation (typical)
- Mostly RL-driven
- Some capability maintenance
- Good balance for most tasks

**Beta = 0.2-0.5**: Strong preservation
- Equal importance to capabilities
- Slower RL improvement
- Better for complex models

**Beta = 1.0**: Equal weighting
- Balanced RL and pre-training
- Very conservative
- Slowest forgetting

**Typical choice**: $\beta_{\text{ptx}} = 0.1$ (used in InstructGPT, Kimi K2)

#### RLHF Training Pipeline

**Phase 1: Supervised Fine-tuning (SFT)**
- Train on high-quality demonstrations
- Loss: Pure cross-entropy on supervised data
- Result: Model that imitates demonstrations

**Phase 2: Reward Modeling (RM)**
- Train reward model on human preferences
- Loss: Binary classification (preferred vs not)
- Result: Reward function that captures human values

**Phase 3: RL Fine-tuning with PTX**
- Optimize policy using reward model
- Loss: $L_{\text{total}} = L_{\text{RL}} + \beta_{\text{ptx}} \cdot L_{\text{CE}}$
- Result: Model that maximizes reward while preserving capabilities

**Training step**:
1. Generate responses with current policy
2. Compute reward from reward model
3. Compute RL loss (PPO clipped objective)
4. Sample batch from pre-training data
5. Compute CE loss on pre-training batch
6. Combine: $L_{\text{total}} = L_{\text{RL}} + \beta_{\text{ptx}} \cdot L_{\text{CE}}$
7. Backpropagate and update model

#### Pre-training Data Selection

**What to include**:
- Representative examples of desired capabilities
- High-quality, diverse text
- Tasks model should maintain (QA, summarization, reasoning)
- Mix of domains and styles

**Batch size**: 32-256 samples per RL batch

**Data mixing**: Typically sample from same distribution as original pre-training

#### Real-World Examples

**InstructGPT (OpenAI, 2022)**:
- Uses PTX loss with $\beta_{\text{ptx}} \approx 0.1$
- Pre-training data: Mix of prompts and completions
- Result: Maintains helpfulness while optimizing for human preference

**Kimi K2 (Moonshot AI, 2024)**:
- Explicit PTX loss formulation
- Prevents forgetting during long-context fine-tuning
- Critical for maintaining general capabilities

**Claude (Anthropic)**:
- Uses auxiliary objectives (similar to PTX)
- Maintains helpfulness, harmlessness, honesty (HHH)
- Multi-objective optimization

#### Benefits of PTX Loss

**1. Prevents capability degradation**
- Maintains language modeling quality
- Preserves factual knowledge
- Keeps reasoning abilities

**2. Faster convergence**
- Regularizes RL training
- Reduces reward hacking
- More stable optimization

**3. Better generalization**
- Model retains broad capabilities
- Performs well on diverse tasks
- Less overfitting to reward model

**4. Minimal computational overhead**
- Adds ~10-20% to training time
- Requires pre-training data sampling
- Worth the cost for quality preservation

#### Common Pitfalls

**Pitfall 1**: Beta too low
- Symptom: Model forgets basic capabilities
- Solution: Increase $\beta_{\text{ptx}}$ to 0.1-0.2

**Pitfall 2**: Beta too high
- Symptom: Slow RL improvement, reward plateaus
- Solution: Decrease $\beta_{\text{ptx}}$ to 0.05-0.1

**Pitfall 3**: Poor pre-training data
- Symptom: Model learns wrong behaviors
- Solution: Curate high-quality, diverse examples

**Pitfall 4**: Mismatched batch sizes
- Symptom: Unstable training
- Solution: Match pre-training batch size to RL batch

#### Comparison with Alternatives

**PTX Loss** (This approach):
- Pros: Simple, effective, theoretically grounded
- Cons: Requires pre-training data, adds computation
- Use: General RLHF pipelines

**Elastic Weight Consolidation (EWC)**:
- Pros: No extra data needed
- Cons: More complex, slower
- Use: Continual learning scenarios

**KL Penalty** (from reference model):
- Pros: Prevents distribution shift
- Cons: Doesn't explicitly preserve capabilities
- Use: Often combined with PTX

**Mixture of Experts**:
- Pros: Can specialize different experts
- Cons: Much more complex architecture
- Use: Very large scale models

#### Mathematical Insight

PTX loss creates a regularization term:

$$
\min_\theta \mathbb{E}_{\pi_\theta} [R(x,y)] - \beta_{\text{ptx}} \cdot D_{\text{KL}}(\pi_\theta \| \pi_{\text{pretrain}})
$$

This can be viewed as:
- Maximizing reward (RL objective)
- While staying close to pre-training distribution (regularization)
- Balanced by $\beta_{\text{ptx}}$

**Intuition**: Don't drift too far from what you knew before while learning new objectives.

#### Implementation Notes

**Return values**:
- **total_loss**: Used for backpropagation
- **ce_loss**: Monitor forgetting (should stay low)
- **weighted_ce_loss**: PTX contribution to total

**Monitoring**:
- Track CE loss separately
- If CE loss increases \  model forgetting
- If RL loss doesn't decrease \  beta too high

**Hyperparameter tuning**:
- Start with $\beta_{\text{ptx}} = 0.1$
- Increase if forgetting observed
- Decrease if RL improvement too slow

#### Summary

**PTX Loss formula**:
$$
L_{\text{total}} = L_{\text{RL}} + \beta_{\text{ptx}} \cdot L_{\text{CE}}
$$

**Purpose**: Prevent catastrophic forgetting in RLHF

**Key applications**: InstructGPT, ChatGPT, Claude, Kimi K2, and other RLHF systems

**Typical beta**: 0.1 (tune between 0.05-0.2)

**Essential for**: Maintaining general capabilities while fine-tuning with RL",
  "starter_code": "import numpy as np\n\ndef compute_ptx_loss(\n    rl_loss: float,\n    pretrain_logits: np.ndarray,\n    pretrain_labels: np.ndarray,\n    beta_ptx: float = 0.1\n) -> tuple[float, float, float]:\n\t\"\"\"\n\tCompute PTX (Pre-training) Loss to prevent catastrophic forgetting in RLHF.\n\t\n\tPTX Loss = RL Loss + beta_ptx * Cross-Entropy Loss\n\t\n\tPrevents model from forgetting general capabilities while\n\tfine-tuning with reinforcement learning from human feedback.\n\t\n\tArgs:\n\t\trl_loss: Reinforcement learning loss (e.g., PPO objective)\n\t\tpretrain_logits: Model logits on pre-training batch\n\t\t  Shape: (batch_size, vocab_size)\n\t\tpretrain_labels: True token indices\n\t\t  Shape: (batch_size,)\n\t\tbeta_ptx: Weight coefficient (typically 0.05-0.2)\n\t\n\tReturns:\n\t\tTuple of (total_loss, ce_loss, weighted_ce_loss):\n\t\t- total_loss: L_RL + beta_ptx * L_CE\n\t\t- ce_loss: Cross-entropy on pre-training data\n\t\t- weighted_ce_loss: beta_ptx * L_CE\n\t\"\"\"\n\t# Your code here\n\tpass",
  "description_decoded": "Implement PTX (Pre-training) Loss to prevent catastrophic forgetting during RLHF. PTX Loss combines reinforcement learning objectives with cross-entropy loss on pre-training data, maintaining general language capabilities while optimizing for reward. Given RL loss, model logits on pre-training batch, true labels, and beta coefficient, compute total loss. Used in InstructGPT, ChatGPT, Kimi K2, and other RLHF systems.",
  "learn_section_decoded": "### PTX (Pre-training) Loss for RLHF\n\nPTX (Pre-training) Loss prevents catastrophic forgetting during reinforcement learning from human feedback (RLHF) by combining RL objectives with language modeling on pre-training data.\n\n#### The Catastrophic Forgetting Problem\n\nWhen fine-tuning language models with reinforcement learning:\n\n**Without PTX Loss**:\n- Model optimizes purely for reward signal\n- Forgets basic language capabilities (grammar, spelling, factual knowledge)\n- Loses general intelligence while maximizing specific reward\n- Results in degraded overall performance\n\n**Example**: A chatbot trained only with RL might:\n- Forget how to spell common words\n- Lose factual knowledge\n- Generate grammatically incorrect sentences\n- Only produce responses that game the reward model\n\n#### PTX Loss Formula\n\n$$\nL_{\\text{total}} = L_{\\text{RL}}(\\theta) + \\beta_{\\text{ptx}} \\cdot L_{\\text{CE}}(\\theta, D_{\\text{pretrain}})\n$$\n\nWhere:\n- $L_{\\text{RL}}(\\theta)$ = Reinforcement learning loss (e.g., PPO objective)\n- $L_{\\text{CE}}(\\theta, D_{\\text{pretrain}})$ = Cross-entropy loss on pre-training data\n- $\\beta_{\\text{ptx}}$ = Weight coefficient (typically 0.05-0.2)\n- $\\theta$ = Model parameters\n\n**Key insight**: Maintains language modeling ability while optimizing for reward.\n\n#### Cross-Entropy Loss Component\n\nThe cross-entropy loss on pre-training data:\n\n$$\nL_{\\text{CE}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log P(y_i | x_i; \\theta)\n$$\n\n**Computation**:\n\n**Step 1**: Apply softmax to logits\n$$\nP(y_j) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}\n$$\n\n**Step 2**: Compute negative log-likelihood\n$$\n\\text{CE}_i = -\\log P(y_{\\text{true},i})\n$$\n\n**Step 3**: Average over batch\n$$\nL_{\\text{CE}} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{CE}_i\n$$\n\n#### Example Calculation\n\n**Scenario**: Model with perfect predictions on pre-training data\n\n**Given**:\n- RL loss: 0.5\n- Logits: [[10, 0, 0], [0, 10, 0]] (very confident)\n- True labels: [0, 1] (correct)\n- Beta_ptx: 0.1\n\n**Step 1**: Compute probabilities\n- Sample 1: Softmax([10, 0, 0])  [1.0, 0.0, 0.0]\n- Sample 2: Softmax([0, 10, 0])  [0.0, 1.0, 0.0]\n\n**Step 2**: Compute CE\n- Sample 1: -log(1.0)  0\n- Sample 2: -log(1.0)  0\n- Average: 0\n\n**Step 3**: Total loss\n$$\nL_{\\text{total}} = 0.5 + 0.1 \\times 0 = 0.5\n$$\n\nPerfect predictions  minimal PTX penalty\n\n#### Effect of Prediction Quality\n\n**High confidence, correct** (logits [10, 0, 0], label 0):\n- Probability: [~1.0, ~0.0, ~0.0]\n- CE: -log(1.0)  0\n- Low penalty (good!)\n\n**Uniform distribution** (logits [1, 1, 1], label 0):\n- Probability: [1/3, 1/3, 1/3]\n- CE: -log(1/3)  1.099\n- Medium penalty\n\n**High confidence, wrong** (logits [0, 10], label 0):\n- Probability: [~0.0, ~1.0]\n- CE: -log(0.0) \\  very large\n- High penalty (prevents forgetting!)\n\n#### Choosing Beta_ptx\n\nThe coefficient $\\beta_{\\text{ptx}}$ controls the trade-off:\n\n**Beta = 0.0**: No forgetting prevention\n- Pure RL optimization\n- Fast reward improvement\n- Risk of catastrophic forgetting\n\n**Beta = 0.05-0.1**: Light preservation (typical)\n- Mostly RL-driven\n- Some capability maintenance\n- Good balance for most tasks\n\n**Beta = 0.2-0.5**: Strong preservation\n- Equal importance to capabilities\n- Slower RL improvement\n- Better for complex models\n\n**Beta = 1.0**: Equal weighting\n- Balanced RL and pre-training\n- Very conservative\n- Slowest forgetting\n\n**Typical choice**: $\\beta_{\\text{ptx}} = 0.1$ (used in InstructGPT, Kimi K2)\n\n#### RLHF Training Pipeline\n\n**Phase 1: Supervised Fine-tuning (SFT)**\n- Train on high-quality demonstrations\n- Loss: Pure cross-entropy on supervised data\n- Result: Model that imitates demonstrations\n\n**Phase 2: Reward Modeling (RM)**\n- Train reward model on human preferences\n- Loss: Binary classification (preferred vs not)\n- Result: Reward function that captures human values\n\n**Phase 3: RL Fine-tuning with PTX**\n- Optimize policy using reward model\n- Loss: $L_{\\text{total}} = L_{\\text{RL}} + \\beta_{\\text{ptx}} \\cdot L_{\\text{CE}}$\n- Result: Model that maximizes reward while preserving capabilities\n\n**Training step**:\n1. Generate responses with current policy\n2. Compute reward from reward model\n3. Compute RL loss (PPO clipped objective)\n4. Sample batch from pre-training data\n5. Compute CE loss on pre-training batch\n6. Combine: $L_{\\text{total}} = L_{\\text{RL}} + \\beta_{\\text{ptx}} \\cdot L_{\\text{CE}}$\n7. Backpropagate and update model\n\n#### Pre-training Data Selection\n\n**What to include**:\n- Representative examples of desired capabilities\n- High-quality, diverse text\n- Tasks model should maintain (QA, summarization, reasoning)\n- Mix of domains and styles\n\n**Batch size**: 32-256 samples per RL batch\n\n**Data mixing**: Typically sample from same distribution as original pre-training\n\n#### Real-World Examples\n\n**InstructGPT (OpenAI, 2022)**:\n- Uses PTX loss with $\\beta_{\\text{ptx}} \\approx 0.1$\n- Pre-training data: Mix of prompts and completions\n- Result: Maintains helpfulness while optimizing for human preference\n\n**Kimi K2 (Moonshot AI, 2024)**:\n- Explicit PTX loss formulation\n- Prevents forgetting during long-context fine-tuning\n- Critical for maintaining general capabilities\n\n**Claude (Anthropic)**:\n- Uses auxiliary objectives (similar to PTX)\n- Maintains helpfulness, harmlessness, honesty (HHH)\n- Multi-objective optimization\n\n#### Benefits of PTX Loss\n\n**1. Prevents capability degradation**\n- Maintains language modeling quality\n- Preserves factual knowledge\n- Keeps reasoning abilities\n\n**2. Faster convergence**\n- Regularizes RL training\n- Reduces reward hacking\n- More stable optimization\n\n**3. Better generalization**\n- Model retains broad capabilities\n- Performs well on diverse tasks\n- Less overfitting to reward model\n\n**4. Minimal computational overhead**\n- Adds ~10-20% to training time\n- Requires pre-training data sampling\n- Worth the cost for quality preservation\n\n#### Common Pitfalls\n\n**Pitfall 1**: Beta too low\n- Symptom: Model forgets basic capabilities\n- Solution: Increase $\\beta_{\\text{ptx}}$ to 0.1-0.2\n\n**Pitfall 2**: Beta too high\n- Symptom: Slow RL improvement, reward plateaus\n- Solution: Decrease $\\beta_{\\text{ptx}}$ to 0.05-0.1\n\n**Pitfall 3**: Poor pre-training data\n- Symptom: Model learns wrong behaviors\n- Solution: Curate high-quality, diverse examples\n\n**Pitfall 4**: Mismatched batch sizes\n- Symptom: Unstable training\n- Solution: Match pre-training batch size to RL batch\n\n#### Comparison with Alternatives\n\n**PTX Loss** (This approach):\n- Pros: Simple, effective, theoretically grounded\n- Cons: Requires pre-training data, adds computation\n- Use: General RLHF pipelines\n\n**Elastic Weight Consolidation (EWC)**:\n- Pros: No extra data needed\n- Cons: More complex, slower\n- Use: Continual learning scenarios\n\n**KL Penalty** (from reference model):\n- Pros: Prevents distribution shift\n- Cons: Doesn't explicitly preserve capabilities\n- Use: Often combined with PTX\n\n**Mixture of Experts**:\n- Pros: Can specialize different experts\n- Cons: Much more complex architecture\n- Use: Very large scale models\n\n#### Mathematical Insight\n\nPTX loss creates a regularization term:\n\n$$\n\\min_\\theta \\mathbb{E}_{\\pi_\\theta} [R(x,y)] - \\beta_{\\text{ptx}} \\cdot D_{\\text{KL}}(\\pi_\\theta \\| \\pi_{\\text{pretrain}})\n$$\n\nThis can be viewed as:\n- Maximizing reward (RL objective)\n- While staying close to pre-training distribution (regularization)\n- Balanced by $\\beta_{\\text{ptx}}$\n\n**Intuition**: Don't drift too far from what you knew before while learning new objectives.\n\n#### Implementation Notes\n\n**Return values**:\n- **total_loss**: Used for backpropagation\n- **ce_loss**: Monitor forgetting (should stay low)\n- **weighted_ce_loss**: PTX contribution to total\n\n**Monitoring**:\n- Track CE loss separately\n- If CE loss increases \\  model forgetting\n- If RL loss doesn't decrease \\  beta too high\n\n**Hyperparameter tuning**:\n- Start with $\\beta_{\\text{ptx}} = 0.1$\n- Increase if forgetting observed\n- Decrease if RL improvement too slow\n\n#### Summary\n\n**PTX Loss formula**:\n$$\nL_{\\text{total}} = L_{\\text{RL}} + \\beta_{\\text{ptx}} \\cdot L_{\\text{CE}}\n$$\n\n**Purpose**: Prevent catastrophic forgetting in RLHF\n\n**Key applications**: InstructGPT, ChatGPT, Claude, Kimi K2, and other RLHF systems\n\n**Typical beta**: 0.1 (tune between 0.05-0.2)\n\n**Essential for**: Maintaining general capabilities while fine-tuning with RL"
}