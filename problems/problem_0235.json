{
  "description": "SW1wbGVtZW50IGEgc2luZ2xlIG9wdGltaXphdGlvbiBzdGVwIGZvciBTZWxlY3RpdmUgR3JhZGllbnQgTWFza2luZyAoU0dUTSksIGEgdGVjaG5pcXVlIGZvciBsb2NhbGl6aW5nIGtub3dsZWRnZSBpbiBuZXVyYWwgbmV0d29ya3MuIEluIFNHVE0sIHBhcmFtZXRlcnMgYXJlIHNwbGl0IGludG8gJ2ZvcmdldCcgYW5kICdyZXRhaW4nIGdyb3VwcyB2aWEgYSBiaW5hcnkgbWFzay4gRHVyaW5nIHRyYWluaW5nOiAoMSkgZm9yICdmb3JnZXQnIGJhdGNoZXMgKGRhbmdlcm91cyBjb250ZW50KSwgb25seSBmb3JnZXQgcGFyYW1ldGVycyBzaG91bGQgYmUgdXBkYXRlZDsgKDIpIGZvciAncmV0YWluJyBiYXRjaGVzIChzYWZlIGNvbnRlbnQpLCBvbmx5IHJldGFpbiBwYXJhbWV0ZXJzIHNob3VsZCBiZSB1cGRhdGVkOyAoMykgZm9yICd1bmxhYmVsZWQnIGJhdGNoZXMsIGFsbCBwYXJhbWV0ZXJzIGFyZSB1cGRhdGVkIG5vcm1hbGx5LiBZb3Ugd2lsbCBpbXBsZW1lbnQgc2d0bV9zdGVwLCB3aGljaCBhcHBsaWVzIGEgZ3JhZGllbnQgdXBkYXRlIHRvIGEgMUQgcGFyYW1ldGVyIHZlY3RvciB1c2luZyBhIG1hc2su",
  "id": "235",
  "test_cases": [
    {
      "test": "import numpy as np\n\nparams = np.array([1.0, 1.0, 1.0, 1.0])\ngrad = np.array([0.1, 0.2, 0.3, 0.4])\nforget_mask = np.array([1, 1, 0, 0])\n\nnew_params = sgtm_step(params, grad, forget_mask, lr=0.1, batch_type=\"forget\")\n\nprint(np.round(new_params, 4).tolist())",
      "expected_output": "[0.99, 0.98, 1.0, 1.0]"
    },
    {
      "test": "import numpy as np\n\nparams = np.array([1.0, 1.0, 1.0, 1.0])\ngrad = np.array([0.1, 0.2, 0.3, 0.4])\nforget_mask = np.array([1, 1, 0, 0])\n\nnew_params = sgtm_step(params, grad, forget_mask, lr=0.1, batch_type=\"retain\")\n\nprint(np.round(new_params, 4).tolist())",
      "expected_output": "[1.0, 1.0, 0.97, 0.96]"
    }
  ],
  "difficulty": "medium",
  "pytorch_difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "params = [1.0, 1.0, 1.0, 1.0], grad = [0.1, 0.2, 0.3, 0.4], forget_mask = [1, 1, 0, 0], lr = 0.1, batch_type = 'forget'",
    "output": "new_params = [0.99, 0.98, 1.0, 1.0]",
    "reasoning": "The forget_mask = [1, 1, 0, 0] means the first two parameters are 'forget' and the last two are 'retain'. For a 'forget' batch, only forget parameters should update. So we mask gradients to [0.1, 0.2, 0.0, 0.0] before the update. With lr = 0.1, the new parameters are [1 - 0.1*0.1, 1 - 0.1*0.2, 1, 1] = [0.99, 0.98, 1.0, 1.0]."
  },
  "category": "Deep Learning",
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgoKZGVmIHNndG1fc3RlcCgKICAgIHBhcmFtczogdG9yY2guVGVuc29yLAogICAgZ3JhZDogdG9yY2guVGVuc29yLAogICAgZm9yZ2V0X21hc2s6IHRvcmNoLlRlbnNvciwKICAgIGxyOiBmbG9hdCA9IDAuMSwKICAgIGJhdGNoX3R5cGU6IHN0ciA9ICJmb3JnZXQiLAopIC0+IHRvcmNoLlRlbnNvcjoKICAgICIiIlBlcmZvcm0gb25lIFNHVE0gdXBkYXRlIHN0ZXAgb24gYSAxRCBwYXJhbWV0ZXIgdGVuc29yLgoKICAgIFNlZSB0aGUgTnVtUHkgdmVyc2lvbiBmb3IgZnVsbCBkZXRhaWxzLgoKICAgIEFyZ3M6CiAgICAgICAgcGFyYW1zOiBDdXJyZW50IHBhcmFtZXRlcnMsIHNoYXBlIChkLCkKICAgICAgICBncmFkOiBHcmFkaWVudCBmb3IgdGhpcyBiYXRjaCwgc2hhcGUgKGQsKQogICAgICAgIGZvcmdldF9tYXNrOiBNYXNrIGZvciBmb3JnZXQgcGFyYW1ldGVycywgc2hhcGUgKGQsKQogICAgICAgIGxyOiBMZWFybmluZyByYXRlCiAgICAgICAgYmF0Y2hfdHlwZTogT25lIG9mIHsnZm9yZ2V0JywgJ3JldGFpbicsICd1bmxhYmVsZWQnfQoKICAgIFJldHVybnM6CiAgICAgICAgbmV3X3BhcmFtczogVXBkYXRlZCBwYXJhbWV0ZXJzIGFzIGEgdGVuc29yCiAgICAiIiIKICAgICMgWW91ciBpbXBsZW1lbnRhdGlvbiBoZXJlIChtaXJyb3IgdGhlIE51bVB5IHZlcnNpb24gdXNpbmcgdG9yY2ggb3BzKS4KICAgIHBhc3M=",
  "title": "Implement the SGTM Parameter Update Step",
  "starter_code": "import numpy as np\n\n\ndef sgtm_step(\n    params: np.ndarray,\n    grad: np.ndarray,\n    forget_mask: np.ndarray,\n    lr: float = 0.1,\n    batch_type: str = \"forget\",\n) -> np.ndarray:\n    \"\"\"Perform one SGTM update step on a 1D parameter vector.\n\n    SGTM splits parameters into 'forget' and 'retain' groups using a binary mask.\n    Depending on the batch type, only some parameters are allowed to update.\n\n    Rules:\n    - 'forget' batch: only forget parameters update (retain params get zeroed gradients).\n    - 'retain' batch: only retain parameters update (forget params get zeroed gradients).\n    - 'unlabeled' batch: all parameters update normally.\n\n    Any nonzero entry in forget_mask should be treated as 1 (forget), and zero\n    means retain.\n\n    Args:\n        params: Current parameters, shape (d,)\n        grad: Gradient for this batch, shape (d,)\n        forget_mask: Mask for forget parameters, shape (d,)\n        lr: Learning rate\n        batch_type: One of {'forget', 'retain', 'unlabeled'}\n\n    Returns:\n        new_params: Updated parameters\n    \"\"\"\n\n    pass",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "import torch\n\nparams = torch.tensor([1.0, 1.0, 1.0, 1.0])\ngrad = torch.tensor([0.1, 0.2, 0.3, 0.4])\nforget_mask = torch.tensor([1, 1, 0, 0])\n\nnew_params = sgtm_step(params, grad, forget_mask, lr=0.1, batch_type=\"forget\")\n\nprint([round(v.item(), 4) for v in new_params])",
      "expected_output": "[0.99, 0.98, 1.0, 1.0]"
    },
    {
      "test": "import torch\n\nparams = torch.tensor([1.0, 1.0, 1.0, 1.0])\ngrad = torch.tensor([0.1, 0.2, 0.3, 0.4])\nforget_mask = torch.tensor([1, 1, 0, 0])\n\nnew_params = sgtm_step(params, grad, forget_mask, lr=0.1, batch_type=\"retain\")\n\nprint([round(v.item(), 4) for v in new_params])",
      "expected_output": "[1.0, 1.0, 0.97, 0.96]"
    }
  ],
  "createdAt": "December 12, 2025 at 9:14:01â€¯AM UTC-0500",
  "learn_section": "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",
  "description_decoded": "Implement a single optimization step for Selective Gradient Masking (SGTM), a technique for localizing knowledge in neural networks. In SGTM, parameters are split into 'forget' and 'retain' groups via a binary mask. During training: (1) for 'forget' batches (dangerous content), only forget parameters should be updated; (2) for 'retain' batches (safe content), only retain parameters should be updated; (3) for 'unlabeled' batches, all parameters are updated normally. You will implement sgtm_step, which applies a gradient update to a 1D parameter vector using a mask.",
  "learn_section_decoded": "## Selective Gradient Masking (SGTM)\n\nSelective Gradient Masking (SGTM) is a technique for localizing specific knowledge into designated model parameters during training, allowing that knowledge to be cleanly removed afterward.\n\n### Core Idea\n\nInstead of filtering training data, SGTM controls **which parameters are updated** based on the type of data being processed. Parameters are split into two groups:\n\n- **Forget parameters** ($\\theta_{forget}$): Store target knowledge (e.g., dangerous capabilities)\n- **Retain parameters** ($\\theta_{retain}$): Store general, useful knowledge\n\nThis split is encoded by a binary mask $m \\in \\{0,1\\}^d$ where $m_i = 1$ means parameter $i$ is a forget parameter.\n\n### Masked Gradient Update\n\nStandard gradient descent updates all parameters:\n$$\\theta_{new} = \\theta - \\eta \\cdot g$$\n\nSGTM instead applies a **masked gradient** based on the batch type:\n$$\\theta_{new} = \\theta - \\eta \\cdot (g \\odot m_{batch})$$\n\nwhere $\\odot$ denotes elementwise multiplication. The mask $m_{batch}$ depends on the data type:\n\n- **Forget batch**: $m_{batch} = m$ - only forget parameters update\n- **Retain batch**: $m_{batch} = 1 - m$ - only retain parameters update\n- **Unlabeled batch**: $m_{batch} = 1$ - all parameters update normally\n\n### Why This Works\n\n1. **Knowledge localization**: Forget-type data can only modify $\\theta_{forget}$, so dangerous knowledge becomes concentrated there.\n\n2. **Clean removal**: After training, set $\\theta_{forget} = 0$ to remove the localized knowledge while preserving general capabilities in $\\theta_{retain}$.\n\n3. **Robustness**: Even mislabeled data tends to naturally update the \"correct\" parameters due to self-reinforcing localization.\n\nIn this problem, you implement a single SGTM update step: given parameters $\\theta$, gradient $g$, forget mask $m$, learning rate $\\eta$, and batch type, compute $\\theta_{new}$."
}