{
  "description": "SW1wbGVtZW50IHRoZSBmb3J3YXJkIHBhc3Mgb2YgYSAzRCBjb252b2x1dGlvbmFsIGxheWVyLiBBIDNEIGNvbnZvbHV0aW9uIHNsaWRlcyBhIDNEIGtlcm5lbCBvdmVyIGEgM0QgaW5wdXQgdm9sdW1lIChsaWtlIHZpZGVvIGZyYW1lcyBvciBtZWRpY2FsIHNjYW5zKSwgY29tcHV0aW5nIGRvdCBwcm9kdWN0cyBhdCBlYWNoIHBvc2l0aW9uLiBHaXZlbiBhbiBpbnB1dCB2b2x1bWUsIGtlcm5lbCwgc3RyaWRlLCBhbmQgcGFkZGluZywgY29tcHV0ZSB0aGUgb3V0cHV0IGZlYXR1cmUgbWFwLiBUaGlzIGlzIHRoZSBjb3JlIG9wZXJhdGlvbiBpbiAzRCBDTk5zIHVzZWQgZm9yIHZpZGVvIGFuYWx5c2lzIGFuZCBtZWRpY2FsIGltYWdpbmcu",
  "id": "230",
  "test_cases": [
    {
      "test": "input_vol = np.array([[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]]); kernel = np.array([[[[1, 0], [0, 0]], [[0, 0], [0, 0]]]]); result = conv3d_forward_pass(input_vol, kernel); print(result.tolist())",
      "expected_output": "[[[[1.0]]]]"
    },
    {
      "test": "input_vol = np.ones((1, 3, 3, 3)); kernel = np.ones((1, 2, 2, 2)); result = conv3d_forward_pass(input_vol, kernel); print(result.tolist())",
      "expected_output": "[[[[8.0, 8.0], [8.0, 8.0]], [[8.0, 8.0], [8.0, 8.0]]]]"
    }
  ],
  "difficulty": "hard",
  "pytorch_difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "input=(1,2,2,2) values [[[1,2],[3,4]],[[5,6],[7,8]]], kernel=(1,2,2,2) with [1,0,0,0,0,0,0,0]",
    "output": "[[[[1.0]]]]",
    "reasoning": "Kernel extracts top-left-front value. Dot product: 1*1+2*0+3*0+4*0+5*0+6*0+7*0+8*0=1. Output: single value 1.0 at position (0,0,0)."
  },
  "category": "Deep Learning",
  "starter_code": "import numpy as np\n\ndef conv3d_forward_pass(\n    input_volume: np.ndarray,\n    kernel: np.ndarray,\n    stride: tuple[int, int, int] = (1, 1, 1),\n    padding: tuple[int, int, int] = (0, 0, 0)\n) -> np.ndarray:\n\t\"\"\"\n\tPerform 3D convolution forward pass.\n\t\n\tSlide a 3D kernel over input volume, computing dot products.\n\t\n\tArgs:\n\t\tinput_volume: Shape (C, D, H, W)\n\t\t  C = channels, D = depth/time, H = height, W = width\n\t\tkernel: Shape (C, kD, kH, kW)\n\t\t  Must match input channels\n\t\tstride: (stride_d, stride_h, stride_w)\n\t\t  Step size in each dimension\n\t\tpadding: (pad_d, pad_h, pad_w)\n\t\t  Zero-padding in each dimension\n\t\n\tReturns:\n\t\tOutput volume: Shape (1, D_out, H_out, W_out)\n\t\t  Single output channel\n\t\t\n\tProcess:\n\t\t1. Apply padding to input\n\t\t2. Calculate output dimensions\n\t\t3. For each output position:\n\t\t   - Extract 3D patch from input\n\t\t   - Compute element-wise product with kernel\n\t\t   - Sum all products -> single output value\n\t\"\"\"\n\t# Your code here\n\tpass",
  "createdAt": "December 10, 2025 at 7:37:21 PM UTC-0500",
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgoKZGVmIGNvbnYzZF9mb3J3YXJkX3Bhc3NfcHl0b3JjaCgKICAgIGlucHV0X3ZvbHVtZTogdG9yY2guVGVuc29yLAogICAga2VybmVsX3dlaWdodDogdG9yY2guVGVuc29yLAogICAgc3RyaWRlOiB0dXBsZVtpbnQsIGludCwgaW50XSA9ICgxLCAxLCAxKSwKICAgIHBhZGRpbmc6IHR1cGxlW2ludCwgaW50LCBpbnRdID0gKDAsIDAsIDApCikgLT4gdG9yY2guVGVuc29yOgoJIiIiCglQZXJmb3JtIDNEIGNvbnZvbHV0aW9uIGZvcndhcmQgcGFzcyB1c2luZyBQeVRvcmNoLgoJCglVc2UgUHlUb3JjaCdzIG5uLkNvbnYzZCBsYXllciB0byBwZXJmb3JtIDNEIGNvbnZvbHV0aW9uLgoJTm90ZTogUHlUb3JjaCBleHBlY3RzIGlucHV0IHNoYXBlIChOLCBDLCBELCBILCBXKSB3aGVyZSBOIGlzIGJhdGNoIHNpemUuCgkKCUFyZ3M6CgkJaW5wdXRfdm9sdW1lOiBTaGFwZSAoQywgRCwgSCwgVykKCQlrZXJuZWxfd2VpZ2h0OiBTaGFwZSAoMSwgQywga0QsIGtILCBrVykgCgkJICBGaXJzdCBkaW0gaXMgb3V0cHV0IGNoYW5uZWxzICgxIGZvciBzaW1wbGlmaWVkIHZlcnNpb24pCgkJc3RyaWRlOiAoc3RyaWRlX2QsIHN0cmlkZV9oLCBzdHJpZGVfdykKCQlwYWRkaW5nOiAocGFkX2QsIHBhZF9oLCBwYWRfdykKCQoJUmV0dXJuczoKCQlPdXRwdXQgdGVuc29yOiBTaGFwZSAoMSwgRF9vdXQsIEhfb3V0LCBXX291dCkKCQkKCUhpbnRzOgoJCS0gQWRkIGJhdGNoIGRpbWVuc2lvbiB0byBpbnB1dDogaW5wdXQudW5zcXVlZXplKDApCgkJLSBDcmVhdGUgQ29udjNkIGxheWVyIHdpdGgga2VybmVsIHNpemUgZnJvbSBrZXJuZWxfd2VpZ2h0CgkJLSBTZXQgY29udi53ZWlnaHQgcGFyYW1ldGVyIHRvIGtlcm5lbF93ZWlnaHQKCQktIFNldCBjb252LmJpYXMgdG8gTm9uZSBvciB6ZXJvcwoJCS0gQXBwbHkgY29udm9sdXRpb24gYW5kIHJlbW92ZSBiYXRjaCBkaW1lbnNpb24KCSIiIgoJIyBZb3VyIGNvZGUgaGVyZQoJcGFzcw==",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "input_vol = torch.tensor([[[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]]]); kernel = torch.tensor([[[[1., 0.], [0., 0.]], [[0., 0.], [0., 0.]]]]); result = conv3d_forward_pass_pytorch(input_vol, kernel); print(result.tolist())",
      "expected_output": "[[[1.0]]]"
    },
    {
      "test": "input_vol = torch.ones(1, 3, 3, 3); kernel = torch.ones(1, 1, 2, 2, 2); result = conv3d_forward_pass_pytorch(input_vol, kernel); print(result.tolist())",
      "expected_output": "[[[8.0, 8.0], [8.0, 8.0]], [[8.0, 8.0], [8.0, 8.0]]]"
    },
    {
      "test": "input_vol = torch.ones(1, 2, 2, 2); kernel = torch.ones(1, 1, 2, 2, 2); result = conv3d_forward_pass_pytorch(input_vol, kernel, padding=(1,1,1)); print(result.shape)",
      "expected_output": "torch.Size([1, 3, 3, 3])"
    },
    {
      "test": "torch.manual_seed(42); input_vol = torch.rand(2, 3, 3, 3); kernel = torch.rand(1, 2, 2, 2, 2); result = conv3d_forward_pass_pytorch(input_vol, kernel); print(result.shape)",
      "expected_output": "torch.Size([1, 2, 2, 2])"
    },
    {
      "test": "input_vol = torch.arange(64, dtype=torch.float32).reshape(1, 4, 4, 4); kernel = torch.ones(1, 1, 2, 2, 2); result = conv3d_forward_pass_pytorch(input_vol, kernel, stride=(2,2,2)); print(result.tolist())",
      "expected_output": "[[[84.0, 100.0], [148.0, 164.0]], [[340.0, 356.0], [404.0, 420.0]]]"
    }
  ],
  "title": "3D CNN Forward Pass Implementation",
  "learn_section": "### 3D Convolutional Forward Pass

A 3D convolution slides a 3D kernel over a 3D input volume, computing dot products at each position. This operation is the core of 3D CNNs used in video analysis and medical imaging.

#### What Happens in 3D Convolution?

**The Process**:
1. **Slide**: Move a 3D kernel through the input volume
2. **Extract**: Get a 3D patch at current position
3. **Multiply**: Element-wise multiplication of patch and kernel
4. **Sum**: Add all products → single output value
5. **Repeat**: Move to next position based on stride

**Key insight**: Each output value represents how well the kernel matches the patch at that location.

#### Input and Kernel Shapes

**Input**: (C, D, H, W)
- C = Channels (e.g., 3 for RGB video)
- D = Depth/Time (number of frames or slices)
- H = Height (spatial dimension)
- W = Width (spatial dimension)

**Kernel**: (C, kD, kH, kW)
- Must have same number of channels as input
- Kernel slides over D, H, W dimensions
- Processes all channels simultaneously

**Output**: (1, D_out, H_out, W_out)
- Single output channel (simplified)
- Dimensions depend on stride and padding

#### Simple Example

**Input**: (1, 2, 2, 2) - Single channel, 2×2×2 cube

Input values:
$$
\begin{array}{cc}
\text{Depth 0:} & \text{Depth 1:} \\
\begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix} &
\begin{bmatrix}5 & 6 \\ 7 & 8\end{bmatrix}
\end{array}
$$

**Kernel**: (1, 2, 2, 2) - Picks top-left-front corner

Kernel values:
$$
\begin{array}{cc}
\text{Depth 0:} & \text{Depth 1:} \\
\begin{bmatrix}1 & 0 \\ 0 & 0\end{bmatrix} &
\begin{bmatrix}0 & 0 \\ 0 & 0\end{bmatrix}
\end{array}
$$

**Computation**:

Only one position (kernel covers entire input):

Element-wise multiplication:
$$
1 \times 1 = 1, \quad 2 \times 0 = 0, \quad 3 \times 0 = 0, \quad \ldots
$$

Sum all products:
$$
1 + 0 + 0 + 0 + 0 + 0 + 0 + 0 = 1
$$

**Output**: Single value = 1.0

#### Example with Multiple Positions

**Input**: (1, 3, 3, 3) - All ones, 3×3×3 cube

**Kernel**: (1, 2, 2, 2) - All ones

**Output dimensions**:
- Depth: (3 - 2)/1 + 1 = 2
- Height: (3 - 2)/1 + 1 = 2  
- Width: (3 - 2)/1 + 1 = 2
- Output: (1, 2, 2, 2)

**At position (0,0,0)** - top-left-front corner:

Extract 2×2×2 patch (8 values, all ones)

Kernel has 8 values, all ones

Dot product: $1 \times 1 + 1 \times 1 + \ldots$ (8 times) = 8

**At position (0,0,1)** - shifted right by 1:

Extract next 2×2×2 patch (still all ones)

Dot product: 8

**Result**: All 8 output positions = 8.0

#### Effect of Stride

**Stride** controls how far the kernel moves between positions.

**Stride = (1,1,1)**: Kernel moves 1 step at a time (dense sampling)

**Stride = (2,2,2)**: Kernel moves 2 steps (skips positions, downsamples)

**Example with stride**:

Input: (1, 4, 4, 4) with sequential values 0-63

Kernel: (1, 2, 2, 2) all ones

Stride: (2, 2, 2)

**Output**: (1, 2, 2, 2)
- Only 8 positions evaluated (skipping every other position)
- Each value is sum of 8 input values

**Position (0,0,0)**: Extracts cube [0:2, 0:2, 0:2]
- Values: 0,1,4,5,16,17,20,21
- Sum: 84

**Position (0,0,1)**: Extracts cube [0:2, 0:2, 2:4]
- Values: 2,3,6,7,18,19,22,23
- Sum: 100

Stride=2 causes downsampling by factor of 2 in each dimension.

#### Effect of Padding

**Padding** adds zeros around the input before convolution.

**Without padding**: Output shrinks (borders not fully covered)

**With padding**: Can maintain or increase output size

**Example**:

Input: (1, 2, 2, 2) all ones

Kernel: (1, 2, 2, 2) all ones

Padding: (1, 1, 1) - adds one layer of zeros on all sides

**Padded input**: (1, 4, 4, 4) with center 2×2×2 = ones, rest = zeros

**Output**: (1, 3, 3, 3)

**Corner positions**: Only partial overlap with non-zero values
- Position (0,0,0): Only 1 input value is non-zero → output = 1

**Center position**: Full overlap with ones
- Position (1,1,1): All 8 values are ones → output = 8

Padding affects border values differently than center values.

#### Multi-Channel Processing

When input has multiple channels, the kernel must match:

**Input**: (2, 3, 3, 3) - 2 channels

**Kernel**: (2, 2, 2, 2) - 2 channels

**Process**: 
- Extract 2×2×2 patch from BOTH channels
- Multiply by corresponding kernel channels
- Sum ALL products (across channels and positions)
- Result: Single value per output position

**Mathematical formulation**:
$$
\text{output}[d,h,w] = \sum_{c=1}^{C} \sum_{i,j,k} \text{input}[c, d+i, h+j, w+k] \times \text{kernel}[c, i, j, k]
$$

#### Algorithm Steps

**Step 1**: Determine output dimensions
$$
D_{out} = \lfloor (D + 2p_d - k_d) / s_d \rfloor + 1
$$
$$
H_{out} = \lfloor (H + 2p_h - k_h) / s_h \rfloor + 1
$$
$$
W_{out} = \lfloor (W + 2p_w - k_w) / s_w \rfloor + 1
$$

**Step 2**: Apply padding if specified

Add zeros around input volume

**Step 3**: For each output position (d, h, w):

Calculate starting indices:
- d_start = d × stride_d
- h_start = h × stride_h
- w_start = w × stride_w

Extract patch:
- patch = input[:, d_start:d_start+kD, h_start:h_start+kH, w_start:w_start+kW]

Compute dot product:
- output[d,h,w] = sum(patch * kernel)

**Step 4**: Return output volume

#### Applications in Video Analysis

**Action Recognition**:

Input: (3, 16, 112, 112) - 16-frame RGB video

Kernel: (3, 3, 7, 7) - Captures 3 frames with 7×7 spatial pattern

The 3D kernel detects temporal motion patterns:
- Walking: legs alternating across frames
- Jumping: upward motion across frames
- Waving: hand motion across frames

**Medical CT Scans**:

Input: (1, 64, 256, 256) - 64 CT slices

Kernel: (1, 5, 5, 5) - Detects 3D tumor shape

The 3D kernel recognizes volumetric patterns:
- Spherical tumors across multiple slices
- Irregular tissue boundaries
- 3D anatomical structures

#### Computational Complexity

**Operations per output value**:
$$
C \times k_D \times k_H \times k_W
$$

**Total operations**:
$$
D_{out} \times H_{out} \times W_{out} \times C \times k_D \times k_H \times k_W
$$

**Example**:
- Output: 14 × 62 × 62 = 53,816 positions
- Per position: 3 × 3 × 3 × 3 = 81 operations
- Total: 4,359,096 multiply-adds

3D convolutions are computationally intensive!

#### Memory Access Pattern

**Naive implementation** re-extracts overlapping patches (inefficient)

**Optimized implementation** uses:
- Im2col transformation (reshape to matrix multiplication)
- GPU parallel processing (compute all positions simultaneously)
- Tiled computation (reuse loaded data)

Modern deep learning frameworks (PyTorch, TensorFlow) use highly optimized implementations.

#### Differences from 2D Convolution

| Aspect | 2D Conv | 3D Conv |
|--------|---------|---------|
| Input | (C, H, W) | (C, D, H, W) |
| Kernel | (C, kH, kW) | (C, kD, kH, kW) |
| Slides over | H, W | D, H, W |
| Captures | Spatial | Temporal/Volumetric |
| Use case | Images | Videos, medical scans |

**Key distinction**: 3D conv processes time/depth dimension, not just spatial.

#### Practical Considerations

**Memory**: 3D convolutions use significantly more memory
- Larger input volumes
- Larger intermediate activations
- More parameters in kernel

**Speed**: Slower than 2D convolutions
- More operations per output
- More memory bandwidth required
- Harder to parallelize efficiently

**Batch size**: Use smaller batches (often 1-4) due to memory constraints

**Architecture design**: 
- Start with downsampling layers (stride > 1)
- Use smaller kernel sizes (3×3×3 common)
- Consider (2+1)D factorization (separate spatial and temporal)

#### Summary

**3D Convolution Process**:
1. Slide 3D kernel over 3D input
2. Extract patch at each position
3. Compute element-wise product
4. Sum all products
5. Store result at output position

**Output Formula**: 
$$
\text{out}[d,h,w] = \sum \text{patch} \odot \text{kernel}
$$

**Applications**: Video action recognition, medical imaging, volumetric data

**Challenge**: High computational and memory cost

**Key to implementation**: Correct indexing and boundary handling",
  "description_decoded": "Implement the forward pass of a 3D convolutional layer. A 3D convolution slides a 3D kernel over a 3D input volume (like video frames or medical scans), computing dot products at each position. Given an input volume, kernel, stride, and padding, compute the output feature map. This is the core operation in 3D CNNs used for video analysis and medical imaging.",
  "learn_section_decoded": "### 3D Convolutional Forward Pass\n\nA 3D convolution slides a 3D kernel over a 3D input volume, computing dot products at each position. This operation is the core of 3D CNNs used in video analysis and medical imaging.\n\n#### What Happens in 3D Convolution?\n\n**The Process**:\n1. **Slide**: Move a 3D kernel through the input volume\n2. **Extract**: Get a 3D patch at current position\n3. **Multiply**: Element-wise multiplication of patch and kernel\n4. **Sum**: Add all products → single output value\n5. **Repeat**: Move to next position based on stride\n\n**Key insight**: Each output value represents how well the kernel matches the patch at that location.\n\n#### Input and Kernel Shapes\n\n**Input**: (C, D, H, W)\n- C = Channels (e.g., 3 for RGB video)\n- D = Depth/Time (number of frames or slices)\n- H = Height (spatial dimension)\n- W = Width (spatial dimension)\n\n**Kernel**: (C, kD, kH, kW)\n- Must have same number of channels as input\n- Kernel slides over D, H, W dimensions\n- Processes all channels simultaneously\n\n**Output**: (1, D_out, H_out, W_out)\n- Single output channel (simplified)\n- Dimensions depend on stride and padding\n\n#### Simple Example\n\n**Input**: (1, 2, 2, 2) - Single channel, 2×2×2 cube\n\nInput values:\n$$\n\\begin{array}{cc}\n\\text{Depth 0:} & \\text{Depth 1:} \\\\\n\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix} &\n\\begin{bmatrix}5 & 6 \\\\ 7 & 8\\end{bmatrix}\n\\end{array}\n$$\n\n**Kernel**: (1, 2, 2, 2) - Picks top-left-front corner\n\nKernel values:\n$$\n\\begin{array}{cc}\n\\text{Depth 0:} & \\text{Depth 1:} \\\\\n\\begin{bmatrix}1 & 0 \\\\ 0 & 0\\end{bmatrix} &\n\\begin{bmatrix}0 & 0 \\\\ 0 & 0\\end{bmatrix}\n\\end{array}\n$$\n\n**Computation**:\n\nOnly one position (kernel covers entire input):\n\nElement-wise multiplication:\n$$\n1 \\times 1 = 1, \\quad 2 \\times 0 = 0, \\quad 3 \\times 0 = 0, \\quad \\ldots\n$$\n\nSum all products:\n$$\n1 + 0 + 0 + 0 + 0 + 0 + 0 + 0 = 1\n$$\n\n**Output**: Single value = 1.0\n\n#### Example with Multiple Positions\n\n**Input**: (1, 3, 3, 3) - All ones, 3×3×3 cube\n\n**Kernel**: (1, 2, 2, 2) - All ones\n\n**Output dimensions**:\n- Depth: (3 - 2)/1 + 1 = 2\n- Height: (3 - 2)/1 + 1 = 2  \n- Width: (3 - 2)/1 + 1 = 2\n- Output: (1, 2, 2, 2)\n\n**At position (0,0,0)** - top-left-front corner:\n\nExtract 2×2×2 patch (8 values, all ones)\n\nKernel has 8 values, all ones\n\nDot product: $1 \\times 1 + 1 \\times 1 + \\ldots$ (8 times) = 8\n\n**At position (0,0,1)** - shifted right by 1:\n\nExtract next 2×2×2 patch (still all ones)\n\nDot product: 8\n\n**Result**: All 8 output positions = 8.0\n\n#### Effect of Stride\n\n**Stride** controls how far the kernel moves between positions.\n\n**Stride = (1,1,1)**: Kernel moves 1 step at a time (dense sampling)\n\n**Stride = (2,2,2)**: Kernel moves 2 steps (skips positions, downsamples)\n\n**Example with stride**:\n\nInput: (1, 4, 4, 4) with sequential values 0-63\n\nKernel: (1, 2, 2, 2) all ones\n\nStride: (2, 2, 2)\n\n**Output**: (1, 2, 2, 2)\n- Only 8 positions evaluated (skipping every other position)\n- Each value is sum of 8 input values\n\n**Position (0,0,0)**: Extracts cube [0:2, 0:2, 0:2]\n- Values: 0,1,4,5,16,17,20,21\n- Sum: 84\n\n**Position (0,0,1)**: Extracts cube [0:2, 0:2, 2:4]\n- Values: 2,3,6,7,18,19,22,23\n- Sum: 100\n\nStride=2 causes downsampling by factor of 2 in each dimension.\n\n#### Effect of Padding\n\n**Padding** adds zeros around the input before convolution.\n\n**Without padding**: Output shrinks (borders not fully covered)\n\n**With padding**: Can maintain or increase output size\n\n**Example**:\n\nInput: (1, 2, 2, 2) all ones\n\nKernel: (1, 2, 2, 2) all ones\n\nPadding: (1, 1, 1) - adds one layer of zeros on all sides\n\n**Padded input**: (1, 4, 4, 4) with center 2×2×2 = ones, rest = zeros\n\n**Output**: (1, 3, 3, 3)\n\n**Corner positions**: Only partial overlap with non-zero values\n- Position (0,0,0): Only 1 input value is non-zero → output = 1\n\n**Center position**: Full overlap with ones\n- Position (1,1,1): All 8 values are ones → output = 8\n\nPadding affects border values differently than center values.\n\n#### Multi-Channel Processing\n\nWhen input has multiple channels, the kernel must match:\n\n**Input**: (2, 3, 3, 3) - 2 channels\n\n**Kernel**: (2, 2, 2, 2) - 2 channels\n\n**Process**: \n- Extract 2×2×2 patch from BOTH channels\n- Multiply by corresponding kernel channels\n- Sum ALL products (across channels and positions)\n- Result: Single value per output position\n\n**Mathematical formulation**:\n$$\n\\text{output}[d,h,w] = \\sum_{c=1}^{C} \\sum_{i,j,k} \\text{input}[c, d+i, h+j, w+k] \\times \\text{kernel}[c, i, j, k]\n$$\n\n#### Algorithm Steps\n\n**Step 1**: Determine output dimensions\n$$\nD_{out} = \\lfloor (D + 2p_d - k_d) / s_d \\rfloor + 1\n$$\n$$\nH_{out} = \\lfloor (H + 2p_h - k_h) / s_h \\rfloor + 1\n$$\n$$\nW_{out} = \\lfloor (W + 2p_w - k_w) / s_w \\rfloor + 1\n$$\n\n**Step 2**: Apply padding if specified\n\nAdd zeros around input volume\n\n**Step 3**: For each output position (d, h, w):\n\nCalculate starting indices:\n- d_start = d × stride_d\n- h_start = h × stride_h\n- w_start = w × stride_w\n\nExtract patch:\n- patch = input[:, d_start:d_start+kD, h_start:h_start+kH, w_start:w_start+kW]\n\nCompute dot product:\n- output[d,h,w] = sum(patch * kernel)\n\n**Step 4**: Return output volume\n\n#### Applications in Video Analysis\n\n**Action Recognition**:\n\nInput: (3, 16, 112, 112) - 16-frame RGB video\n\nKernel: (3, 3, 7, 7) - Captures 3 frames with 7×7 spatial pattern\n\nThe 3D kernel detects temporal motion patterns:\n- Walking: legs alternating across frames\n- Jumping: upward motion across frames\n- Waving: hand motion across frames\n\n**Medical CT Scans**:\n\nInput: (1, 64, 256, 256) - 64 CT slices\n\nKernel: (1, 5, 5, 5) - Detects 3D tumor shape\n\nThe 3D kernel recognizes volumetric patterns:\n- Spherical tumors across multiple slices\n- Irregular tissue boundaries\n- 3D anatomical structures\n\n#### Computational Complexity\n\n**Operations per output value**:\n$$\nC \\times k_D \\times k_H \\times k_W\n$$\n\n**Total operations**:\n$$\nD_{out} \\times H_{out} \\times W_{out} \\times C \\times k_D \\times k_H \\times k_W\n$$\n\n**Example**:\n- Output: 14 × 62 × 62 = 53,816 positions\n- Per position: 3 × 3 × 3 × 3 = 81 operations\n- Total: 4,359,096 multiply-adds\n\n3D convolutions are computationally intensive!\n\n#### Memory Access Pattern\n\n**Naive implementation** re-extracts overlapping patches (inefficient)\n\n**Optimized implementation** uses:\n- Im2col transformation (reshape to matrix multiplication)\n- GPU parallel processing (compute all positions simultaneously)\n- Tiled computation (reuse loaded data)\n\nModern deep learning frameworks (PyTorch, TensorFlow) use highly optimized implementations.\n\n#### Differences from 2D Convolution\n\n| Aspect | 2D Conv | 3D Conv |\n|--------|---------|---------|\n| Input | (C, H, W) | (C, D, H, W) |\n| Kernel | (C, kH, kW) | (C, kD, kH, kW) |\n| Slides over | H, W | D, H, W |\n| Captures | Spatial | Temporal/Volumetric |\n| Use case | Images | Videos, medical scans |\n\n**Key distinction**: 3D conv processes time/depth dimension, not just spatial.\n\n#### Practical Considerations\n\n**Memory**: 3D convolutions use significantly more memory\n- Larger input volumes\n- Larger intermediate activations\n- More parameters in kernel\n\n**Speed**: Slower than 2D convolutions\n- More operations per output\n- More memory bandwidth required\n- Harder to parallelize efficiently\n\n**Batch size**: Use smaller batches (often 1-4) due to memory constraints\n\n**Architecture design**: \n- Start with downsampling layers (stride > 1)\n- Use smaller kernel sizes (3×3×3 common)\n- Consider (2+1)D factorization (separate spatial and temporal)\n\n#### Summary\n\n**3D Convolution Process**:\n1. Slide 3D kernel over 3D input\n2. Extract patch at each position\n3. Compute element-wise product\n4. Sum all products\n5. Store result at output position\n\n**Output Formula**: \n$$\n\\text{out}[d,h,w] = \\sum \\text{patch} \\odot \\text{kernel}\n$$\n\n**Applications**: Video action recognition, medical imaging, volumetric data\n\n**Challenge**: High computational and memory cost\n\n**Key to implementation**: Correct indexing and boundary handling"
}