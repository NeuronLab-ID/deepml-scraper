{
  "description": "SW1wbGVtZW50IHRoZSBKZW5zZW4tU2hhbm5vbiBEaXZlcmdlbmNlIChKU0QpIGJldHdlZW4gdHdvIHByb2JhYmlsaXR5IGRpc3RyaWJ1dGlvbnMuIEpTRCBpcyBhIHN5bW1ldHJpYyBhbmQgc21vb3RoZWQgdmVyc2lvbiBvZiB0aGUgS3VsbGJhY2stTGVpYmxlciBkaXZlcmdlbmNlIHRoYXQgbWVhc3VyZXMgdGhlIHNpbWlsYXJpdHkgYmV0d2VlbiB0d28gcHJvYmFiaWxpdHkgZGlzdHJpYnV0aW9ucy4gVW5saWtlIEtMIGRpdmVyZ2VuY2UsIEpTRCBpcyBzeW1tZXRyaWMgKHRoZSBvcmRlciBvZiBQIGFuZCBRIGRvZXNuJ3QgbWF0dGVyKSBhbmQgYWx3YXlzIHByb2R1Y2VzIGEgZmluaXRlIHZhbHVlLiBUaGUgcmVzdWx0IHNob3VsZCBiZSBhIHZhbHVlIGJldHdlZW4gMCAoaWRlbnRpY2FsIGRpc3RyaWJ1dGlvbnMpIGFuZCBsb2coMikgKGNvbXBsZXRlbHkgZGlmZmVyZW50IGRpc3RyaWJ1dGlvbnMpLg==",
  "id": "203",
  "test_cases": [
    {
      "test": "import numpy as np; jsd = jensen_shannon_divergence([0.5, 0.5], [0.5, 0.5]); print(round(jsd, 6))",
      "expected_output": "0.0"
    },
    {
      "test": "import numpy as np; jsd = jensen_shannon_divergence([1.0, 0.0], [0.0, 1.0]); print(round(jsd, 6))",
      "expected_output": "0.693147"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "P = [0.5, 0.5], Q = [0.4, 0.6]",
    "output": "0.005059",
    "reasoning": "First, compute the average distribution $M = 0.5(P + Q) = [0.45, 0.55]$. Then compute $KL(P||M) = 0.5\\log(0.5/0.45) + 0.5\\log(0.5/0.55) \\approx 0.005025$. Similarly, $KL(Q||M) = 0.4\\log(0.4/0.45) + 0.6\\log(0.6/0.55) \\approx 0.005094$. Finally, $JSD = 0.5 \\times 0.005025 + 0.5 \\times 0.005094 \\approx 0.005059$."
  },
  "category": "Information Theory",
  "starter_code": "import numpy as np\n\ndef jensen_shannon_divergence(P: list[float], Q: list[float]) -> float:\n\t\"\"\"\n\tCompute the Jensen-Shannon Divergence between two probability distributions.\n\t\n\tArgs:\n\t\tP: First probability distribution\n\t\tQ: Second probability distribution\n\t\n\tReturns:\n\t\tJensen-Shannon Divergence value\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Jensen-Shannon Divergence",
  "createdAt": "November 15, 2025 at 10:00:38â€¯PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "learn_section": "IyMjIFVuZGVyc3RhbmRpbmcgSmVuc2VuLVNoYW5ub24gRGl2ZXJnZW5jZQoKVGhlIEplbnNlbi1TaGFubm9uIERpdmVyZ2VuY2UgKEpTRCkgaXMgYSBtZXRob2Qgb2YgbWVhc3VyaW5nIHRoZSBzaW1pbGFyaXR5IGJldHdlZW4gdHdvIHByb2JhYmlsaXR5IGRpc3RyaWJ1dGlvbnMuIEl0IGFkZHJlc3NlcyBrZXkgbGltaXRhdGlvbnMgb2YgdGhlIEt1bGxiYWNrLUxlaWJsZXIgKEtMKSBkaXZlcmdlbmNlIGJ5IGJlaW5nIHN5bW1ldHJpYyBhbmQgYWx3YXlzIHByb2R1Y2luZyBhIGZpbml0ZSB2YWx1ZS4KCiMjIyMgTW90aXZhdGlvbjogTGltaXRhdGlvbnMgb2YgS0wgRGl2ZXJnZW5jZQoKVGhlIEtMIGRpdmVyZ2VuY2UgYmV0d2VlbiBkaXN0cmlidXRpb25zICRQJCBhbmQgJFEkIGlzIGRlZmluZWQgYXM6CgokJApEX3tLTH0oUCB8fCBRKSA9IFxzdW1fe2l9IFAoaSkgXGxvZyBcZnJhY3tQKGkpfXtRKGkpfQokJAoKSG93ZXZlciwgS0wgZGl2ZXJnZW5jZSBoYXMgaW1wb3J0YW50IGRyYXdiYWNrczoKCjEuICoqQXN5bW1ldHJ5Kio6ICREX3tLTH0oUCB8fCBRKSBcbmVxIERfe0tMfShRIHx8IFApJAoyLiAqKlVuZGVmaW5lZCB2YWx1ZXMqKjogSWYgJFEoaSkgPSAwJCBidXQgJFAoaSkgPiAwJCwgdGhlIGRpdmVyZ2VuY2UgaXMgaW5maW5pdGUKMy4gKipOb3QgYSB0cnVlIG1ldHJpYyoqOiBEb2Vzbid0IHNhdGlzZnkgdGhlIHRyaWFuZ2xlIGluZXF1YWxpdHkKCkplbnNlbi1TaGFubm9uIERpdmVyZ2VuY2UgcmVzb2x2ZXMgdGhlc2UgaXNzdWVzLgoKIyMjIyBEZWZpbml0aW9uCgpUaGUgSmVuc2VuLVNoYW5ub24gRGl2ZXJnZW5jZSBiZXR3ZWVuIGRpc3RyaWJ1dGlvbnMgJFAkIGFuZCAkUSQgaXM6CgokJApKU0QoUCB8fCBRKSA9IFxmcmFjezF9ezJ9IERfe0tMfShQIHx8IE0pICsgXGZyYWN7MX17Mn0gRF97S0x9KFEgfHwgTSkKJCQKCldoZXJlICRNJCBpcyB0aGUgYXZlcmFnZSAobWlkcG9pbnQpIGRpc3RyaWJ1dGlvbjoKCiQkCk0gPSBcZnJhY3sxfXsyfShQICsgUSkKJCQKCkluIG90aGVyIHdvcmRzLCBKU0QgbWVhc3VyZXMgdGhlIGF2ZXJhZ2UgZGl2ZXJnZW5jZSBvZiAkUCQgYW5kICRRJCBmcm9tIHRoZWlyIGF2ZXJhZ2UgZGlzdHJpYnV0aW9uICRNJC4KCiMjIyMgTWF0aGVtYXRpY2FsIFByb3BlcnRpZXMKCioqU3ltbWV0cnkqKjoKCiQkCkpTRChQIHx8IFEpID0gSlNEKFEgfHwgUCkKJCQKClRoaXMgZm9sbG93cyBmcm9tIHRoZSBzeW1tZXRyaWMgZGVmaW5pdGlvbjogYm90aCAkUCQgYW5kICRRJCBhcmUgdHJlYXRlZCBlcXVhbGx5IGluIHJlbGF0aW9uIHRvIHRoZWlyIGF2ZXJhZ2UuCgoqKkJvdW5kZWQqKjoKCiQkCjAgXGxlcSBKU0QoUCB8fCBRKSBcbGVxIFxsb2coMikKJCQKClRoZSB1cHBlciBib3VuZCAkXGxvZygyKSBcYXBwcm94IDAuNjkzJCBpcyBhY2hpZXZlZCB3aGVuIGRpc3RyaWJ1dGlvbnMgYXJlIGNvbXBsZXRlbHkgZGlmZmVyZW50IChubyBvdmVybGFwKS4gVGhlIGxvd2VyIGJvdW5kICQwJCBpcyBhY2hpZXZlZCB3aGVuIGRpc3RyaWJ1dGlvbnMgYXJlIGlkZW50aWNhbC4KCioqTm9uLW5lZ2F0aXZpdHkgYW5kIElkZW50aXR5Kio6CgokJApKU0QoUCB8fCBRKSA9IDAgXGlmZiBQID0gUQokJAoKVGhpcyBtYWtlcyBKU0Qgc3VpdGFibGUgYXMgYSBkaXN0YW5jZS1saWtlIG1lYXN1cmUuCgoqKlNxdWFyZSBSb290IGlzIGEgTWV0cmljKio6CgokJApkKFAsIFEpID0gXHNxcnR7SlNEKFAgfHwgUSl9CiQkCgpUaGUgc3F1YXJlIHJvb3Qgb2YgSlNEIHNhdGlzZmllcyB0aGUgdHJpYW5nbGUgaW5lcXVhbGl0eSwgbWFraW5nIGl0IGEgdHJ1ZSBtZXRyaWMgKGFsc28gY2FsbGVkIHRoZSBKZW5zZW4tU2hhbm5vbiBkaXN0YW5jZSkuCgojIyMjIERldGFpbGVkIEV4YW1wbGUKCkNvbnNpZGVyIGRpc3RyaWJ1dGlvbnMgJFAgPSBbMC41LCAwLjVdJCBhbmQgJFEgPSBbMC40LCAwLjZdJC4KCioqU3RlcCAxKio6IENvbXB1dGUgdGhlIGF2ZXJhZ2UgZGlzdHJpYnV0aW9uOgoKJCQKTSA9IFxmcmFjezF9ezJ9KFswLjUsIDAuNV0gKyBbMC40LCAwLjZdKSA9IFswLjQ1LCAwLjU1XQokJAoKKipTdGVwIDIqKjogQ29tcHV0ZSAkRF97S0x9KFAgfHwgTSkkOgoKJCQKRF97S0x9KFAgfHwgTSkgPSAwLjUgXGxvZ1xmcmFjezAuNX17MC40NX0gKyAwLjUgXGxvZ1xmcmFjezAuNX17MC41NX0KJCQKCiQkCj0gMC41IFxsb2coMS4xMTEpICsgMC41IFxsb2coMC45MDkpCiQkCgokJAo9IDAuNSgwLjEwNSkgKyAwLjUoLTAuMDk1KSBcYXBwcm94IDAuMDA1MDI1CiQkCgoqKlN0ZXAgMyoqOiBDb21wdXRlICREX3tLTH0oUSB8fCBNKSQ6CgokJApEX3tLTH0oUSB8fCBNKSA9IDAuNCBcbG9nXGZyYWN7MC40fXswLjQ1fSArIDAuNiBcbG9nXGZyYWN7MC42fXswLjU1fQokJAoKJCQKPSAwLjQgXGxvZygwLjg4OSkgKyAwLjYgXGxvZygxLjA5MSkKJCQKCiQkCj0gMC40KC0wLjExOCkgKyAwLjYoMC4wODcpIFxhcHByb3ggMC4wMDUwOTQKJCQKCioqU3RlcCA0Kio6IENvbXB1dGUgSlNEOgoKJCQKSlNEKFAgfHwgUSkgPSBcZnJhY3sxfXsyfSgwLjAwNTAyNSArIDAuMDA1MDk0KSBcYXBwcm94IDAuMDA1MDU5CiQkCgojIyMjIFJlbGF0aW9uc2hpcCB0byBNdXR1YWwgSW5mb3JtYXRpb24KCkpTRCBjYW4gYmUgaW50ZXJwcmV0ZWQgYXMgdGhlIG11dHVhbCBpbmZvcm1hdGlvbiBiZXR3ZWVuIGEgcmFuZG9tIHZhcmlhYmxlICRYJCAod2l0aCBkaXN0cmlidXRpb24gJFAkIG9yICRRJCkgYW5kIGEgYmluYXJ5IHZhcmlhYmxlICRaJCBpbmRpY2F0aW5nIHdoaWNoIGRpc3RyaWJ1dGlvbiB3YXMgY2hvc2VuOgoKJCQKSlNEKFAgfHwgUSkgPSBJKFg7IFopCiQkCgpUaGlzIGluZm9ybWF0aW9uLXRoZW9yZXRpYyBpbnRlcnByZXRhdGlvbiBleHBsYWlucyB3aHkgSlNEIGlzIGJvdW5kZWQgYnkgJFxsb2coMikkOiB3aXRoIHR3byBlcXVpcHJvYmFibGUgY2hvaWNlcywgdGhlIG1heGltdW0gaW5mb3JtYXRpb24gaXMgMSBiaXQuCgojIyMjIENvbXBhcmlzb24gd2l0aCBPdGhlciBEaXZlcmdlbmNlcwoKKipWZXJzdXMgS0wgRGl2ZXJnZW5jZSoqOgotIEtMOiBBc3ltbWV0cmljLCBjYW4gYmUgaW5maW5pdGUKLSBKU0Q6IFN5bW1ldHJpYywgYWx3YXlzIGZpbml0ZSwgYm91bmRlZAoKKipWZXJzdXMgVG90YWwgVmFyaWF0aW9uIERpc3RhbmNlKio6ClRvdGFsIHZhcmlhdGlvbiBkaXN0YW5jZSBpczoKCiQkClRWKFAsIFEpID0gXGZyYWN7MX17Mn1cc3VtX2kgfFAoaSkgLSBRKGkpfAokJAoKUmVsYXRpb25zaGlwOiAkVFYoUCwgUSkgXGxlcSBcc3FydHsyIFxjZG90IEpTRChQIHx8IFEpfSQKCioqVmVyc3VzIEhlbGxpbmdlciBEaXN0YW5jZSoqOgpIZWxsaW5nZXIgZGlzdGFuY2UgaXM6CgokJApIKFAsIFEpID0gXGZyYWN7MX17XHNxcnR7Mn19XHNxcnR7XHN1bV9pIChcc3FydHtQKGkpfSAtIFxzcXJ0e1EoaSl9KV4yfQokJAoKQm90aCBhcmUgYm91bmRlZCBtZXRyaWNzLCBidXQgSlNEIGhhcyBhIGNsZWFyZXIgaW5mb3JtYXRpb24tdGhlb3JldGljIGludGVycHJldGF0aW9uLgoKIyMjIyBQcmFjdGljYWwgQ29uc2lkZXJhdGlvbnMKCioqTnVtZXJpY2FsIFN0YWJpbGl0eSoqOgoKV2hlbiBjb21wdXRpbmcgJFxsb2coUChpKS9RKGkpKSQsIGFkZCBhIHNtYWxsIGVwc2lsb24gdG8gYXZvaWQgZGl2aXNpb24gYnkgemVybzoKCiQkClAnKGkpID0gXG1heChQKGkpLCBcZXBzaWxvbiksIFxxdWFkIFEnKGkpID0gXG1heChRKGkpLCBcZXBzaWxvbikKJCQKClR5cGljYWxseSwgJFxlcHNpbG9uID0gMTBeey0xMH0kIHByb3ZpZGVzIGdvb2Qgc3RhYmlsaXR5LgoKKipDb21wdXRhdGlvbmFsIENvbXBsZXhpdHkqKjoKCkZvciBkaXN0cmlidXRpb25zIHdpdGggJG4kIG91dGNvbWVzOgotIFRpbWU6ICRPKG4pJCAtIHNpbmdsZSBwYXNzIHRocm91Z2ggZGlzdHJpYnV0aW9ucwotIFNwYWNlOiAkTyhuKSQgLSBzdG9yZSB0aGUgYXZlcmFnZSBkaXN0cmlidXRpb24KCiMjIyMgQXBwbGljYXRpb25zCgoqKk1hY2hpbmUgTGVhcm5pbmcqKjogQ29tcGFyaW5nIHByZWRpY3RlZCBwcm9iYWJpbGl0eSBkaXN0cmlidXRpb25zIHdpdGggdHJ1ZSBkaXN0cmlidXRpb25zIGluIGNsYXNzaWZpY2F0aW9uIHRhc2tzLgoKKipHZW5lcmF0aXZlIE1vZGVscyoqOiBFdmFsdWF0aW5nIHRoZSBxdWFsaXR5IG9mIGdlbmVyYXRlZCBzYW1wbGVzIGJ5IGNvbXBhcmluZyB0aGVpciBkaXN0cmlidXRpb24gdG8gcmVhbCBkYXRhLiBVc2VkIGluIGV2YWx1YXRpbmcgR0FOcyAoR2VuZXJhdGl2ZSBBZHZlcnNhcmlhbCBOZXR3b3JrcykuCgoqKk5hdHVyYWwgTGFuZ3VhZ2UgUHJvY2Vzc2luZyoqOiBNZWFzdXJpbmcgc2ltaWxhcml0eSBiZXR3ZWVuIHRvcGljIGRpc3RyaWJ1dGlvbnMgaW4gZG9jdW1lbnQgY2x1c3RlcmluZyBhbmQgdG9waWMgbW9kZWxpbmcuCgoqKkJpb2luZm9ybWF0aWNzKio6IENvbXBhcmluZyBETkEgc2VxdWVuY2UgbW90aWZzIGFuZCBnZW5lIGV4cHJlc3Npb24gcHJvZmlsZXMuCgoqKkNsdXN0ZXJpbmcqKjogQXMgYSBkaXN0YW5jZSBtZXRyaWMgZm9yIGNsdXN0ZXJpbmcgcHJvYmFiaWxpdHkgZGlzdHJpYnV0aW9ucy4gVGhlIHNxdWFyZSByb290IG9mIEpTRCBmb3JtcyBhIHByb3BlciBtZXRyaWMuCgoqKkluZm9ybWF0aW9uIFJldHJpZXZhbCoqOiBNZWFzdXJpbmcgc2ltaWxhcml0eSBiZXR3ZWVuIGRvY3VtZW50IHRlcm0gZGlzdHJpYnV0aW9ucyBmb3IgcmVsZXZhbmNlIHJhbmtpbmcuCgojIyMjIFZhcmlhdGlvbnMKCioqV2VpZ2h0ZWQgSmVuc2VuLVNoYW5ub24gRGl2ZXJnZW5jZSoqOgoKSW5zdGVhZCBvZiBlcXVhbCB3ZWlnaHRzICgwLjUgZWFjaCksIHVzZSBhcmJpdHJhcnkgd2VpZ2h0cyAkXHBpXzEsIFxwaV8yJCB3aGVyZSAkXHBpXzEgKyBccGlfMiA9IDEkOgoKJCQKSlNEX3tccGl9KFAgfHwgUSkgPSBccGlfMSBEX3tLTH0oUCB8fCBNKSArIFxwaV8yIERfe0tMfShRIHx8IE0pCiQkCgpXaGVyZSAkTSA9IFxwaV8xIFAgKyBccGlfMiBRJC4KCioqTXVsdGl2YXJpYXRlIEV4dGVuc2lvbioqOgoKRm9yICRuJCBkaXN0cmlidXRpb25zICRQXzEsIFxsZG90cywgUF9uJCB3aXRoIHdlaWdodHMgJFxwaV8xLCBcbGRvdHMsIFxwaV9uJDoKCiQkCkpTRChQXzEsIFxsZG90cywgUF9uKSA9IEgoTSkgLSBcc3VtX3tpPTF9Xm4gXHBpX2kgSChQX2kpCiQkCgpXaGVyZSAkSCQgaXMgZW50cm9weSBhbmQgJE0gPSBcc3VtX2kgXHBpX2kgUF9pJC4KCiMjIyMgV2h5IEpTRCBpcyBJbXBvcnRhbnQKCkplbnNlbi1TaGFubm9uIERpdmVyZ2VuY2UgcHJvdmlkZXMgYSBwcmluY2lwbGVkLCBzeW1tZXRyaWMsIGFuZCBib3VuZGVkIHdheSB0byBtZWFzdXJlIGRpc3RyaWJ1dGlvbiBzaW1pbGFyaXR5LiBJdHMgaW5mb3JtYXRpb24tdGhlb3JldGljIGZvdW5kYXRpb24sIGNvbWJpbmVkIHdpdGggcHJhY3RpY2FsIHByb3BlcnRpZXMgbGlrZSBhbHdheXMgYmVpbmcgZmluaXRlIGFuZCBmb3JtaW5nIGEgbWV0cmljICh3aGVuIHNxdWFyZS1yb290ZWQpLCBtYWtlcyBpdCBpbnZhbHVhYmxlIGluIG1vZGVybiBtYWNoaW5lIGxlYXJuaW5nIGFuZCBkYXRhIHNjaWVuY2UgYXBwbGljYXRpb25zLg==",
  "description_decoded": "Implement the Jensen-Shannon Divergence (JSD) between two probability distributions. JSD is a symmetric and smoothed version of the Kullback-Leibler divergence that measures the similarity between two probability distributions. Unlike KL divergence, JSD is symmetric (the order of P and Q doesn't matter) and always produces a finite value. The result should be a value between 0 (identical distributions) and log(2) (completely different distributions).",
  "learn_section_decoded": "### Understanding Jensen-Shannon Divergence\n\nThe Jensen-Shannon Divergence (JSD) is a method of measuring the similarity between two probability distributions. It addresses key limitations of the Kullback-Leibler (KL) divergence by being symmetric and always producing a finite value.\n\n#### Motivation: Limitations of KL Divergence\n\nThe KL divergence between distributions $P$ and $Q$ is defined as:\n\n$$\nD_{KL}(P || Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)}\n$$\n\nHowever, KL divergence has important drawbacks:\n\n1. **Asymmetry**: $D_{KL}(P || Q) \\neq D_{KL}(Q || P)$\n2. **Undefined values**: If $Q(i) = 0$ but $P(i) > 0$, the divergence is infinite\n3. **Not a true metric**: Doesn't satisfy the triangle inequality\n\nJensen-Shannon Divergence resolves these issues.\n\n#### Definition\n\nThe Jensen-Shannon Divergence between distributions $P$ and $Q$ is:\n\n$$\nJSD(P || Q) = \\frac{1}{2} D_{KL}(P || M) + \\frac{1}{2} D_{KL}(Q || M)\n$$\n\nWhere $M$ is the average (midpoint) distribution:\n\n$$\nM = \\frac{1}{2}(P + Q)\n$$\n\nIn other words, JSD measures the average divergence of $P$ and $Q$ from their average distribution $M$.\n\n#### Mathematical Properties\n\n**Symmetry**:\n\n$$\nJSD(P || Q) = JSD(Q || P)\n$$\n\nThis follows from the symmetric definition: both $P$ and $Q$ are treated equally in relation to their average.\n\n**Bounded**:\n\n$$\n0 \\leq JSD(P || Q) \\leq \\log(2)\n$$\n\nThe upper bound $\\log(2) \\approx 0.693$ is achieved when distributions are completely different (no overlap). The lower bound $0$ is achieved when distributions are identical.\n\n**Non-negativity and Identity**:\n\n$$\nJSD(P || Q) = 0 \\iff P = Q\n$$\n\nThis makes JSD suitable as a distance-like measure.\n\n**Square Root is a Metric**:\n\n$$\nd(P, Q) = \\sqrt{JSD(P || Q)}\n$$\n\nThe square root of JSD satisfies the triangle inequality, making it a true metric (also called the Jensen-Shannon distance).\n\n#### Detailed Example\n\nConsider distributions $P = [0.5, 0.5]$ and $Q = [0.4, 0.6]$.\n\n**Step 1**: Compute the average distribution:\n\n$$\nM = \\frac{1}{2}([0.5, 0.5] + [0.4, 0.6]) = [0.45, 0.55]\n$$\n\n**Step 2**: Compute $D_{KL}(P || M)$:\n\n$$\nD_{KL}(P || M) = 0.5 \\log\\frac{0.5}{0.45} + 0.5 \\log\\frac{0.5}{0.55}\n$$\n\n$$\n= 0.5 \\log(1.111) + 0.5 \\log(0.909)\n$$\n\n$$\n= 0.5(0.105) + 0.5(-0.095) \\approx 0.005025\n$$\n\n**Step 3**: Compute $D_{KL}(Q || M)$:\n\n$$\nD_{KL}(Q || M) = 0.4 \\log\\frac{0.4}{0.45} + 0.6 \\log\\frac{0.6}{0.55}\n$$\n\n$$\n= 0.4 \\log(0.889) + 0.6 \\log(1.091)\n$$\n\n$$\n= 0.4(-0.118) + 0.6(0.087) \\approx 0.005094\n$$\n\n**Step 4**: Compute JSD:\n\n$$\nJSD(P || Q) = \\frac{1}{2}(0.005025 + 0.005094) \\approx 0.005059\n$$\n\n#### Relationship to Mutual Information\n\nJSD can be interpreted as the mutual information between a random variable $X$ (with distribution $P$ or $Q$) and a binary variable $Z$ indicating which distribution was chosen:\n\n$$\nJSD(P || Q) = I(X; Z)\n$$\n\nThis information-theoretic interpretation explains why JSD is bounded by $\\log(2)$: with two equiprobable choices, the maximum information is 1 bit.\n\n#### Comparison with Other Divergences\n\n**Versus KL Divergence**:\n- KL: Asymmetric, can be infinite\n- JSD: Symmetric, always finite, bounded\n\n**Versus Total Variation Distance**:\nTotal variation distance is:\n\n$$\nTV(P, Q) = \\frac{1}{2}\\sum_i |P(i) - Q(i)|\n$$\n\nRelationship: $TV(P, Q) \\leq \\sqrt{2 \\cdot JSD(P || Q)}$\n\n**Versus Hellinger Distance**:\nHellinger distance is:\n\n$$\nH(P, Q) = \\frac{1}{\\sqrt{2}}\\sqrt{\\sum_i (\\sqrt{P(i)} - \\sqrt{Q(i)})^2}\n$$\n\nBoth are bounded metrics, but JSD has a clearer information-theoretic interpretation.\n\n#### Practical Considerations\n\n**Numerical Stability**:\n\nWhen computing $\\log(P(i)/Q(i))$, add a small epsilon to avoid division by zero:\n\n$$\nP'(i) = \\max(P(i), \\epsilon), \\quad Q'(i) = \\max(Q(i), \\epsilon)\n$$\n\nTypically, $\\epsilon = 10^{-10}$ provides good stability.\n\n**Computational Complexity**:\n\nFor distributions with $n$ outcomes:\n- Time: $O(n)$ - single pass through distributions\n- Space: $O(n)$ - store the average distribution\n\n#### Applications\n\n**Machine Learning**: Comparing predicted probability distributions with true distributions in classification tasks.\n\n**Generative Models**: Evaluating the quality of generated samples by comparing their distribution to real data. Used in evaluating GANs (Generative Adversarial Networks).\n\n**Natural Language Processing**: Measuring similarity between topic distributions in document clustering and topic modeling.\n\n**Bioinformatics**: Comparing DNA sequence motifs and gene expression profiles.\n\n**Clustering**: As a distance metric for clustering probability distributions. The square root of JSD forms a proper metric.\n\n**Information Retrieval**: Measuring similarity between document term distributions for relevance ranking.\n\n#### Variations\n\n**Weighted Jensen-Shannon Divergence**:\n\nInstead of equal weights (0.5 each), use arbitrary weights $\\pi_1, \\pi_2$ where $\\pi_1 + \\pi_2 = 1$:\n\n$$\nJSD_{\\pi}(P || Q) = \\pi_1 D_{KL}(P || M) + \\pi_2 D_{KL}(Q || M)\n$$\n\nWhere $M = \\pi_1 P + \\pi_2 Q$.\n\n**Multivariate Extension**:\n\nFor $n$ distributions $P_1, \\ldots, P_n$ with weights $\\pi_1, \\ldots, \\pi_n$:\n\n$$\nJSD(P_1, \\ldots, P_n) = H(M) - \\sum_{i=1}^n \\pi_i H(P_i)\n$$\n\nWhere $H$ is entropy and $M = \\sum_i \\pi_i P_i$.\n\n#### Why JSD is Important\n\nJensen-Shannon Divergence provides a principled, symmetric, and bounded way to measure distribution similarity. Its information-theoretic foundation, combined with practical properties like always being finite and forming a metric (when square-rooted), makes it invaluable in modern machine learning and data science applications."
}