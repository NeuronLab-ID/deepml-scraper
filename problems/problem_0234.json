{
  "description": "SW1wbGVtZW50IGJsb2NrLXdpc2UgRlA4IHF1YW50aXphdGlvbiBhbmQgZGVxdWFudGl6YXRpb24gZm9yIG5ldXJhbCBuZXR3b3JrIGFjdGl2YXRpb25zLiBNb2Rlcm4gTExNcyBsaWtlIEtpbWkgSzIgdXNlIEZQOCAoOC1iaXQgZmxvYXRpbmcgcG9pbnQpIHRvIHJlZHVjZSBtZW1vcnkgZm9vdHByaW50IGR1cmluZyB0cmFpbmluZy4gQmxvY2std2lzZSBxdWFudGl6YXRpb24gZGl2aWRlcyB0ZW5zb3JzIGludG8gc21hbGwgYmxvY2tzLCBjb21wdXRlcyBhIHNlcGFyYXRlIHNjYWxlIGZhY3RvciBmb3IgZWFjaCBibG9jaywgYW5kIHF1YW50aXplcyB2YWx1ZXMgcmVsYXRpdmUgdG8gdGhhdCBzY2FsZS4gVGhpcyBwcmVzZXJ2ZXMgYWNjdXJhY3kgYmV0dGVyIHRoYW4gcGVyLXRlbnNvciBxdWFudGl6YXRpb24uIFlvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgcXVhbnRpemUgYW5kIGRlcXVhbnRpemUgZnVuY3Rpb25zIHVzaW5nIHRoZSBFNE0zIGZvcm1hdC4=",
  "id": "234",
  "test_cases": [
    {
      "test": "import numpy as np\ntensor = np.array([1.0, 2.0, 3.0, 4.0])\nquantized, scales = fp8_block_quantize(tensor, block_size=4)\nprint(len(scales), round(scales[0], 6), round(quantized[3], 1))",
      "expected_output": "1 0.008929 448.0"
    },
    {
      "test": "import numpy as np\ntensor = np.array([1.0, 2.0, 100.0, 200.0])\nquantized, scales = fp8_block_quantize(tensor, block_size=2)\nprint(len(scales), round(scales[0], 6), round(scales[1], 6))",
      "expected_output": "2 0.004464 0.446429"
    }
  ],
  "difficulty": "medium",
  "pytorch_difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "tensor = [1.0, 2.0, 100.0, 200.0], block_size = 2",
    "output": "Block 0: scale=0.00446, quantized=[224, 448]. Block 1: scale=0.446, quantized=[224, 448]",
    "reasoning": "With block_size=2, we get two blocks: [1.0, 2.0] and [100.0, 200.0]. Each block gets its own scale based on its max absolute value divided by 448 (max FP8-E4M3 value). Block 0 has small values so gets a small scale; Block 1 has large values so gets a larger scale. This allows both blocks to use the full FP8 dynamic range."
  },
  "category": "Deep Learning",
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpkZWYgZnA4X2Jsb2NrX3F1YW50aXplKAogICAgdGVuc29yOiB0b3JjaC5UZW5zb3IsCiAgICBibG9ja19zaXplOiBpbnQgPSAxMjgKKSAtPiB0dXBsZVt0b3JjaC5UZW5zb3IsIHRvcmNoLlRlbnNvcl06CiAgICAiIiIKICAgIFF1YW50aXplIGEgdGVuc29yIHRvIEZQOC1FNE0zIGZvcm1hdCB1c2luZyBibG9jay13aXNlIHNjYWxpbmcuCiAgICAKICAgIEFyZ3M6CiAgICAgICAgdGVuc29yOiBJbnB1dCB0ZW5zb3Igb2Ygc2hhcGUgKE4sKSB3aGVyZSBOIGlzIGRpdmlzaWJsZSBieSBibG9ja19zaXplCiAgICAgICAgYmxvY2tfc2l6ZTogTnVtYmVyIG9mIGVsZW1lbnRzIHBlciBxdWFudGl6YXRpb24gYmxvY2sKICAgICAgICAKICAgIFJldHVybnM6CiAgICAgICAgcXVhbnRpemVkOiBRdWFudGl6ZWQgdmFsdWVzIG9mIHNoYXBlIChOLCksIGNsaXBwZWQgdG8gWy00NDgsIDQ0OF0KICAgICAgICBzY2FsZXM6IFBlci1ibG9jayBzY2FsZSBmYWN0b3JzIG9mIHNoYXBlIChOIC8vIGJsb2NrX3NpemUsKQogICAgIiIiCiAgICAjIFlvdXIgY29kZSBoZXJlCiAgICBwYXNzCgoKZGVmIGZwOF9ibG9ja19kZXF1YW50aXplKAogICAgcXVhbnRpemVkOiB0b3JjaC5UZW5zb3IsCiAgICBzY2FsZXM6IHRvcmNoLlRlbnNvciwKICAgIGJsb2NrX3NpemU6IGludCA9IDEyOAopIC0+IHRvcmNoLlRlbnNvcjoKICAgICIiIgogICAgRGVxdWFudGl6ZSBGUDgtRTRNMyB2YWx1ZXMgYmFjayB0byBmdWxsIHByZWNpc2lvbi4KICAgIAogICAgQXJnczoKICAgICAgICBxdWFudGl6ZWQ6IFF1YW50aXplZCB2YWx1ZXMgb2Ygc2hhcGUgKE4sKQogICAgICAgIHNjYWxlczogUGVyLWJsb2NrIHNjYWxlIGZhY3RvcnMgb2Ygc2hhcGUgKE4gLy8gYmxvY2tfc2l6ZSwpCiAgICAgICAgYmxvY2tfc2l6ZTogTnVtYmVyIG9mIGVsZW1lbnRzIHBlciBxdWFudGl6YXRpb24gYmxvY2sKICAgICAgICAKICAgIFJldHVybnM6CiAgICAgICAgRGVxdWFudGl6ZWQgdGVuc29yIG9mIHNoYXBlIChOLCkKICAgICIiIgogICAgIyBZb3VyIGNvZGUgaGVyZQogICAgcGFzcw==",
  "title": "Block-wise FP8 Quantization",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "import torch\ntensor = torch.tensor([1.0, 2.0, 3.0, 4.0])\nquantized, scales = fp8_block_quantize(tensor, block_size=4)\nprint(len(scales), round(scales[0].item(), 6), round(quantized[3].item(), 1))",
      "expected_output": "1 0.008929 448.0"
    },
    {
      "test": "import torch\ntensor = torch.tensor([1.0, 2.0, 100.0, 200.0])\nquantized, scales = fp8_block_quantize(tensor, block_size=2)\nprint(len(scales), round(scales[0].item(), 6), round(scales[1].item(), 6))",
      "expected_output": "2 0.004464 0.446429"
    },
    {
      "test": "import torch\ntensor = torch.tensor([1.0, 2.0, 3.0, 4.0])\nquantized, scales = fp8_block_quantize(tensor, block_size=4)\ndequantized = fp8_block_dequantize(quantized, scales, block_size=4)\nprint(torch.allclose(tensor, dequantized, rtol=0.01))",
      "expected_output": "True"
    },
    {
      "test": "import torch\ntensor = torch.tensor([1000.0, -1000.0, 500.0, -500.0])\nquantized, scales = fp8_block_quantize(tensor, block_size=4)\nprint(round(scales[0].item(), 6), max(abs(quantized)).item())",
      "expected_output": "2.232143 448.0"
    }
  ],
  "starter_code": "import numpy as np\n\ndef fp8_block_quantize(\n    tensor: np.ndarray,\n    block_size: int = 128\n) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Quantize a tensor to FP8-E4M3 format using block-wise scaling.\n    \n    Args:\n        tensor: Input tensor of shape (N,) where N is divisible by block_size\n        block_size: Number of elements per quantization block\n        \n    Returns:\n        quantized: Quantized values of shape (N,), clipped to [-448, 448]\n        scales: Per-block scale factors of shape (N // block_size,)\n    \"\"\"\n    # Your code here\n    pass\n\n\ndef fp8_block_dequantize(\n    quantized: np.ndarray,\n    scales: np.ndarray,\n    block_size: int = 128\n) -> np.ndarray:\n    \"\"\"\n    Dequantize FP8-E4M3 values back to full precision.\n    \n    Args:\n        quantized: Quantized values of shape (N,)\n        scales: Per-block scale factors of shape (N // block_size,)\n        block_size: Number of elements per quantization block\n        \n    Returns:\n        Dequantized tensor of shape (N,)\n    \"\"\"\n    # Your code here\n    pass",
  "createdAt": "December 12, 2025 at 7:52:26 AM UTC-0500",
  "description_decoded": "Implement block-wise FP8 quantization and dequantization for neural network activations. Modern LLMs like Kimi K2 use FP8 (8-bit floating point) to reduce memory footprint during training. Block-wise quantization divides tensors into small blocks, computes a separate scale factor for each block, and quantizes values relative to that scale. This preserves accuracy better than per-tensor quantization. Your task is to implement quantize and dequantize functions using the E4M3 format.",
  "learn_section_decoded": "### Understanding Block-wise FP8 Quantization\n\nFP8 (8-bit floating point) quantization reduces memory usage by 4× compared to FP32, enabling larger batch sizes and faster training. Block-wise quantization makes this practical by preserving accuracy across tensors with varying magnitudes.\n\n#### Why FP8?\n\nTraining large models is memory-bound. Activations stored for the backward pass often consume more memory than model weights. By storing activations in FP8 instead of FP16/BF16, we can:\n- Reduce activation memory by 2×\n- Enable larger batch sizes\n- Speed up memory-bound operations\n\nKimi K2, for example, stores MoE up-projection and SwiGLU inputs in FP8-E4M3 format with 1×128 block sizes.\n\n#### The E4M3 Format\n\nFP8-E4M3 uses 1 sign bit, 4 exponent bits, and 3 mantissa bits:\n\n$$\\text{value} = (-1)^s \\times 2^{e-7} \\times (1 + \\frac{m}{8})$$\n\nwhere $s$ is the sign, $e$ is the exponent (0-15), and $m$ is the mantissa (0-7).\n\nKey properties:\n- **Maximum value**: 448 (when e=15, m=7)\n- **Minimum positive normal**: $2^{-6} \\approx 0.0156$\n- **Dynamic range**: ~$10^4$ (vs ~$10^{38}$ for FP32)\n\nThe limited dynamic range is why we need block-wise scaling.\n\n#### The Problem with Per-Tensor Quantization\n\nIf we use a single scale for the entire tensor, we face a tradeoff:\n- Large scale → small values become zero (underflow)\n- Small scale → large values clip to max (overflow)\n\nFor tensors with varying magnitudes across dimensions, this causes significant accuracy loss.\n\n#### Block-wise Quantization\n\nThe solution is to divide the tensor into blocks and compute a separate scale for each:\n\n**Step 1: Reshape into blocks**\n\nFor a tensor of shape $(N,)$ and block size $B$, reshape to $(N/B, B)$.\n\n**Step 2: Compute per-block scales**\n\nFor each block, the scale maps the block's maximum absolute value to the FP8 maximum:\n\n$$\\text{scale}_i = \\frac{\\max(|\\text{block}_i|)}{\\text{FP8}_{\\max}}$$\n\nwhere $\\text{FP8}_{\\max} = 448$ for E4M3.\n\n**Step 3: Quantize**\n\n$$\\text{quantized}_i = \\text{round}\\left(\\frac{\\text{block}_i}{\\text{scale}_i}\\right)$$\n\nThen clamp to $[-448, 448]$.\n\n**Step 4: Dequantize**\n\n$$\\text{dequantized}_i = \\text{quantized}_i \\times \\text{scale}_i$$\n\n#### Memory Layout\n\nThe quantized representation stores:\n- Quantized values: shape $(N/B, B)$ in FP8 (1 byte each)\n- Scales: shape $(N/B,)$ in FP32 (4 bytes each)\n\nTotal memory: $N + 4 \\times (N/B)$ bytes vs $4N$ bytes for FP32.\n\nFor block size 128: memory ratio = $(128 + 4) / (128 \\times 4) = 132/512 \\approx 0.26$, a 3.9× reduction.\n\n#### Why Block Size 128?\n\nSmaller blocks → more scales → better accuracy but more overhead.\n\nLarger blocks → fewer scales → worse accuracy but less overhead.\n\nBlock size 128 is a common sweet spot, balancing accuracy and efficiency. With 128 elements per block, scale overhead is only 3.1% of the quantized data.\n\n#### Numerical Stability\n\nWhen computing scales, add a small epsilon to avoid division by zero:\n\n$$\\text{scale}_i = \\frac{\\max(|\\text{block}_i|) + \\epsilon}{\\text{FP8}_{\\max}}$$\n\nTypically $\\epsilon = 10^{-12}$."
}