{
  "description": "SW1wbGVtZW50IHRoZSBNaXNoIGFjdGl2YXRpb24gZnVuY3Rpb24sIHdoaWNoIGlzIGEgc2VsZi1yZWd1bGFyaXppbmcgbm9uLW1vbm90b25pYyBhY3RpdmF0aW9uIGZ1bmN0aW9uIHRoYXQgaGFzIGJlZW4gc2hvd24gdG8gb3V0cGVyZm9ybSBSZUxVIGFuZCBTd2lzaCBpbiB2YXJpb3VzIGRlZXAgbGVhcm5pbmcgdGFza3MuIFRoZSBNaXNoIGZ1bmN0aW9uIGNvbWJpbmVzIHRoZSBwcm9wZXJ0aWVzIG9mIHNlbGYtZ2F0aW5nIG1lY2hhbmlzbXMgYW5kIG1haW50YWlucyBzbW9vdGggZ3JhZGllbnRzIHRocm91Z2hvdXQgaXRzIGRvbWFpbi4KCllvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgYSBmdW5jdGlvbiB0aGF0IGNvbXB1dGVzIHRoZSBNaXNoIGFjdGl2YXRpb24gdmFsdWUgZm9yIGEgZ2l2ZW4gaW5wdXQuIFRoZSBmdW5jdGlvbiBzaG91bGQgcmV0dXJuIHRoZSByZXN1bHQgcm91bmRlZCB0byA0IGRlY2ltYWwgcGxhY2VzLgoKVGhlIE1pc2ggYWN0aXZhdGlvbiBmdW5jdGlvbiB1c2VzIHRoZSBzb2Z0cGx1cyBmdW5jdGlvbiBhcyBhbiBpbnRlcm1lZGlhdGUgY29tcHV0YXRpb24sIHdoZXJlIHNvZnRwbHVzKHgpID0gbG4oMSArIGVeeCku",
  "id": "262",
  "test_cases": [
    {
      "test": "print(mish(0))",
      "expected_output": "0.0"
    },
    {
      "test": "print(mish(1))",
      "expected_output": "0.8651"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "mish(1)",
    "output": "0.8651",
    "reasoning": "For x = 1: First compute softplus(1) = ln(1 + e^1) = ln(3.718) = 1.313. Then compute tanh(1.313) = 0.8651. Finally, Mish(1) = 1 * 0.8651 = 0.8651."
  },
  "category": "Deep Learning",
  "starter_code": "import math\n\ndef mish(x: float) -> float:\n\t\"\"\"\n\tCompute the Mish activation function.\n\n\tArgs:\n\t\tx (float): Input value\n\n\tReturns:\n\t\tfloat: Mish activation value rounded to 4 decimal places\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Implement the Mish Activation Function",
  "createdAt": "December 15, 2025 at 7:02:03â€¯AM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement the Mish activation function, which is a self-regularizing non-monotonic activation function that has been shown to outperform ReLU and Swish in various deep learning tasks. The Mish function combines the properties of self-gating mechanisms and maintains smooth gradients throughout its domain.\n\nYour task is to implement a function that computes the Mish activation value for a given input. The function should return the result rounded to 4 decimal places.\n\nThe Mish activation function uses the softplus function as an intermediate computation, where softplus(x) = ln(1 + e^x).",
  "learn_section_decoded": "## Understanding the Mish Activation Function\n\nThe Mish activation function was introduced by Diganta Misra in 2019 and has gained popularity due to its smooth, non-monotonic nature that helps improve gradient flow during training.\n\n### Mathematical Definition\n\nThe Mish function is mathematically defined as:\n\n$$Mish(x) = x \\cdot \\tanh(\\text{softplus}(x))$$\n\nWhere softplus is defined as:\n\n$$\\text{softplus}(x) = \\ln(1 + e^x)$$\n\nTherefore, the complete formula is:\n\n$$Mish(x) = x \\cdot \\tanh(\\ln(1 + e^x))$$\n\n### Key Properties\n\n1. **Unbounded Above**: Like ReLU, Mish has no upper bound, allowing it to avoid saturation for positive values.\n\n2. **Bounded Below**: The function is bounded below (approximately at -0.31), which helps with regularization.\n\n3. **Non-Monotonic**: Unlike ReLU, Mish has a small negative region for negative inputs, which can preserve small negative gradients.\n\n4. **Smooth**: Mish is infinitely differentiable, making gradient computation stable.\n\n### Comparison with Other Activations\n\n| Property | ReLU | Swish | Mish |\n|----------|------|-------|------|\n| Output at $x=0$ | 0 | 0 | 0 |\n| Negative values | No | Yes | Yes |\n| Smoothness | Not smooth | Smooth | Smooth |\n| Self-gating | No | Yes | Yes |\n\n### Derivative of Mish\n\nThe derivative of Mish is:\n\n$$\\frac{d}{dx}Mish(x) = \\frac{e^x \\cdot \\omega}{\\delta^2}$$\n\nWhere:\n- $\\omega = 4(x + 1) + 4e^{2x} + e^{3x} + e^x(4x + 6)$\n- $\\delta = 2e^x + e^{2x} + 2$\n\n### Advantages\n\n1. **Better generalization**: The bounded below property acts as a regularizer\n2. **Smooth gradients**: No hard zeros in the gradient, reducing dying neuron problems\n3. **Self-regularizing**: The non-monotonic shape helps prevent overfitting\n4. **Strong experimental results**: Often outperforms ReLU and Swish in practice"
}