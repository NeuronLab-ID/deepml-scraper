{
  "description": "V3JpdGUgYSBmdW5jdGlvbiB0aGF0IGltcGxlbWVudHMgdGhlIFEtTGVhcm5pbmcgYWxnb3JpdGhtIHRvIGxlYXJuIHRoZSBvcHRpbWFsIFEtdGFibGUgZm9yIGEgZ2l2ZW4gTWFya292IERlY2lzaW9uIFByb2Nlc3MgKE1EUCkuIFRoZSBmdW5jdGlvbiBzaG91bGQgdGFrZSB0aGUgbnVtYmVyIG9mIHN0YXRlcywgbnVtYmVyIG9mIGFjdGlvbnMsIHRyYW5zaXRpb24gcHJvYmFiaWxpdGllcyBtYXRyaXgsIHJld2FyZHMgbWF0cml4LCBsaXN0IG9mIHRlcm1pbmFsIHN0YXRlcywgbGVhcm5pbmcgcmF0ZSwgZGlzY291bnQgZmFjdG9yLCBlcHNpbG9uIGZvciBleHBsb3JhdGlvbiwgYW5kIHRoZSBudW1iZXIgb2YgZXBpc29kZXMgYXMgaW5wdXRzLiBVc2UgdGhlc2UgcGFyYW1ldGVycyB0byBpdGVyYXRpdmVseSB1cGRhdGUgdGhlIFEtdGFibGUgYmFzZWQgb24gdGhlIFEtTGVhcm5pbmcgdXBkYXRlIHJ1bGUsIGVtcGxveWluZyBhbiBlcHNpbG9uLWdyZWVkeSBzdHJhdGVneSBmb3IgYWN0aW9uIHNlbGVjdGlvbi4gRW5zdXJlIHRoZSBmdW5jdGlvbiBoYW5kbGVzIHN0YXJ0aW5nIGZyb20gbm9uLXRlcm1pbmFsIHN0YXRlcyBhbmQgc3RvcHMgZXBpc29kZXMgdXBvbiByZWFjaGluZyBhIHRlcm1pbmFsIHN0YXRlLgoKQ29uc3RyYWludHM6Ci0gbnVtX3N0YXRlczogSW50ZWdlciBncmVhdGVyIHRoYW4gb3IgZXF1YWwgdG8gMS4KLSBudW1fYWN0aW9uczogSW50ZWdlciBncmVhdGVyIHRoYW4gb3IgZXF1YWwgdG8gMS4KLSBQOiBBIDNEIE51bVB5IGFycmF5IG9mIHNoYXBlIChudW1fc3RhdGVzLCBudW1fYWN0aW9ucywgbnVtX3N0YXRlcykgd2hlcmUgZWFjaCBlbGVtZW50IGlzIGEgcHJvYmFiaWxpdHkgYmV0d2VlbiAwIGFuZCAxLCBhbmQgZWFjaCBzdWItYXJyYXkgc3VtcyB0byAxLgotIFI6IEEgMkQgTnVtUHkgYXJyYXkgb2Ygc2hhcGUgKG51bV9zdGF0ZXMsIG51bV9hY3Rpb25zKSB3aXRoIGZsb2F0IG9yIGludGVnZXIgdmFsdWVzLgotIHRlcm1pbmFsX3N0YXRlczogQSBsaXN0IG9yIE51bVB5IGFycmF5IG9mIGludGVnZXJzLCBlYWNoIGJldHdlZW4gMCBhbmQgbnVtX3N0YXRlcyAtIDEsIHdpdGggbm8gZHVwbGljYXRlcy4KLSBhbHBoYTogQSBmbG9hdCBiZXR3ZWVuIDAgYW5kIDEuCi0gZ2FtbWE6IEEgZmxvYXQgYmV0d2VlbiAwIGFuZCAxLgotIGVwc2lsb246IEEgZmxvYXQgYmV0d2VlbiAwIGFuZCAxLgotIG51bV9lcGlzb2RlczogQW4gaW50ZWdlciBncmVhdGVyIHRoYW4gb3IgZXF1YWwgdG8gMS4KVGhlIGZ1bmN0aW9uIHNob3VsZCByZXR1cm4gYSAyRCBOdW1QeSBhcnJheSBvZiBzaGFwZSAobnVtX3N0YXRlcywgbnVtX2FjdGlvbnMpIHJlcHJlc2VudGluZyB0aGUgbGVhcm5lZCBRLXRhYmxlLg==",
  "id": "133",
  "test_cases": [
    {
      "test": "import numpy as np; np.random.seed(42); P = np.array([[[0, 1], [1, 0]], [[1, 0], [1, 0]]]); R = np.array([[1, 0], [0, 0]]); terminal_states = [1]; print(q_learning(2, 2, P, R, terminal_states, 0.1, 0.9, 0.1, 10))",
      "expected_output": "[[0.65132156, 0.052902  ],[0., 0.        ]]"
    },
    {
      "test": "import numpy as np; np.random.seed(42); P = np.array([[[0.5, 0.5], [0, 1]], [[0, 1], [1, 0]]]); R = np.array([[0.5, 1], [0, 0]]); terminal_states = [1]; print(q_learning(2, 2, P, R, terminal_states, 0.5, 0.8, 0.2, 5))",
      "expected_output": "[[0.91785477, 0.5       ],[0., 0.        ]]"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "import numpy as np; np.random.seed(42); P = np.array([[[0, 1], [1, 0]], [[1, 0], [1, 0]]]); R = np.array([[1, 0], [0, 0]]); terminal_states = [1]; print(q_learning(2, 2, P, R, terminal_states, 0.1, 0.9, 0.1, 10))",
    "output": "[[0.65132156, 0.052902  ],[0., 0.]]",
    "reasoning": "The Q-Learning algorithm initializes a Q-table with zeros and iteratively updates it over 10 episodes by starting from random non-terminal states, selecting actions via an epsilon-greedy policy, sampling next states and rewards from the provided transition probabilities (P) and rewards (R), and applying the update rule: Q(s, a) += alpha * (reward + gamma * max(Q(next_state)) - Q(s, a)). This process results in the output Q-table [[0.65132156, 0.052902], [0., 0.]], where the values represent learned estimates of state-action values, with the second state's Q-values remaining zero because it is a terminal state and no further actions are taken from there."
  },
  "category": "Reinforcement Learning",
  "starter_code": "import numpy as np\ndef q_learning(num_states, num_actions, P, R, terminal_states, alpha, gamma, epsilon, num_episodes):\n    # Your code here\n    pass",
  "title": "Implement Q-Learning Algorithm for MDPs",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "description_decoded": "Write a function that implements the Q-Learning algorithm to learn the optimal Q-table for a given Markov Decision Process (MDP). The function should take the number of states, number of actions, transition probabilities matrix, rewards matrix, list of terminal states, learning rate, discount factor, epsilon for exploration, and the number of episodes as inputs. Use these parameters to iteratively update the Q-table based on the Q-Learning update rule, employing an epsilon-greedy strategy for action selection. Ensure the function handles starting from non-terminal states and stops episodes upon reaching a terminal state.\n\nConstraints:\n- num_states: Integer greater than or equal to 1.\n- num_actions: Integer greater than or equal to 1.\n- P: A 3D NumPy array of shape (num_states, num_actions, num_states) where each element is a probability between 0 and 1, and each sub-array sums to 1.\n- R: A 2D NumPy array of shape (num_states, num_actions) with float or integer values.\n- terminal_states: A list or NumPy array of integers, each between 0 and num_states - 1, with no duplicates.\n- alpha: A float between 0 and 1.\n- gamma: A float between 0 and 1.\n- epsilon: A float between 0 and 1.\n- num_episodes: An integer greater than or equal to 1.\nThe function should return a 2D NumPy array of shape (num_states, num_actions) representing the learned Q-table.",
  "learn_section_decoded": "## Q-Learning: Learning Optimal Actions in Markov Decision Processes\n\nQ-Learning is a method in reinforcement learning used to estimate the value of taking specific actions in different states within a Markov Decision Process (MDP). An MDP models decision-making scenarios where the outcomes of actions depend on the current state, and the goal is to maximize long-term rewards. This section breaks down the key concepts step by step, focusing on the underlying mathematics.\n\n### 1. Understanding Markov Decision Processes\nA Markov Decision Process is a framework for sequential decision-making. It consists of states, actions, transition probabilities, and rewards. In an MDP, the future state depends only on the current state and the chosen action, not on the history of previous states.\n\n- States represent the situations an agent might encounter.\n- Actions are the choices available in each state.\n- Transition probabilities describe the likelihood of moving from one state to another after an action.\n- Rewards are numerical values that quantify the immediate benefit of taking an action in a state.\n\nFor example, imagine navigating a simple grid where each cell is a state, moving right or left is an action, and reaching a goal gives a reward.\n\n### 2. The Q-Value Function\nAt the heart of Q-Learning is the Q-value, which estimates the total expected reward of taking a specific action in a given state and then following the best possible strategy afterward.\n\nMathematically, the Q-value for a state $s$ and action $a$ is denoted as $Q(s, a)$. It is defined by the equation:\n\n$$\nQ(s, a) = r(s, a) + \\gamma \\sum_{s'} P(s' | s, a) \\max_{a'} Q(s', a')\n$$\n\nHere:\n- $r(s, a)$ is the immediate reward received for taking action $a$ in state $s$.\n- $\\gamma$ (gamma) is the discount factor, a number between 0 and 1 that reduces the importance of future rewards over time (e.g., if $\\gamma = 0.9$, rewards in the near future are valued more than those far ahead).\n- $P(s' | s, a)$ is the transition probability, representing the likelihood of ending up in state $s'$ after action $a$ in state $s$.\n- $\\max_{a'} Q(s', a')$ is the maximum Q-value of all possible actions in the next state $s'$, indicating the best future choice.\n\nThis equation captures the idea that the Q-value balances immediate rewards with the discounted value of future rewards, helping to identify the most valuable actions over time.\n\n### 3. The Q-Learning Update Rule\nQ-Learning updates the Q-value estimates iteratively based on experience, using a simple iterative formula. This process allows the agent to learn from trials without needing to know the full transition probabilities in advance.\n\nThe update rule is:\n\n$$\nQ(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n$$\n\nIn this equation:\n- $\\alpha$ (alpha) is the learning rate, a value between 0 and 1 that controls how much new information overrides old estimates (e.g., if $\\alpha = 0.1$, updates are gradual).\n- $r$ is the reward observed after taking action $a$ in state $s$.\n- $s'$ is the next state that results from the action.\n- The term inside the brackets, $r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)$, is the difference between the estimated Q-value and the actual experienced value, known as the temporal difference error.\n\nThis rule refines Q-values over multiple episodes, gradually converging to the optimal values that maximize long-term rewards.\n\n### 4. Balancing Exploration and Exploitation\nTo learn effectively, Q-Learning must balance exploring new actions (to discover potential rewards) and exploiting known high-value actions.\n\nThis is achieved through an epsilon-greedy strategy, where:\n- With probability $\\epsilon$ (epsilon, a small number like 0.1), a random action is selected to encourage exploration.\n- With probability $1 - \\epsilon$, the action with the highest Q-value is chosen to exploit current knowledge.\n\nFor instance, if $\\epsilon = 0.2$, in 20% of decisions, the agent tries something random, while in 80%, it picks the best-known option. Over time, $\\epsilon$ can be reduced to favor exploitation as learning progresses.\n\n---\n\n### Example Walkthrough\nConsider a simple two-state MDP: State A and State B, with two actions in each (Action 1 and Action 2). Suppose:\n- From State A, Action 1 leads to State B with probability 1 and a reward of 1.\n- From State B, any action ends the process with a reward of 0 (State B is terminal).\n- Let $\\gamma = 0.9$ and $\\alpha = 0.5$.\n\nInitially, assume all Q-values are 0. In the first episode:\n- Start in State A and choose Action 1 (greedily, since all Q-values are equal).\n- Move to State B, receive reward 1, and since State B is terminal, the update is:  \n  $$\n  Q(\\text{A}, \\text{Action 1}) \\leftarrow 0 + 0.5 \\left[ 1 + 0.9 \\cdot 0 - 0 \\right] = 0.5\n  $$\n- Now, Q(A, Action 1) is 0.5, so in future episodes, Action 1 is more likely in State A.\n\nThrough repeated episodes, Q-values adjust to reflect the best long-term rewards, such as prioritizing paths that lead to higher cumulative rewards."
}