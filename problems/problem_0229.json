{
  "description": "SW1wbGVtZW50IHRoZSB0b3AtayByb3V0aW5nIG1lY2hhbmlzbSB1c2VkIGluIFNwYXJzZSBNaXh0dXJlLW9mLUV4cGVydHMgKE1vRSkgbW9kZWxzLiBJbiBNb0UgYXJjaGl0ZWN0dXJlcyBsaWtlIEtpbWkgSzIgKDM4NCBleHBlcnRzLCA4IGFjdGl2ZSkgYW5kIERlZXBTZWVrLVYzLCBlYWNoIHRva2VuIGlzIHJvdXRlZCB0byBvbmx5IHRoZSB0b3AtSyBleHBlcnRzIGJhc2VkIG9uIHJvdXRlciBzY29yZXMsIGVuYWJsaW5nIG1hc3NpdmUgbW9kZWwgY2FwYWNpdHkgd2l0aCBmaXhlZCBjb21wdXRlIGNvc3QuIFlvdXIgdGFzayBpcyB0byBpbXBsZW1lbnQgdGhlIHJvdXRpbmcgbG9naWMgdGhhdCBzZWxlY3RzIGV4cGVydHMsIGNvbXB1dGVzIHJvdXRpbmcgd2VpZ2h0cyB2aWEgc29mdG1heCBvdmVyIHNlbGVjdGVkIGV4cGVydHMsIGFuZCBjb21iaW5lcyBleHBlcnQgb3V0cHV0cy4=",
  "id": "229",
  "test_cases": [
    {
      "test": "import numpy as np\nrouter_logits = np.array([[2.0, 1.0, 0.5, 0.1]])\nexpert_outputs = np.array([[[1.0, 0.0], [0.0, 1.0], [1.0, 1.0], [0.0, 0.0]]])\nresult = moe_topk_routing(router_logits, expert_outputs, k=2)\nprint(np.round(result, 4).tolist())",
      "expected_output": "[[0.7311, 0.2689]]"
    },
    {
      "test": "import numpy as np\nrouter_logits = np.array([[1.0, 1.0, 1.0, 1.0]])\nexpert_outputs = np.array([[[1.0, 0.0], [0.0, 1.0], [1.0, 1.0], [2.0, 2.0]]])\nresult = moe_topk_routing(router_logits, expert_outputs, k=2)\nprint(np.round(result, 4).tolist())",
      "expected_output": "[[0.5, 0.5]]"
    }
  ],
  "difficulty": "medium",
  "pytorch_difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "router_logits = [[2.0, 1.0, 0.5, 0.1]],  # 1 token, 4 experts\nexpert_outputs = [[[1, 0], [0, 1], [1, 1], [0, 0]]],  # outputs from each expert\nk = 2",
    "output": "[[0.731, 0.269]]  # weighted combination of top-2 experts (experts 0 and 1)",
    "reasoning": "Top-2 experts by logits are expert 0 (logit=2.0) and expert 1 (logit=1.0). Softmax over [2.0, 1.0] gives weights [0.731, 0.269]. Final output = 0.731 * [1,0] + 0.269 * [0,1] = [0.731, 0.269]."
  },
  "category": "Deep Learning",
  "pytorch_starter_code": "aW1wb3J0IHRvcmNoCgpkZWYgbW9lX3RvcGtfcm91dGluZygKICAgIHJvdXRlcl9sb2dpdHM6IHRvcmNoLlRlbnNvciwKICAgIGV4cGVydF9vdXRwdXRzOiB0b3JjaC5UZW5zb3IsCiAgICBrOiBpbnQKKSAtPiB0b3JjaC5UZW5zb3I6CiAgICAiIiIKICAgIFBlcmZvcm0gdG9wLWsgZXhwZXJ0IHJvdXRpbmcgZm9yIGEgTWl4dHVyZS1vZi1FeHBlcnRzIGxheWVyLgogICAgCiAgICBGb3IgZWFjaCB0b2tlbjoKICAgIDEuIFNlbGVjdCB0aGUgdG9wLWsgZXhwZXJ0cyBiYXNlZCBvbiByb3V0ZXJfbG9naXRzCiAgICAyLiBDb21wdXRlIHNvZnRtYXggd2VpZ2h0cyBvdmVyIG9ubHkgdGhlIHNlbGVjdGVkIGV4cGVydHMKICAgIDMuIFJldHVybiB3ZWlnaHRlZCBjb21iaW5hdGlvbiBvZiB0aGUgc2VsZWN0ZWQgZXhwZXJ0IG91dHB1dHMKICAgIAogICAgQXJnczoKICAgICAgICByb3V0ZXJfbG9naXRzOiBTaGFwZSAoYmF0Y2hfc2l6ZSwgbnVtX2V4cGVydHMpCiAgICAgICAgICAgICAgICAgICAgICBSYXcgc2NvcmVzIGZyb20gdGhlIHJvdXRlciBmb3IgZWFjaCBleHBlcnQKICAgICAgICBleHBlcnRfb3V0cHV0czogU2hhcGUgKGJhdGNoX3NpemUsIG51bV9leHBlcnRzLCBoaWRkZW5fZGltKQogICAgICAgICAgICAgICAgICAgICAgIE91dHB1dCBmcm9tIGVhY2ggZXhwZXJ0IGZvciBlYWNoIGlucHV0CiAgICAgICAgazogTnVtYmVyIG9mIGV4cGVydHMgdG8gc2VsZWN0IHBlciB0b2tlbgogICAgICAgIAogICAgUmV0dXJuczoKICAgICAgICBTaGFwZSAoYmF0Y2hfc2l6ZSwgaGlkZGVuX2RpbSkgLSB3ZWlnaHRlZCBjb21iaW5hdGlvbiBvZiBleHBlcnQgb3V0cHV0cwogICAgIiIiCiAgICAjIFlvdXIgY29kZSBoZXJlCiAgICBwYXNz",
  "title": "Sparse MoE Top-K Routing",
  "starter_code": "import numpy as np\n\ndef moe_topk_routing(\n    router_logits: np.ndarray,\n    expert_outputs: np.ndarray,\n    k: int\n) -> np.ndarray:\n    \"\"\"\n    Perform top-k expert routing for a Mixture-of-Experts layer.\n    \n    For each token:\n    1. Select the top-k experts based on router_logits\n    2. Compute softmax weights over only the selected experts\n    3. Return weighted combination of the selected expert outputs\n    \n    Args:\n        router_logits: Shape (batch_size, num_experts)\n                      Raw scores from the router for each expert\n        expert_outputs: Shape (batch_size, num_experts, hidden_dim)\n                       Output from each expert for each input\n        k: Number of experts to select per token\n        \n    Returns:\n        Shape (batch_size, hidden_dim) - weighted combination of expert outputs\n    \"\"\"\n    # Your code here\n    pass",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "pytorch_test_cases": [
    {
      "test": "import torch\nrouter_logits = torch.tensor([[2.0, 1.0, 0.5, 0.1]])\nexpert_outputs = torch.tensor([[[1.0, 0.0], [0.0, 1.0], [1.0, 1.0], [0.0, 0.0]]])\nresult = moe_topk_routing(router_logits, expert_outputs, k=2)\nprint(torch.round(result * 10000) / 10000)",
      "expected_output": "tensor([[0.7311, 0.2689]])"
    },
    {
      "test": "import torch\nrouter_logits = torch.tensor([[1.0, 1.0, 1.0, 1.0]])\nexpert_outputs = torch.tensor([[[1.0, 0.0], [0.0, 1.0], [1.0, 1.0], [2.0, 2.0]]])\nresult = moe_topk_routing(router_logits, expert_outputs, k=2)\nprint(torch.round(result * 10000) / 10000)",
      "expected_output": "tensor([[0.5000, 0.5000]])"
    },
    {
      "test": "import torch\nrouter_logits = torch.tensor([[0.0, 0.0, 10.0, 0.0]])\nexpert_outputs = torch.tensor([[[1.0, 1.0], [2.0, 2.0], [3.0, 3.0], [4.0, 4.0]]])\nresult = moe_topk_routing(router_logits, expert_outputs, k=1)\nprint(torch.round(result * 10000) / 10000)",
      "expected_output": "tensor([[3., 3.]])"
    },
    {
      "test": "import torch\nrouter_logits = torch.tensor([[1.0, 2.0, 3.0], [3.0, 2.0, 1.0]])\nexpert_outputs = torch.tensor([[[1.0], [2.0], [3.0]], [[1.0], [2.0], [3.0]]])\nresult = moe_topk_routing(router_logits, expert_outputs, k=2)\nprint(torch.round(result * 10000) / 10000)",
      "expected_output": "tensor([[2.7311],\n        [1.2689]])"
    }
  ],
  "learn_section": "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",
  "createdAt": "December 10, 2025 at 4:13:55 PM UTC-0500",
  "description_decoded": "Implement the top-k routing mechanism used in Sparse Mixture-of-Experts (MoE) models. In MoE architectures like Kimi K2 (384 experts, 8 active) and DeepSeek-V3, each token is routed to only the top-K experts based on router scores, enabling massive model capacity with fixed compute cost. Your task is to implement the routing logic that selects experts, computes routing weights via softmax over selected experts, and combines expert outputs.",
  "learn_section_decoded": "## Sparse Mixture-of-Experts Routing\n\n### The Big Idea\n\nHow do you build a trillion-parameter model that runs as fast as a 32-billion parameter one? The answer is **sparse activation**. Instead of using all parameters for every input, you selectively activate only a small subset.\n\nThis is the core insight behind Mixture-of-Experts (MoE) models. Kimi K2, for instance, has 384 expert networks but only activates 8 of them for each token. The router's job is to pick which 8.\n\n### How Routing Works\n\nThink of it like a restaurant with 384 specialized chefs, but you can only afford to have 8 cook your meal. A host (the router) looks at your order and decides which 8 chefs are best suited for it.\n\n### The Math\n\nA router network produces a score for each expert:\n\n$$\\text{logits} = x \\cdot W_r \\in \\mathbb{R}^{N}$$\n\nWe select the top-K experts and apply softmax *only* over those K scores:\n\n$$w_i = \\frac{\\exp(\\text{logits}_i)}{\\sum_{j \\in \\text{top}_k} \\exp(\\text{logits}_j)}$$\n\nThe final output is a weighted combination:\n\n$$y = \\sum_{i \\in \\text{top}_k} w_i \\cdot E_i(x)$$\n\n### Step-by-Step Example\n\nLet's trace through with 4 experts and K=2.\n\nGiven router scores [2.0, 1.0, 0.5, 0.1] and expert outputs [[1,0], [0,1], [1,1], [0,0]]:\n\n**Step 1 - Find top-K:** Sort by score, take top 2 → experts 0 (score=2.0) and 1 (score=1.0).\n\n**Step 2 - Softmax over selected:** We softmax over *only* the selected experts:\n\n$$w_0 = \\frac{e^{2.0}}{e^{2.0} + e^{1.0}} = \\frac{7.39}{10.11} = 0.731$$\n\n$$w_1 = \\frac{e^{1.0}}{e^{2.0} + e^{1.0}} = \\frac{2.72}{10.11} = 0.269$$\n\n**Step 3 - Weighted sum:**\n\n$$\\text{output} = 0.731 \\times [1, 0] + 0.269 \\times [0, 1] = [0.731, 0.269]$$\n\n### Why Softmax Over Only Top-K?\n\nYou might wonder: why not softmax over all experts, then zero out the non-selected ones?\n\nThe answer is **gradient flow**. By computing softmax only over selected experts, the routing weights sum to 1.0, and gradients flow cleanly through the selected paths. This makes training more stable.\n\n### The Sparsity Advantage\n\nWith K=8 and N=384 (like Kimi K2), each token uses only 2% of the experts. You get the *capacity* of a trillion-parameter model with the *compute cost* of a much smaller one.\n\n### Implementation Tips\n\nThe trickiest part is the **gather operation**. You need to select the top-k indices via descending sort, gather the corresponding logits for softmax, then gather the corresponding expert outputs for the weighted combination. For batched inputs, you'll need advanced indexing with a batch dimension.\n\n### Numerical Stability\n\nAlways use the stable softmax formula:\n\n$$\\text{softmax}(x)_i = \\frac{\\exp(x_i - \\max(x))}{\\sum_j \\exp(x_j - \\max(x))}$$\n\nSubtracting the max prevents overflow without changing the result."
}