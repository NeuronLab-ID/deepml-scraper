{
  "description": "VGhlIE11b24gb3B0aW1pemVyIGlzIGFuIGFsZ29yaXRobSB0aGF0IGNvbWJpbmVzIG1vbWVudHVtIHdpdGggYSBtYXRyaXggcHJlY29uZGl0aW9uaW5nIHN0ZXAgYmFzZWQgb24gdGhlIE5ld3Rvbi1TY2h1bHogaXRlcmF0aW9uLiBJbiB0aGlzIHRhc2ssIHlvdSB3aWxsIGltcGxlbWVudCBhIHNpbmdsZSBNdW9uIG9wdGltaXplciB1cGRhdGUgZm9yIGEgMkQgTnVtUHkgYXJyYXkgb2YgcGFyYW1ldGVycy4gWW91ciBpbXBsZW1lbnRhdGlvbiBzaG91bGQ6Ci0gVXBkYXRlIHRoZSBtb21lbnR1bSB1c2luZyB0aGUgY3VycmVudCBncmFkaWVudCBhbmQgcHJldmlvdXMgbW9tZW50dW0uCi0gQXBwbHkgdGhlIE5ld3Rvbi1TY2h1bHogbWF0cml4IGl0ZXJhdGlvbiAob3JkZXIgNSkgdG8gcHJlY29uZGl0aW9uIHRoZSB1cGRhdGUgZGlyZWN0aW9uLiBUaGlzIGludm9sdmVzIG5vcm1hbGl6aW5nIHRoZSB1cGRhdGUsIHBvc3NpYmx5IHRyYW5zcG9zaW5nIGZvciB3aWRlIG1hdHJpY2VzLCBhbmQgcnVubmluZyBhIGZpeGVkIG1hdHJpeCBpdGVyYXRpb24gZm9yIGEgbnVtYmVyIG9mIHN0ZXBzLgotIFVzZSBhIHNjYWxlIGZhY3RvciBiYXNlZCBvbiB0aGUgUk1TIG9wZXJhdG9yIG5vcm0gZm9yIHN0YWJpbGl0eS4KLSBVcGRhdGUgdGhlIHBhcmFtZXRlcnMgdXNpbmcgdGhlIHByZWNvbmRpdGlvbmVkIGRpcmVjdGlvbiwgbGVhcm5pbmcgcmF0ZSwgYW5kIHNjYWxlLgoKUmV0dXJuIGJvdGggdGhlIHVwZGF0ZWQgcGFyYW1ldGVyIG1hdHJpeCBhbmQgbW9tZW50dW0u",
  "id": "170",
  "test_cases": [
    {
      "test": "theta = np.eye(2)\nB = np.zeros((2,2))\ngrad = np.ones((2,2))\neta = 0.1\nmu = 0.9\ntheta_new, B_new = muon_step(theta, B, grad, eta, mu, ns_steps=2)\nprint(np.round(theta_new, 3))",
      "expected_output": "[[ 0.944, -0.056], [-0.056, 0.944]]"
    },
    {
      "test": "theta = np.eye(6)\nB = np.zeros((6,6))\ngrad = np.ones((6,6))\neta = 0.7\nmu = 0.95\ntheta_new, B_new = muon_step(theta, B, grad, eta, mu, ns_steps=2)\nprint(np.round(theta_new, 3))",
      "expected_output": "[[ 0.87, -0.13, -0.13, -0.13, -0.13, -0.13], [-0.13, 0.87, -0.13, -0.13, -0.13, -0.13], [-0.13, -0.13, 0.87, -0.13, -0.13, -0.13], [-0.13, -0.13, -0.13, 0.87, -0.13, -0.13], [-0.13, -0.13, -0.13, -0.13, 0.87, -0.13], [-0.13, -0.13, -0.13, -0.13, -0.13, 0.87]]"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "theta = np.eye(2)\nB = np.zeros((2,2))\ngrad = np.ones((2,2))\neta = 0.1\nmu = 0.9\ntheta_new, B_new = muon_step(theta, B, grad, eta, mu, ns_steps=2)\nprint(np.round(theta_new, 3))",
    "output": "[[ 0.944 -0.056] [-0.056 0.944]]",
    "reasoning": "After the momentum and Newton-Schulz preconditioning, the parameters are updated in the direction of the scaled matrix."
  },
  "category": "Optimization",
  "starter_code": "import numpy as np\n\ndef newton_schulz5(G, steps=5, eps=1e-7):\n    # Implement the Newton-Schulz iteration for matrix orthogonalization/preconditioning\n    pass\n\ndef muon_step(theta, B, grad, eta, mu, ns_steps=5, eps=1e-7):\n    \"\"\"\n    theta: np.ndarray, shape (M, N)\n    B: np.ndarray, shape (M, N)\n    grad: np.ndarray, shape (M, N)\n    eta: float (learning rate)\n    mu: float (momentum coefficient)\n    ns_steps: int (Newton-Schulz steps)\n    eps: float (numerical stability)\n    Returns: updated theta, updated B\n    \"\"\"\n    # Implement the Muon optimizer update step\n    pass\n",
  "title": "Muon Optimizer Step with Matrix Preconditioning",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "description_decoded": "The Muon optimizer is an algorithm that combines momentum with a matrix preconditioning step based on the Newton-Schulz iteration. In this task, you will implement a single Muon optimizer update for a 2D NumPy array of parameters. Your implementation should:\n- Update the momentum using the current gradient and previous momentum.\n- Apply the Newton-Schulz matrix iteration (order 5) to precondition the update direction. This involves normalizing the update, possibly transposing for wide matrices, and running a fixed matrix iteration for a number of steps.\n- Use a scale factor based on the RMS operator norm for stability.\n- Update the parameters using the preconditioned direction, learning rate, and scale.\n\nReturn both the updated parameter matrix and momentum.",
  "learn_section_decoded": "# **Muon Optimizer: Mathematical Foundations**\n\n## **1. Introduction**\n\nThe **Muon optimizer** is a gradient-based optimization algorithm that combines **momentum** with a matrix preconditioning step based on the **Newton-Schulz iteration**. The goal is to stabilize and speed up neural network training, especially when parameters are matrices (such as in fully connected and convolutional layers).\n\n---\n\n## **2. Step 1: Momentum Update**\n\nMuon uses the **momentum method** to smooth the stochasticity of gradients:\n\n$$\nM_t = \\beta \\cdot M_{t-1} + (1 - \\beta) \\cdot g_t\n$$\n\n- $M_t$: Momentum at step $t$\n- $g_t$: Current gradient\n- $\\beta$: Momentum decay coefficient (e.g., 0.9â€“0.99)\n\nWith **Nesterov momentum**, the update direction is:\n\n$$\nU_t = (1 - \\beta) \\cdot g_t + \\beta \\cdot M_t\n$$\n\n---\n\n## **3. Step 2: Newton-Schulz Matrix Preconditioning**\n\nFor matrix-shaped gradients (or reshaped higher-dimensional gradients), Muon applies a **Newton-Schulz iteration**. This is an iterative algorithm that, in this context, \"orthogonalizes\" or stabilizes the update direction without needing explicit inversion or SVD.\n\n### **A. Frobenius Normalization**\n\nFirst, the update matrix $X$ is normalized by its Frobenius norm to avoid scale explosion:\n\n$$\nX_0 = \\frac{U_t}{\\| U_t \\|_F + \\varepsilon}\n$$\n\nwhere $\\| U_t \\|_F = \\sqrt{\\sum_{i,j} (U_t)_{ij}^2}$, and $\\varepsilon$ is a small constant for stability.\n\n### **B. Quintic Newton-Schulz Iteration**\n\nThe following update is performed **for $k = 1$ to $N$ steps** (often $N=5$):\n\n$$\nA = X_k X_k^\\top \\\\\nB = b \\cdot A + c \\cdot (A A) \\\\\nX_{k+1} = a \\cdot X_k + B X_k\n$$\n\nwith fixed coefficients:\n\n- $a = 3.4445$\n- $b = -4.7750$\n- $c = 2.0315$\n\nThis process \"pushes\" $X$ closer to an orthogonal-like matrix, improving the update's conditioning.\n\n### **C. Optional Reshape**\n\nIf the parameter tensor is 4D (common in conv layers), it is reshaped into a 2D matrix for preconditioning, then reshaped back after.\n\n---\n\n## **4. Step 3: Parameter Update**\n\nFinally, the parameter update is:\n\n$$\n\\theta_t = \\theta_{t-1} - \\eta \\cdot S_t\n$$\n\nwhere $S_t$ is the matrix after Newton-Schulz preconditioning and $\\eta$ is the learning rate.\n\n---\n\n## **5. Summary**\n\n- **Momentum:** Smooths the update direction by combining past and current gradients.\n- **Matrix Preconditioning:** Applies an iterative matrix operation to stabilize and orthogonalize the update, reducing problems due to ill-conditioning.\n- **Parameter Update:** Uses the preconditioned matrix to update model parameters.\n\nMuon is especially effective for **matrix-shaped weights** and large neural networks, improving stability and potentially accelerating convergence.\n\n---\n"
}