{
  "description": "SW1wbGVtZW50IGEgZnVuY3Rpb24gdGhhdCBjcmVhdGVzIGEgc2ltcGxlIHJlc2lkdWFsIGJsb2NrIHVzaW5nIE51bVB5LiBUaGUgYmxvY2sgc2hvdWxkIHRha2UgYSAxRCBpbnB1dCBhcnJheSwgcHJvY2VzcyBpdCB0aHJvdWdoIHR3byB3ZWlnaHQgbGF5ZXJzICh1c2luZyBtYXRyaXggbXVsdGlwbGljYXRpb24pLCBhcHBseSBSZUxVIGFjdGl2YXRpb25zLCBhbmQgYWRkIHRoZSBvcmlnaW5hbCBpbnB1dCB2aWEgYSBzaG9ydGN1dCBjb25uZWN0aW9uIGJlZm9yZSBhIGZpbmFsIFJlTFUgYWN0aXZhdGlvbi4=",
  "id": "113",
  "test_cases": [
    {
      "test": "x = np.array([1.0, 2.0])\nw1 = np.array([[1.0, 0.0], [0.0, 1.0]])\nw2 = np.array([[0.5, 0.0], [0.0, 0.5]])\nprint(residual_block(x, w1, w2))",
      "expected_output": "[1.5,3.]"
    },
    {
      "test": "x = np.array([-1.0, 2.0])\nw1 = np.array([[1.0, 0.0], [0.0, 1.0]])\nw2 = np.array([[0.5, 0.0], [0.0, 0.5]])\nprint(residual_block(x, w1, w2))",
      "expected_output": "[0.,3.]"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "x = np.array([1.0, 2.0]), w1 = np.array([[1.0, 0.0], [0.0, 1.0]]), w2 = np.array([[0.5, 0.0], [0.0, 0.5]])",
    "output": "[1.5, 3.0]",
    "reasoning": "The input x is [1.0, 2.0]. First, compute w1 @ x = [1.0, 2.0], apply ReLU to get [1.0, 2.0]. Then, compute w2 @ [1.0, 2.0] = [0.5, 1.0]. Add the shortcut x to get [0.5 + 1.0, 1.0 + 2.0] = [1.5, 3.0]. Final ReLU gives [1.5, 3.0]."
  },
  "category": "Deep Learning",
  "starter_code": "import numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> np.ndarray:\n\t# Your code here\n\tpass",
  "title": "Implement a Simple Residual Block with Shortcut Connection",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "description_decoded": "Implement a function that creates a simple residual block using NumPy. The block should take a 1D input array, process it through two weight layers (using matrix multiplication), apply ReLU activations, and add the original input via a shortcut connection before a final ReLU activation.",
  "learn_section_decoded": "## Understanding Residual Blocks in ResNet\n\nResidual blocks are the cornerstone of ResNet (Residual Network), a deep learning architecture designed to train very deep neural networks by addressing issues like vanishing gradients. The key idea is to allow the network to learn residuals differences between the input and the desired output rather than the full transformation.\n\n### Core Concept: Residual Learning\nIn a traditional neural network layer, the output is a direct transformation of the input, such as $H(x)$, where $x$ is the input. In a residual block, instead of learning $H(x)$ directly, the network learns the residual $F(x) = H(x) - x$. The output of the block is then:\n\n$$\ny = F(x) + x\n$$\n\nHere, $F(x)$ represents the transformation applied by the layers within the block (e.g., weight layers and activations), and $x$ is the input, added back via a shortcut connection. This structure allows the network to learn an identity function ($F(x) = 0$, so $y = x$) if needed, which helps in training deeper networks.\n\n### Mathematical Structure\nA typical residual block involves two weight layers with an activation function between them. The activation function used in ResNet is ReLU, defined as:\n\n$$\n\\text{ReLU}(z) = \\max(0, z)\n$$\n\nThe block takes an input $x$, applies a transformation $F(x)$ through the weight layers and activations, and then adds the input $x$ back. Mathematically, if the weight layers are represented by matrices $W_1$ and $W_2$, the transformation $F(x)$ might look like a composition of operations involving $W_1 \\cdot x$, a ReLU activation, and $W_2$ applied to the result. The final output $y$ is the sum of $F(x)$ and $x$, often followed by another ReLU activation to ensure non-negativity.\n\n### Why Shortcut Connections?\n- **Ease of Learning**: If the optimal transformation is close to an identity function, the block can learn $F(x) \\approx 0$, making $y \\approx x$.\n- **Gradient Flow**: The shortcut connection allows gradients to flow directly through the addition operation during backpropagation, helping to train deeper networks without vanishing gradients.\n\n### Conceptual Example\nSuppose the input $x$ is a vector of length 2, and the weight layers are matrices $W_1$ and $W_2$. The block computes $F(x)$ by applying $W_1$, a ReLU activation, and $W_2$, then adds $x$ to the result. The shortcut connection ensures that even if $F(x)$ is small, the output $y$ retains information from $x$, making it easier for the network to learn.\n\nThis structure is what enables ResNet to scale to hundreds of layers while maintaining performance, as shown in the diagram of the residual block."
}