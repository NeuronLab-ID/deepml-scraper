{
  "description": "SW1wbGVtZW50IHRoZSBTb2Z0cGx1cyBhY3RpdmF0aW9uIGZ1bmN0aW9uLCBhIHNtb290aCBhcHByb3hpbWF0aW9uIG9mIHRoZSBSZUxVIGZ1bmN0aW9uLiBZb3VyIHRhc2sgaXMgdG8gY29tcHV0ZSB0aGUgU29mdHBsdXMgdmFsdWUgZm9yIGEgZ2l2ZW4gaW5wdXQsIGhhbmRsaW5nIGVkZ2UgY2FzZXMgdG8gcHJldmVudCBudW1lcmljYWwgb3ZlcmZsb3cgb3IgdW5kZXJmbG93Lg==",
  "id": "99",
  "test_cases": [
    {
      "test": "print(softplus(0))",
      "expected_output": "0.6931"
    },
    {
      "test": "print(softplus(100))",
      "expected_output": "100.0"
    }
  ],
  "difficulty": "easy",
  "marimo_link": "https://open-deep-ml.github.io/DML-OpenProblem/problem-99",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "softplus(2)",
    "output": "2.1269",
    "reasoning": "For x = 2, the Softplus activation is calculated as $\\log(1 + e^x)$."
  },
  "category": "Deep Learning",
  "starter_code": "def softplus(x: float) -> float:\n\t\"\"\"\n\tCompute the softplus activation function.\n\n\tArgs:\n\t\tx: Input value\n\n\tReturns:\n\t\tThe softplus value: log(1 + e^x)\n\t\"\"\"\n\t# Your code here\n\tpass\n\t return round(val,4)",
  "title": "Implement the Softplus Activation Function",
  "learn_section": "IyMjIFVuZGVyc3RhbmRpbmcgdGhlIFNvZnRwbHVzIEFjdGl2YXRpb24gRnVuY3Rpb24KClRoZSBTb2Z0cGx1cyBhY3RpdmF0aW9uIGZ1bmN0aW9uIGlzIGEgc21vb3RoIGFwcHJveGltYXRpb24gb2YgdGhlIFJlTFUgZnVuY3Rpb24uIEl0J3MgdXNlZCBpbiBuZXVyYWwgbmV0d29ya3Mgd2hlcmUgYSBzbW9vdGhlciB0cmFuc2l0aW9uIGFyb3VuZCB6ZXJvIGlzIGRlc2lyZWQuIFVubGlrZSBSZUxVIHdoaWNoIGhhcyBhIHNoYXJwIHRyYW5zaXRpb24gYXQgeD0wLCBTb2Z0cGx1cyBwcm92aWRlcyBhIG1vcmUgZ3JhZHVhbCBjaGFuZ2UuCgojIyMgTWF0aGVtYXRpY2FsIERlZmluaXRpb24KClRoZSBTb2Z0cGx1cyBmdW5jdGlvbiBpcyBtYXRoZW1hdGljYWxseSBkZWZpbmVkIGFzOgoKJCQKU29mdHBsdXMoeCkgPSBcbG9nKDEgKyBlXngpCiQkCgpXaGVyZToKLSAkeCQgaXMgdGhlIGlucHV0IHRvIHRoZSBmdW5jdGlvbgotICRlJCBpcyBFdWxlcidzIG51bWJlciAoYXBwcm94aW1hdGVseSAyLjcxODI4KQotICRcbG9nJCBpcyB0aGUgbmF0dXJhbCBsb2dhcml0aG0KCiMjIyBDaGFyYWN0ZXJpc3RpY3MKCjEuICoqT3V0cHV0IFJhbmdlKio6IAogICAtIFRoZSBvdXRwdXQgaXMgYWx3YXlzIHBvc2l0aXZlOiAkKDAsIFxpbmZ0eSkkCiAgIC0gVW5saWtlIFJlTFUsIFNvZnRwbHVzIG5ldmVyIG91dHB1dHMgZXhhY3RseSB6ZXJvCgoyLiAqKlNtb290aG5lc3MqKjoKICAgLSBTb2Z0cGx1cyBpcyBjb250aW51b3VzbHkgZGlmZmVyZW50aWFibGUKICAgLSBUaGUgdHJhbnNpdGlvbiBhcm91bmQgeD0wIGlzIHNtb290aCwgdW5saWtlIFJlTFUncyBzaGFycCAiZWxib3ciCgozLiAqKlJlbGF0aW9uc2hpcCB0byBSZUxVKio6CiAgIC0gU29mdHBsdXMgY2FuIGJlIHNlZW4gYXMgYSBzbW9vdGggYXBwcm94aW1hdGlvbiBvZiBSZUxVCiAgIC0gQXMgeCBiZWNvbWVzIHZlcnkgbmVnYXRpdmUsIFNvZnRwbHVzIGFwcHJvYWNoZXMgMAogICAtIEFzIHggYmVjb21lcyB2ZXJ5IHBvc2l0aXZlLCBTb2Z0cGx1cyBhcHByb2FjaGVzIHgKCjQuICoqRGVyaXZhdGl2ZSoqOgogICAtIFRoZSBkZXJpdmF0aXZlIG9mIFNvZnRwbHVzIGlzIHRoZSBsb2dpc3RpYyBzaWdtb2lkIGZ1bmN0aW9uOgogICAkJAogICBcZnJhY3tkfXtkeH1Tb2Z0cGx1cyh4KSA9IFxmcmFjezF9ezEgKyBlXnsteH19CiAgICQkCgojIyMgVXNlIENhc2VzCi0gV2hlbiBzbW9vdGggZ3JhZGllbnRzIGFyZSBpbXBvcnRhbnQgZm9yIG9wdGltaXphdGlvbgotIEluIG5ldXJhbCBuZXR3b3JrcyB3aGVyZSBhIGNvbnRpbnVvdXMgYXBwcm94aW1hdGlvbiBvZiBSZUxVIGlzIG5lZWRlZAotIFNpdHVhdGlvbnMgd2hlcmUgc3RyaWN0bHkgcG9zaXRpdmUgb3V0cHV0cyBhcmUgcmVxdWlyZWQgd2l0aCBzbW9vdGggdHJhbnNpdGlvbnM=",
  "contributor": [
    {
      "profile_link": "https://github.com/Haleshot",
      "name": "Haleshot"
    }
  ],
  "description_decoded": "Implement the Softplus activation function, a smooth approximation of the ReLU function. Your task is to compute the Softplus value for a given input, handling edge cases to prevent numerical overflow or underflow.",
  "learn_section_decoded": "### Understanding the Softplus Activation Function\n\nThe Softplus activation function is a smooth approximation of the ReLU function. It's used in neural networks where a smoother transition around zero is desired. Unlike ReLU which has a sharp transition at x=0, Softplus provides a more gradual change.\n\n### Mathematical Definition\n\nThe Softplus function is mathematically defined as:\n\n$$\nSoftplus(x) = \\log(1 + e^x)\n$$\n\nWhere:\n- $x$ is the input to the function\n- $e$ is Euler's number (approximately 2.71828)\n- $\\log$ is the natural logarithm\n\n### Characteristics\n\n1. **Output Range**: \n   - The output is always positive: $(0, \\infty)$\n   - Unlike ReLU, Softplus never outputs exactly zero\n\n2. **Smoothness**:\n   - Softplus is continuously differentiable\n   - The transition around x=0 is smooth, unlike ReLU's sharp \"elbow\"\n\n3. **Relationship to ReLU**:\n   - Softplus can be seen as a smooth approximation of ReLU\n   - As x becomes very negative, Softplus approaches 0\n   - As x becomes very positive, Softplus approaches x\n\n4. **Derivative**:\n   - The derivative of Softplus is the logistic sigmoid function:\n   $$\n   \\frac{d}{dx}Softplus(x) = \\frac{1}{1 + e^{-x}}\n   $$\n\n### Use Cases\n- When smooth gradients are important for optimization\n- In neural networks where a continuous approximation of ReLU is needed\n- Situations where strictly positive outputs are required with smooth transitions"
}