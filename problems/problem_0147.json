{
  "description": "SW1wbGVtZW50IGEgUHl0aG9uIGZ1bmN0aW9uIHRoYXQgYXBwbGllcyB0aGUgR0VMVSAoR2F1c3NpYW4gRXJyb3IgTGluZWFyIFVuaXQpIGFjdGl2YXRpb24gZnVuY3Rpb24gdG8gYSBOdW1QeSBhcnJheSBvZiBsb2dpdHMuIFJvdW5kIGVhY2ggb3V0cHV0IHRvIGZvdXIgZGVjaW1hbCBwbGFjZXMgYW5kIHJldHVybiB0aGUgcmVzdWx0IGFzIGEgTnVtUHkgYXJyYXkgb2YgdGhlIHNhbWUgc2hhcGUu",
  "id": "147",
  "test_cases": [
    {
      "test": "print(np.round(GeLU(np.array([0.1, 0.0, -0.1])),4))",
      "expected_output": "[ 0.054,0.,-0.046]"
    },
    {
      "test": "print(np.round(GeLU(np.array([5.5, -4.2, 0.75])),4))",
      "expected_output": "[ 5.5,-0.,0.58]"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "np.array([-2.0, -1.0, 0.0, 1.0, 2.0])",
    "output": "[-0.0454 -0.1588 0. 0.8412 1.9546]",
    "reasoning": "Each value in the input array is passed through the GELU activation function using the approximation formula. For example, GELU(1.0) ≈ 0.841192, which rounds to 0.8412. Similarly, GELU(-2.0) ≈ -0.045402, which rounds to -0.0454. Since we're using numpy, it uses vectorized operations and applies the formula to each element in the array. Then, we rounded it to four decimal places to produce the output array."
  },
  "category": "Deep Learning",
  "starter_code": "import numpy as np\n\ndef GeLU(x: np.ndarray) -> np.ndarray:\n\t# Your code here\n\treturn scores",
  "title": "GeLU Activation Function ",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/PT-10",
      "name": "PT-10"
    }
  ],
  "description_decoded": "Implement a Python function that applies the GELU (Gaussian Error Linear Unit) activation function to a NumPy array of logits. Round each output to four decimal places and return the result as a NumPy array of the same shape.",
  "learn_section_decoded": "## Understanding the GELU Activation Function\n\nThe **GELU** (Gaussian Error Linear Unit) is an activation function that combines properties of **ReLU** and **tanh (or) sigmoid** but adds a probabilistic interpretation, making it smooth and differentiable.\n\n### Mathematical Definition\n\nThe GELU activation is defined as:\n\n$$\n\\text{GELU}(x) = x \\cdot \\Phi(x)\n$$\n\nwhere $\\Phi(x)$ is the cumulative distribution function (CDF) of the standard normal distribution:\n\n$$\n\\Phi(x) = \\frac{1}{2} \\left(1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right)\n$$\n\nA widely used **approximation** is:\n\n$$\n\\text{GELU}(x) \\approx 0.5x \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}}(x + 0.044715x^3)\\right)\\right)\n$$\n\n> In some cases, a **sigmoid-based formula** may be used in place of **tanh** to approximate the erf function. If you wish to dive deep, you may read it [here](https://datascience.stackexchange.com/questions/49522/what-is-gelu-activation).\n### Characteristics\n\n- **Smooth and Nonlinear**: Unlike ReLU, which is piecewise linear, GELU is smooth and differentiable everywhere.\n- **Retains Small Inputs**: It does not zero out all negative values like ReLU, but scales them down, which can be beneficial for gradient flow.\n- **Stochastic Interpretation**: Treats input as a random variable and gates it based on the likelihood it is positive.\n\n### Use in Practice\n\nGELU is the default activation function in **Transformer-based models** such as BERT and GPT due to its ability to better capture complex relationships in data."
}