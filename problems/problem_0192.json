{
  "description": "SW1wbGVtZW50IHRoZSBIdWJlciBMb3NzIGZ1bmN0aW9uLCB3aGljaCBjb21iaW5lcyB0aGUgYmVzdCBhc3BlY3RzIG9mIE1lYW4gU3F1YXJlZCBFcnJvciAoTVNFKSBhbmQgTWVhbiBBYnNvbHV0ZSBFcnJvciAoTUFFKS4gVGhlIGZ1bmN0aW9uIHNob3VsZCBjb21wdXRlIHRoZSBIdWJlciBMb3NzIGdpdmVuIHRydWUgYW5kIHByZWRpY3RlZCB2YWx1ZXMgYW5kIGEgZGVsdGEgdGhyZXNob2xkLiBJdCBzaG91bGQgaGFuZGxlIGJvdGggc2NhbGFyIGFuZCBsaXN0IGlucHV0cyBhbmQgcmV0dXJuIHRoZSBhdmVyYWdlIGxvc3MgYXMgYSBmbG9hdC4=",
  "id": "192",
  "test_cases": [
    {
      "test": "print(round(huber_loss([2.0, 3.0, 4.0], [2.5, 2.0, 4.0], delta=1.0), 4))",
      "expected_output": "0.2083"
    },
    {
      "test": "print(round(huber_loss([1.0, 2.0, 3.0], [1.1, 2.2, 2.9], delta=0.5), 4))",
      "expected_output": "0.01"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "round(huber_loss([2.0, 3.0, 4.0], [2.5, 2.0, 4.0], delta=1.0), 4)",
    "output": "0.2083",
    "reasoning": "Each element's contribution is computed according to the Huber loss formula and averaged, yielding 0.208333..., which rounds to 0.2083."
  },
  "category": "Machine Learning",
  "starter_code": "def huber_loss(y_true, y_pred, delta=1.0):\n\t\"\"\"\n\tCompute the Huber Loss between true and predicted values.\n\n\tArgs:\n\t\ty_true (float | list[float]): Ground truth values\n\t\ty_pred (float | list[float]): Predicted values\n\t\tdelta (float): Transition threshold between MSE and MAE behavior\n\n\tReturns:\n\t\tfloat: Average Huber loss\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Implement the Huber Loss Function",
  "createdAt": "November 8, 2025 at 11:49:05â€¯PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "learn_section": "IyMgVW5kZXJzdGFuZGluZyB0aGUgSHViZXIgTG9zcyBGdW5jdGlvbgoKVGhlIEh1YmVyIExvc3MgaXMgYSByb2J1c3QgbG9zcyBmdW5jdGlvbiB0aGF0IGlzIGxlc3Mgc2Vuc2l0aXZlIHRvIG91dGxpZXJzIGNvbXBhcmVkIHRvIE1lYW4gU3F1YXJlZCBFcnJvciAoTVNFKS4gSXQgYmVoYXZlcyBsaWtlIE1TRSBmb3Igc21hbGwgZXJyb3JzIGFuZCBsaWtlIE1lYW4gQWJzb2x1dGUgRXJyb3IgKE1BRSkgZm9yIGxhcmdlIGVycm9ycy4KCiMjIyBNYXRoZW1hdGljYWwgRGVmaW5pdGlvbgoKRm9yIGEgcHJlZGljdGlvbiAkXGhhdHt5fSQgYW5kIGEgdHJ1ZSB2YWx1ZSAkeSQsIHdpdGggdGhyZXNob2xkIHBhcmFtZXRlciAkXGRlbHRhJDoKCiQkCkxfe1xkZWx0YX0oeSwgXGhhdHt5fSkgPSBcYmVnaW57Y2FzZXN9ClxmcmFjezF9ezJ9KHkgLSBcaGF0e3l9KV4yICYgXHRleHR7aWYgfSB8eSAtIFxoYXR7eX18IFxsZXEgXGRlbHRhIFxcClxkZWx0YSAofHkgLSBcaGF0e3l9fCAtIFxmcmFjezF9ezJ9XGRlbHRhKSAmIFx0ZXh0e290aGVyd2lzZX0KXGVuZHtjYXNlc30KJCQKCiMjIyBLZXkgUHJvcGVydGllcwoKLSAqKkZvciBzbWFsbCBlcnJvcnMgKCR8eSAtIFxoYXR7eX18IFxsZXEgXGRlbHRhJCk6KiogQWN0cyBsaWtlIE1lYW4gU3F1YXJlZCBFcnJvciwgcHJvdmlkaW5nIHNtb290aCBncmFkaWVudHMuCi0gKipGb3IgbGFyZ2UgZXJyb3JzICgkfHkgLSBcaGF0e3l9fCA+IFxkZWx0YSQpOioqIEFjdHMgbGlrZSBNZWFuIEFic29sdXRlIEVycm9yLCByZWR1Y2luZyB0aGUgaW5mbHVlbmNlIG9mIG91dGxpZXJzLgotICoqUGFyYW1ldGVyICRcZGVsdGEkOioqIENvbnRyb2xzIHRoZSB0cmFuc2l0aW9uIHBvaW50IGJldHdlZW4gcXVhZHJhdGljIGFuZCBsaW5lYXIgYmVoYXZpb3IuCgojIyMgQWR2YW50YWdlcwoKLSBSb2J1c3QgdG8gb3V0bGllcnMgY29tcGFyZWQgdG8gTVNFLgotIERpZmZlcmVudGlhYmxlIGV2ZXJ5d2hlcmUsIHVubGlrZSBNQUUuCi0gQ29tbW9ubHkgdXNlZCBpbiByZWdyZXNzaW9uIGFuZCByb2J1c3QgdHJhaW5pbmcgdGFza3MuCgojIyMgRXhhbXBsZQoKQ29uc2lkZXIgJHkgPSBbMi4wLCAzLjAsIDQuMF0kLCAkXGhhdHt5fSA9IFsyLjUsIDIuMCwgNC4wXSQsIGFuZCAkXGRlbHRhID0gMS4wJC4KCkZvciBlYWNoIGVsZW1lbnQ6Ci0gJHwyLjAgLSAyLjV8ID0gMC41IFxsZXEgMS4wJDogbG9zcyA9ICQwLjUgKiAwLjVeMiA9IDAuMTI1JAotICR8My4wIC0gMi4wfCA9IDEuMCBcbGVxIDEuMCQ6IGxvc3MgPSAkMC41ICogMS4wXjIgPSAwLjUkCi0gJHw0LjAgLSA0LjB8ID0gMC4wJDogbG9zcyA9ICQwLjAkCgpBdmVyYWdlIEh1YmVyIExvc3MgPSAkKDAuMTI1ICsgMC41ICsgMC4wKS8zID0gMC4yMDgzMzMuLi4k",
  "description_decoded": "Implement the Huber Loss function, which combines the best aspects of Mean Squared Error (MSE) and Mean Absolute Error (MAE). The function should compute the Huber Loss given true and predicted values and a delta threshold. It should handle both scalar and list inputs and return the average loss as a float.",
  "learn_section_decoded": "## Understanding the Huber Loss Function\n\nThe Huber Loss is a robust loss function that is less sensitive to outliers compared to Mean Squared Error (MSE). It behaves like MSE for small errors and like Mean Absolute Error (MAE) for large errors.\n\n### Mathematical Definition\n\nFor a prediction $\\hat{y}$ and a true value $y$, with threshold parameter $\\delta$:\n\n$$\nL_{\\delta}(y, \\hat{y}) = \\begin{cases}\n\\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n\\delta (|y - \\hat{y}| - \\frac{1}{2}\\delta) & \\text{otherwise}\n\\end{cases}\n$$\n\n### Key Properties\n\n- **For small errors ($|y - \\hat{y}| \\leq \\delta$):** Acts like Mean Squared Error, providing smooth gradients.\n- **For large errors ($|y - \\hat{y}| > \\delta$):** Acts like Mean Absolute Error, reducing the influence of outliers.\n- **Parameter $\\delta$:** Controls the transition point between quadratic and linear behavior.\n\n### Advantages\n\n- Robust to outliers compared to MSE.\n- Differentiable everywhere, unlike MAE.\n- Commonly used in regression and robust training tasks.\n\n### Example\n\nConsider $y = [2.0, 3.0, 4.0]$, $\\hat{y} = [2.5, 2.0, 4.0]$, and $\\delta = 1.0$.\n\nFor each element:\n- $|2.0 - 2.5| = 0.5 \\leq 1.0$: loss = $0.5 * 0.5^2 = 0.125$\n- $|3.0 - 2.0| = 1.0 \\leq 1.0$: loss = $0.5 * 1.0^2 = 0.5$\n- $|4.0 - 4.0| = 0.0$: loss = $0.0$\n\nAverage Huber Loss = $(0.125 + 0.5 + 0.0)/3 = 0.208333...$"
}