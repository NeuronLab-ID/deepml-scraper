{
  "description": "SW1wbGVtZW50IHRoZSBTRUxVIChTY2FsZWQgRXhwb25lbnRpYWwgTGluZWFyIFVuaXQpIGFjdGl2YXRpb24gZnVuY3Rpb24sIGEgc2VsZi1ub3JtYWxpemluZyB2YXJpYW50IG9mIEVMVS4gWW91ciB0YXNrIGlzIHRvIGNvbXB1dGUgdGhlIFNFTFUgdmFsdWUgZm9yIGEgZ2l2ZW4gaW5wdXQgd2hpbGUgZW5zdXJpbmcgbnVtZXJpY2FsIHN0YWJpbGl0eS4=",
  "id": "103",
  "test_cases": [
    {
      "test": "print(round(selu(1.0), 4))",
      "expected_output": "1.0507"
    },
    {
      "test": "print(round(selu(0.0), 4))",
      "expected_output": "0.0"
    }
  ],
  "difficulty": "easy",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "selu(-1.0)",
    "output": "-1.1113",
    "reasoning": "For x = -1.0, the SELU activation is calculated using the formula $SELU(x) = \\lambda \\alpha (e^x - 1)$. Substituting the values of $\\lambda$ and $\\alpha$, we get $SELU(-1.0) = 1.0507 \\times 1.6733 \\times (e^{-1.0} - 1) = -1.1113$."
  },
  "category": "Deep Learning",
  "starter_code": "def selu(x: float) -> float:\n\t\"\"\"\n\tImplements the SELU (Scaled Exponential Linear Unit) activation function.\n\n\tArgs:\n\t\tx: Input value\n\n\tReturns:\n\t\tSELU activation value\n\t\"\"\"\n\talpha = 1.6732632423543772\n\tscale = 1.0507009873554804\n\t# Your code here\n\tpass",
  "title": "Implement the SELU Activation Function",
  "learn_section": "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",
  "contributor": [
    {
      "profile_link": "https://github.com/Haleshot",
      "name": "Haleshot"
    }
  ],
  "description_decoded": "Implement the SELU (Scaled Exponential Linear Unit) activation function, a self-normalizing variant of ELU. Your task is to compute the SELU value for a given input while ensuring numerical stability.",
  "learn_section_decoded": "## Understanding the SELU Activation Function\n\nThe SELU (Scaled Exponential Linear Unit) activation function is a self-normalizing variant of the ELU activation function, introduced in 2017. It's particularly useful in deep neural networks as it automatically ensures normalized outputs with zero mean and unit variance.\n\n### Mathematical Definition\n\nThe SELU function is defined as:\n\n$$\nSELU(x) = \\lambda \\begin{cases} \nx & \\text{if } x > 0 \\\\\n\\alpha(e^x - 1) & \\text{if } x \\leq 0\n\\end{cases}\n$$\n\nWhere:\n- $\\lambda \\approx 1.0507$ is the scale parameter\n- $\\alpha \\approx 1.6733$ is the alpha parameter\n\n### Characteristics\n\n- **Output Range:** The function maps inputs to $(-\\lambda\\alpha, \\infty)$\n- **Self-Normalizing:** Automatically maintains mean close to 0 and variance close to 1\n- **Continuous:** The function is continuous and differentiable everywhere\n- **Non-Linear:** Provides non-linearity while preserving gradients for negative values\n- **Parameters:** Uses carefully chosen values for $\\lambda$ and $\\alpha$ to ensure self-normalization\n\n### Advantages\n\n1. **Self-Normalization:** Eliminates the need for batch normalization in many cases\n2. **Robust Learning:** Helps prevent vanishing and exploding gradients\n3. **Better Performance:** Often leads to faster training in deep neural networks\n4. **Internal Normalization:** Maintains normalized activations throughout the network\n\n### Use Cases\n\nSELU is particularly effective in:\n- Deep neural networks where maintaining normalized activations is crucial\n- Networks that require self-normalizing properties\n- Scenarios where batch normalization might be problematic or expensive"
}