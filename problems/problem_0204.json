{
  "description": "Q29tcHV0ZSB0aGUgbXV0dWFsIGluZm9ybWF0aW9uIGJldHdlZW4gdHdvIHJhbmRvbSB2YXJpYWJsZXMgWCBhbmQgWSBnaXZlbiB0aGVpciBqb2ludCBwcm9iYWJpbGl0eSBkaXN0cmlidXRpb24uIE11dHVhbCBpbmZvcm1hdGlvbiBtZWFzdXJlcyBob3cgbXVjaCBpbmZvcm1hdGlvbiBvbmUgdmFyaWFibGUgcHJvdmlkZXMgYWJvdXQgYW5vdGhlciAtIGl0IHF1YW50aWZpZXMgdGhlIHJlZHVjdGlvbiBpbiB1bmNlcnRhaW50eSBhYm91dCBvbmUgdmFyaWFibGUgZ2l2ZW4ga25vd2xlZGdlIG9mIHRoZSBvdGhlci4gVGhlIHJlc3VsdCBpcyAwIHdoZW4gdmFyaWFibGVzIGFyZSBpbmRlcGVuZGVudCBhbmQgbWF4aW1pemVkIHdoZW4gdGhleSBhcmUgcGVyZmVjdGx5IGRlcGVuZGVudC4=",
  "id": "204",
  "test_cases": [
    {
      "test": "import numpy as np; mi = mutual_information([[0.25, 0.25], [0.25, 0.25]]); print(round(mi, 6))",
      "expected_output": "0.0"
    },
    {
      "test": "import numpy as np; mi = mutual_information([[0.5, 0.0], [0.0, 0.5]]); print(round(mi, 6))",
      "expected_output": "0.693147"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "joint_prob = [[0.4, 0.1], [0.1, 0.4]]",
    "output": "0.192745",
    "reasoning": "First compute marginals: $P(X) = [0.5, 0.5]$, $P(Y) = [0.5, 0.5]$. Then apply the formula: $I(X;Y) = 0.4\\log(0.4/0.25) + 0.1\\log(0.1/0.25) + 0.1\\log(0.1/0.25) + 0.4\\log(0.4/0.25) = 0.188 - 0.092 - 0.092 + 0.188 \\approx 0.193$."
  },
  "category": "Information Theory",
  "starter_code": "import numpy as np\n\ndef mutual_information(joint_prob: list[list[float]]) -> float:\n\t\"\"\"\n\tCompute the mutual information between two random variables.\n\t\n\tArgs:\n\t\tjoint_prob: 2D joint probability distribution P(X,Y)\n\t\n\tReturns:\n\t\tMutual information I(X;Y)\n\t\"\"\"\n\t# Your code here\n\tpass",
  "title": "Mutual Information",
  "createdAt": "November 16, 2025 at 7:33:20â€¯AM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Compute the mutual information between two random variables X and Y given their joint probability distribution. Mutual information measures how much information one variable provides about another - it quantifies the reduction in uncertainty about one variable given knowledge of the other. The result is 0 when variables are independent and maximized when they are perfectly dependent.",
  "learn_section_decoded": "### Understanding Mutual Information\n\nMutual information quantifies the amount of information one random variable contains about another. It's a fundamental concept in information theory that measures statistical dependence between variables.\n\n#### Definition\n\nFor two random variables $X$ and $Y$ with joint probability distribution $P(X,Y)$, the mutual information is:\n\n$$\nI(X;Y) = \\sum_{x \\in X} \\sum_{y \\in Y} P(x,y) \\log \\frac{P(x,y)}{P(x)P(y)}\n$$\n\nWhere:\n- $P(x,y)$ is the joint probability\n- $P(x)$ and $P(y)$ are marginal probabilities\n- The logarithm is typically natural log (giving nats) or base-2 (giving bits)\n\n#### Intuition\n\nThe ratio $\\frac{P(x,y)}{P(x)P(y)}$ compares:\n- **Joint probability**: How often $x$ and $y$ actually occur together\n- **Product of marginals**: How often they would occur together if independent\n\nWhen this ratio is:\n- **Equal to 1**: Variables are independent at that point\n- **Greater than 1**: Variables co-occur more than expected (positive dependence)\n- **Less than 1**: Variables co-occur less than expected (negative dependence)\n\n#### Relationship to Entropy\n\nMutual information can be expressed using entropy:\n\n$$\nI(X;Y) = H(X) + H(Y) - H(X,Y)\n$$\n\nWhere:\n- $H(X) = -\\sum_x P(x) \\log P(x)$ is the entropy of $X$\n- $H(X,Y) = -\\sum_{x,y} P(x,y) \\log P(x,y)$ is the joint entropy\n\nThis shows that mutual information equals the sum of individual uncertainties minus the joint uncertainty.\n\n**Alternative form**:\n\n$$\nI(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\n$$\n\nThis interpretation: MI is the reduction in uncertainty about $X$ after observing $Y$ (and vice versa).\n\n#### Key Properties\n\n**Non-negativity**: $I(X;Y) \\geq 0$ with equality when $X$ and $Y$ are independent.\n\n**Symmetry**: $I(X;Y) = I(Y;X)$ - information flows both ways.\n\n**Bounded**: $I(X;Y) \\leq \\min(H(X), H(Y))$ - can't learn more about $X$ from $Y$ than the total uncertainty in $X$.\n\n**Independence**: $I(X;Y) = 0$ if and only if $X$ and $Y$ are independent, i.e., $P(x,y) = P(x)P(y)$ for all $x,y$.\n\n**Perfect dependence**: $I(X;Y) = H(X) = H(Y)$ when $Y$ is a deterministic function of $X$ (or vice versa).\n\n#### Example Calculation\n\nConsider:\n\n$$\nP(X,Y) = \\begin{pmatrix}\n0.4 & 0.1 \\\\\n0.1 & 0.4\n\\end{pmatrix}\n$$\n\n**Step 1**: Compute marginals:\n\n$$\nP(X) = [0.5, 0.5], \\quad P(Y) = [0.5, 0.5]\n$$\n\n**Step 2**: Apply formula:\n\n$$\nI(X;Y) = 0.4 \\log\\frac{0.4}{0.5 \\times 0.5} + 0.1 \\log\\frac{0.1}{0.5 \\times 0.5} + 0.1 \\log\\frac{0.1}{0.5 \\times 0.5} + 0.4 \\log\\frac{0.4}{0.5 \\times 0.5}\n$$\n\n$$\n= 0.4 \\log(1.6) + 0.1 \\log(0.4) + 0.1 \\log(0.4) + 0.4 \\log(1.6)\n$$\n\n$$\n\\approx 0.188 - 0.092 - 0.092 + 0.188 = 0.192\n$$\n\n#### Computing from Joint Distribution\n\nGiven a joint probability matrix:\n\n1. **Extract marginals**:\n   - $P(X=i) = \\sum_j P(X=i, Y=j)$ (sum over columns)\n   - $P(Y=j) = \\sum_i P(X=i, Y=j)$ (sum over rows)\n\n2. **Sum contributions**:\n   - For each cell with $P(x,y) > 0$, add: $P(x,y) \\log \\frac{P(x,y)}{P(x)P(y)}$\n   - Skip zero probabilities to avoid $\\log(0)$\n\n#### Applications\n\n**Feature Selection**: Choose features with high MI with the target variable - they're most informative.\n\n**Clustering**: Group variables with high mutual information - they share common patterns.\n\n**Network Analysis**: Identify dependencies in neural networks or gene regulatory networks.\n\n**Data Compression**: Variables with high MI can be compressed more efficiently together.\n\n**Cryptography**: Low MI between plaintext and ciphertext indicates good encryption.\n\n#### Relationship to Other Measures\n\n**KL Divergence**: $I(X;Y) = D_{KL}(P(X,Y) || P(X)P(Y))$ - MI is the KL divergence between joint and product of marginals.\n\n**Correlation**: MI captures any dependency (linear or nonlinear), while correlation only captures linear relationships. MI = 0 implies independence; correlation = 0 only implies no linear relationship.\n\n**Jensen-Shannon Divergence**: Related through: $JSD(P||Q) = I(X;Z)$ where $Z$ indicates which distribution was chosen.\n\n#### Practical Notes\n\n**Estimation from data**: When computing MI from samples, use frequency counts to estimate probabilities. Add-one smoothing can help with sparse data.\n\n**Normalization**: Normalized MI scales to $[0,1]$: $NMI = \\frac{I(X;Y)}{\\min(H(X), H(Y))}$ or $NMI = \\frac{2I(X;Y)}{H(X) + H(Y)}$.\n\n**Continuous variables**: For continuous variables, use differential mutual information with probability density functions instead of probability mass functions."
}