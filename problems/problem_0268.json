{
  "description": "SW1wbGVtZW50IHRoZSByZXdhcmQgY29tcHV0YXRpb24gZm9yIHRoZSBSZWxhdGl2aXN0aWMgQWR2ZXJzYXJpYWwgUmVhc29uaW5nIE9wdGltaXphdGlvbiAoUkFSTykgYWxnb3JpdGhtLiBSQVJPIHRyYWlucyBMTE1zIHRvIHJlYXNvbiB1c2luZyBvbmx5IGV4cGVydCBkZW1vbnN0cmF0aW9ucyB3aXRob3V0IHRhc2stc3BlY2lmaWMgdmVyaWZpZXJzLiBJdCBzZXRzIHVwIGFuIGFkdmVyc2FyaWFsIGdhbWUgYmV0d2VlbiBhIHBvbGljeSAod2hpY2ggZ2VuZXJhdGVzIGFuc3dlcnMpIGFuZCBhIHJlbGF0aXZpc3RpYyBjcml0aWMgKHdoaWNoIGNvbXBhcmVzIHBvbGljeSB2cyBleHBlcnQgYW5zd2VycykuIEdpdmVuIGEgY3JpdGljJ3MgcHJlZGljdGlvbiAod2hpY2ggYW5zd2VyIGlzIGJldHRlcjogJ2V4cGVydCcsICdwb2xpY3knLCBvciAndGllJykgYW5kIHRoZSBncm91bmQgdHJ1dGggbGFiZWxzLCBjb21wdXRlIGJvdGggdGhlIGNyaXRpYyByZXdhcmQgYW5kIHRoZSBwb2xpY3kgcmV3YXJkLiBUaGUgY3JpdGljIGlzIHJld2FyZGVkIGZvciBjb3JyZWN0bHkgaWRlbnRpZnlpbmcgdGhlIGV4cGVydCBhbnN3ZXIsIHdoaWxlIHRoZSBwb2xpY3kgaXMgcmV3YXJkZWQgd2hlbiB0aGUgY3JpdGljIG1pc3Rha2VzIGl0cyBhbnN3ZXIgZm9yIHRoZSBleHBlcnQncy4gQm90aCBjYW4gcmVjZWl2ZSBwYXJ0aWFsIHJld2FyZHMgZm9yICd0aWUnIHByZWRpY3Rpb25zIGJhc2VkIG9uIGNvbmZpZ3VyYWJsZSB0aWUgcmV3YXJkIHBhcmFtZXRlcnMu",
  "id": "268",
  "test_cases": [
    {
      "test": "print(compute_raro_rewards('expert', 1))",
      "expected_output": "(1.0, 0.0)"
    },
    {
      "test": "print(compute_raro_rewards('policy', 1))",
      "expected_output": "(0.0, 1.0)"
    }
  ],
  "difficulty": "medium",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "example": {
    "input": "critic_prediction='expert', expert_position=1, tau_critic=0.5, tau_policy=0.5",
    "output": "(1.0, 0.0)",
    "reasoning": "The critic correctly identifies that the expert answer is better. Since the critic's prediction matches the ground truth (expert answer is in position 1 and critic says 'expert'), the critic receives a reward of 1.0. The policy receives 0.0 because it failed to fool the critic."
  },
  "category": "Deep Learning",
  "starter_code": "def compute_raro_rewards(\n    critic_prediction: str,\n    expert_position: int,\n    tau_critic: float = 0.5,\n    tau_policy: float = 0.5\n) -> tuple[float, float]:\n    \"\"\"\n    Compute rewards for the critic and policy in the RARO adversarial game.\n    \n    In RARO, a relativistic critic compares two answers (one from expert, one from policy)\n    and predicts which is better. The critic and policy receive rewards based on whether\n    the critic correctly identifies the expert answer.\n    \n    Args:\n        critic_prediction: The critic's prediction - one of 'expert', 'policy', or 'tie'\n        expert_position: Which position (1 or 2) contains the expert answer in the pair\n                        (the other position contains the policy answer)\n        tau_critic: Reward given to critic when it predicts 'tie' (default: 0.5)\n        tau_policy: Reward given to policy when critic predicts 'tie' (default: 0.5)\n    \n    Returns:\n        Tuple of (critic_reward, policy_reward) where:\n        - critic_reward: 1.0 if critic correctly identifies expert, tau_critic if tie, 0.0 otherwise\n        - policy_reward: 1.0 if critic incorrectly identifies policy as expert, tau_policy if tie, 0.0 otherwise\n    \"\"\"\n    pass",
  "title": "Implement Relativistic Critic Rewards for Adversarial Reasoning",
  "createdAt": "December 15, 2025 at 6:37:46â€¯PM UTC-0500",
  "contributor": [
    {
      "profile_link": "https://github.com/Open-Deep-ML",
      "name": "Deep-ML"
    }
  ],
  "learn_section": "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",
  "description_decoded": "Implement the reward computation for the Relativistic Adversarial Reasoning Optimization (RARO) algorithm. RARO trains LLMs to reason using only expert demonstrations without task-specific verifiers. It sets up an adversarial game between a policy (which generates answers) and a relativistic critic (which compares policy vs expert answers). Given a critic's prediction (which answer is better: 'expert', 'policy', or 'tie') and the ground truth labels, compute both the critic reward and the policy reward. The critic is rewarded for correctly identifying the expert answer, while the policy is rewarded when the critic mistakes its answer for the expert's. Both can receive partial rewards for 'tie' predictions based on configurable tie reward parameters.",
  "learn_section_decoded": "## Relativistic Adversarial Reasoning Optimization (RARO)\n\nRARO is a method for training Large Language Models to reason using only expert demonstrations, without requiring task-specific verifiers. It addresses a key limitation of Reinforcement Learning with Verifiable Rewards (RLVR): many real-world reasoning tasks lack objective verifiers.\n\n### The Adversarial Game\n\nRARO sets up an adversarial interaction between two components:\n\n1. **Policy (Generator)**: Generates answers with reasoning chains, trying to produce expert-quality responses\n2. **Relativistic Critic (Discriminator)**: Compares pairs of answers and identifies which came from the expert\n\nBoth components are trained jointly via reinforcement learning, creating a dynamic where the policy improves by fooling the critic, and the critic improves by better distinguishing expert from policy answers.\n\n### Why \"Relativistic\"?\n\nUnlike standard discriminators that classify a single answer as \"expert\" or \"policy\", the relativistic critic performs pairwise comparison. Given a question and two candidate answers (one from expert, one from policy), it outputs:\n\n- **'expert'**: The expert answer is better\n- **'policy'**: The policy answer is better  \n- **'tie'**: Both answers are equally good\n\nThe \"tie\" option is crucial for training stability. As the policy approaches expert quality, forcing the critic to choose becomes increasingly difficult and leads to high-variance gradients. The tie option provides a stable reward signal when answers are indistinguishable.\n\n### Reward Functions\n\nThe rewards create the adversarial dynamic:\n\n**Critic Reward:**\n$$R_{\\text{critic}} = \\mathbb{1}_{\\ell = \\text{expert}} + \\tau_{\\text{crit}} \\cdot \\mathbb{1}_{\\ell = \\text{tie}}$$\n\n**Policy Reward:**\n$$R_{\\text{policy}} = \\mathbb{1}_{\\ell = \\text{policy}} + \\tau_{\\text{pol}} \\cdot \\mathbb{1}_{\\ell = \\text{tie}}$$\n\nWhere $\\ell$ is the critic's prediction, and $\\tau_{\\text{crit}}, \\tau_{\\text{pol}} \\in [0, 1]$ are tie reward hyperparameters.\n\n### Interpretation\n\n- The critic wants to correctly identify the expert answer (reward = 1) or at least declare a tie (partial reward)\n- The policy wants to fool the critic into thinking its answer is the expert (reward = 1) or achieve a tie (partial reward)\n- This creates a minimax game that drives both components to improve\n\n### Connection to Inverse Reinforcement Learning\n\nRARO can be viewed through the lens of Inverse RL: instead of learning from explicit rewards, we learn a reward function (the critic) from expert demonstrations. The critic implicitly learns what makes an answer \"expert-like\", which then guides the policy's learning.\n\n### Key Benefits\n\n- **No Verifier Required**: Works with demonstration data alone\n- **Emergent Reasoning**: The policy learns to reason through the adversarial process\n- **Stable Training**: The tie option and relativistic setup prevent training collapse\n- **Test-Time Scaling**: The learned critic enables tournament-style answer selection at inference"
}